From 93c793fd9c474517f76b80f9d94f93244077940d Mon Sep 17 00:00:00 2001
From: "anand.vidwansa" <anand_vidwansa@xyratex.com>
Date: Sun, 11 Aug 2013 23:09:38 -0700
Subject: [PATCH 09/50] bigmemalloc: - Made code common to user-space and
 kernel-space. - Context specific routines are moved to lib/linux_kernel and
 lib/user_space   directories. - Introduced first draft of a generic radix
 tree like data structure used as a   virtual array. - Depth first tree
 traversal is done. - Introduced m0_pow(base, exp) which calculates nth
 exponential power of   number base. - Introduced a lookup table in m0_varr in
 order to avoid repeated invocations   of exactly same computations. - UT is
 still due. - An optimized version of tree traversal needed during iteration
 of virtual   array is still due.

---
 lib/Kbuild.sub          |   1 +
 lib/Makefile.sub        |   5 +-
 lib/arith.h             |   7 +
 lib/linux_kernel/varr.c | 403 ++---------------------------------
 lib/linux_kernel/varr.h | 292 --------------------------
 lib/misc.c              |  12 ++
 lib/user_space/varr.c   |  57 +++++
 lib/ut/varr.c           |  41 ++--
 lib/varr.c              | 545 ++++++++++++++++++++++++++++++++++++++++++++++++
 lib/varr.h              | 227 ++++++++++++++++++++
 10 files changed, 892 insertions(+), 698 deletions(-)
 delete mode 100644 lib/linux_kernel/varr.h
 create mode 100644 lib/user_space/varr.c
 create mode 100644 lib/varr.c
 create mode 100644 lib/varr.h

diff --git a/lib/Kbuild.sub b/lib/Kbuild.sub
index 1ef109c..815adac 100644
--- a/lib/Kbuild.sub
+++ b/lib/Kbuild.sub
@@ -26,6 +26,7 @@ m0mero_objects += lib/assert.o \
                   lib/trace.o \
                   lib/types_xc.o \
                   lib/uuid.o \
+                  lib/varr.o \
                   lib/vec.o \
                   lib/vec.o \
                   lib/vec_xc.o
diff --git a/lib/Makefile.sub b/lib/Makefile.sub
index fbe6889..45e6437 100644
--- a/lib/Makefile.sub
+++ b/lib/Makefile.sub
@@ -40,6 +40,7 @@ nobase_mero_include_HEADERS += lib/adt.h \
                                lib/types_xc.h \
                                lib/ub.h \
                                lib/uuid.h \
+                               lib/varr.h \
                                lib/vec.h \
                                lib/vec_xc.h \
                                lib/user_space/cdefs.h \
@@ -88,6 +89,7 @@ mero_libmero_la_SOURCES += lib/assert.c \
                            lib/types_xc.c \
                            lib/ub.c \
                            lib/uuid.c \
+                           lib/varr.c \
                            lib/vec.c \
                            lib/vec_xc.c \
                            lib/user_space/finject_init.c \
@@ -101,7 +103,8 @@ mero_libmero_la_SOURCES += lib/assert.c \
                            lib/user_space/umisc.c \
                            lib/user_space/uthread.c \
                            lib/user_space/utime.c \
-                           lib/user_space/utrace.c
+                           lib/user_space/utrace.c \
+                           lib/user_space/varr.c
 
 XC_FILES += lib/buf_xc.h \
             lib/cookie_xc.h \
diff --git a/lib/arith.h b/lib/arith.h
index 9373e4f..e93cda8 100644
--- a/lib/arith.h
+++ b/lib/arith.h
@@ -216,6 +216,13 @@ static bool inline m0_addu64_will_overflow(uint64_t a, uint64_t b)
 	return a + b < a;
 }
 
+/**
+ * Finds out yth power of x.
+ * This computation can be expensive to run. Use with caution!
+ * Caller should take care overflow of resultant value.
+ */
+M0_INTERNAL uint64_t m0_pow(uint64_t x, uint64_t y);
+
 /** @} end of arith group */
 
 /* __MERO_LIB_ARITH_H__ */
diff --git a/lib/linux_kernel/varr.c b/lib/linux_kernel/varr.c
index c89e5dd..fda883f 100644
--- a/lib/linux_kernel/varr.c
+++ b/lib/linux_kernel/varr.c
@@ -18,402 +18,41 @@
  * Original creation date: 12/17/2012
  */
 
-#include "lib/linux_kernel/varr.h" /* m0_varr */
+#include "lib/varr.h"	/* m0_varr */
 #include "lib/bob.h"	/* m0_bob_type */
-#include "lib/memory.h"
-#include "lib/misc.h"	/* m0_forall */
-#include "lib/errno.h"	/* Includes appropriate errno header. */
 #include "lib/types.h"	/* Includes appropriate types header. */
-#include "lib/trace.h"	/* M0_ENTRY() */
+#include <linux/pagemap.h>
 
-static const struct m0_bob_type varr_bobtype;
-
-M0_BOB_DEFINE(static, &varr_bobtype, m0_varr);
-
-static const struct m0_bob_type varr_bobtype = {
-	.bt_name         = "generic_array_bobtype",
-	.bt_magix_offset = offsetof(struct m0_varr, va_magic),
-	.bt_magix        = M0_LIB_GENARRAY_MAGIC,
-	.bt_check        = NULL,
-};
-
-M0_INTERNAL int level_find(unsigned long pg);
-static bool varr_invariant(const struct m0_varr *arr);
-static int varr_ln_alloc(struct m0_varr *arr,
-			     unsigned long    nr,
-			     int              level);
-static int varr_l0_alloc(struct m0_varr *arr,
-			     unsigned long   *addr);
-static int varr_l1_alloc(struct m0_varr *arr,
-			     unsigned long   *addr,
-			     unsigned long    nr);
-static int varr_l2_alloc(struct m0_varr *arr,
-			     unsigned long   *addr,
-			     unsigned long    nr);
-static void varr_ln_free(struct m0_varr *arr,
-			     unsigned long   *addr,
-			     int              level);
-static unsigned long buff_nr_in_ln(const struct m0_varr *arr,
-				   int                    level);
-static void varr_l0_free(struct m0_varr *arr,
-			 unsigned long   *addr);
-static void varr_l1_free(struct m0_varr *arr,
-			 unsigned long   *addr);
-static void varr_l2_free(struct m0_varr *arr,
-			 unsigned long   *addr);
-M0_INTERNAL void varr_fini(struct m0_varr *arr);
-M0_INTERNAL unsigned long buff_index_in_level_n(unsigned long pg,
-						int           level);
-static unsigned long buff_nr_left_after_level_n(unsigned long pg,
-						int           level);
-
-M0_INTERNAL unsigned long cont_nr_for_objs(unsigned long nr,
-					   unsigned long obj_nr_in_1_cont);
-
-M0_INTERNAL unsigned long varr_obj_nr_in_buff(const struct m0_varr *arr)
-{
-	M0_PRE(arr != NULL);
-	M0_PRE(arr->va_sizeof > 0);
-
-	return PAGE_CACHE_SIZE / arr->va_sizeof;
-}
-
-M0_INTERNAL unsigned long cont_nr_for_objs(unsigned long nr,
-					   unsigned long obj_nr_in_1_cont)
-{
-	M0_PRE(obj_nr_in_1_cont > 0);
-
-	return nr / obj_nr_in_1_cont + (nr % obj_nr_in_1_cont == 0 ? 0 : 1);
-}
-
-static bool varr_invariant(const struct m0_varr *arr)
-{
-	return
-		m0_varr_bob_check(arr) &&
-		arr->va_nr > 0 &&
-		arr->va_alloc > 0 &&
-		arr->va_sizeof > 0 &&
-		M0_0VEC_ALIGN % arr->va_sizeof == 0 &&
-		m0_forall(i, level_find(cont_nr_for_objs(arr->va_nr,
-			  varr_obj_nr_in_buff(arr))),
-			  (unsigned long *)arr->va_levels[i] != NULL);
-}
-
-M0_INTERNAL int varr_init(struct m0_varr *arr,
-			  uint64_t        nr,
-			  size_t          size)
-{
-	int            rc = 0;
-	int            level;
-	unsigned long  buff_nr;
-	int            level_nr;
-
-	M0_PRE(arr != NULL);
-	M0_PRE(nr   > 0);
-	M0_PRE(size > 0);
-	M0_PRE(PAGE_CACHE_SIZE % size == 0);
-
-	/*
-	 * Since two successive buffs are not guaranteed to be contiguous,
-	 * structures bigger than page size can't fit in such array since
-	 * any attempt to dereference structure members can go over a
-	 * page size and can fault the program.
-	 */
-	if (nr > (M0_0VEC_ALIGN / size * PAGE_NR_TILL_LEVEL2) ||
-	    size > M0_0VEC_ALIGN)
-		return -EINVAL;
-
-	arr->va_nr     = nr;
-	arr->va_alloc  = arr->va_dealloc = 0;
-	arr->va_sizeof = size;
-	arr->va_levels[VA_LEVEL0] = arr->va_levels[VA_LEVEL1] =
-		arr->va_levels[VA_LEVEL2] = 0;
-	m0_varr_bob_init(arr);
-
-	buff_nr  = cont_nr_for_objs(nr, varr_obj_nr_in_buff(arr));
-	level_nr = level_find(buff_nr);
-
-	for (level = VA_LEVEL0; level <= level_nr; ++level) {
-		rc = varr_ln_alloc(arr, buff_nr_in_ln(arr, level), level);
-		if (rc != 0)
-			break;
-	}
-
-	if (rc != 0)
-		varr_fini(arr);
-	else
-		M0_POST_EX(varr_invariant(arr));
-	return rc;
-}
-
-M0_INTERNAL void varr_fini(struct m0_varr *arr)
-{
-	int l;
-	int level;
-
-	M0_PRE(arr != NULL);
-	M0_PRE_EX(varr_invariant(arr));
-
-	level = level_find(arr->va_nr / varr_obj_nr_in_buff(arr));
-	for (l = 0; l <= level; ++l)
-		varr_ln_free(arr, &arr->va_levels[l], l);
-
-	M0_POST(arr->va_alloc == arr->va_dealloc);
-	m0_varr_bob_fini(arr);
-	arr->va_nr = 0;
-	arr->va_sizeof = 0;
-	arr->va_alloc = arr->va_dealloc = 0;
-}
-
-M0_INTERNAL unsigned long *varr_buff(const struct m0_varr *arr,
-				     unsigned long         id)
-{
-	int            level;
-	unsigned long  pg;
-	unsigned long *pgptr;
-
-	M0_PRE(arr != NULL);
-	M0_PRE(id  <  arr->va_nr);
-
-	pg    = id / varr_obj_nr_in_buff(arr);
-	level = level_find(pg);
-	pgptr = (unsigned long *)arr->va_levels[level];
-
-	if (level == VA_LEVEL0)
-		return pgptr;
-
-	for (; level > 0; --level) {
-		pgptr += buff_index_in_level_n(pg, level);
-		pg     = buff_nr_left_after_level_n(pg, level);
-		/* Dereferences the buffer pointer at given offset. */
-		M0_ASSERT((unsigned long *)*pgptr != NULL);
-		pgptr  = (unsigned long *)*pgptr;
-	}
-
-	M0_POST_EX(varr_invariant(arr));
-	return pgptr;
-}
-
-static void varr_l0_free(struct m0_varr *arr,
-			     unsigned long   *addr)
-{
-	if (*addr != 0) {
-		free_page(*addr);
-		*addr = 0;
-		++arr->va_dealloc;
-	}
-}
-
-static void varr_l1_free(struct m0_varr *arr,
-			     unsigned long   *addr)
-{
-	unsigned long id;
-	unsigned long *ptr;
-
-	M0_PRE(arr  != NULL);
-	M0_PRE(addr != NULL);
-
-	if (*addr != 0) {
-		for (id = 0; id < M0_0VEC_ALIGN / sizeof id; ++id) {
-			ptr = (unsigned long *)*addr;
-			ptr += id;
-			varr_l0_free(arr, ptr);
-		}
-		varr_l0_free(arr, addr);
-	}
-}
-
-static void varr_l2_free(struct m0_varr *arr,
-			     unsigned long   *addr)
-{
-	unsigned long  id;
-	unsigned long *tier1;
-
-	M0_PRE(arr  != NULL);
-	M0_PRE(addr != NULL);
-
-	if (arr->va_levels[VA_LEVEL2] != 0) {
-		tier1 = (unsigned long *)(arr->va_levels[2]);
-		for (id = 0; id < (M0_0VEC_ALIGN / sizeof id) && *tier1 != 0;
-		     ++id, tier1++)
-			varr_l1_free(arr, tier1);
-		varr_l0_free(arr, &arr->va_levels[VA_LEVEL2]);
-	}
-}
-
-static void varr_ln_free(struct m0_varr *arr,
-			     unsigned long   *addr,
-			     int              level)
-{
-	M0_PRE(arr  != NULL);
-	M0_PRE(addr != NULL);
-	M0_PRE(level >= VA_LEVEL0 && level < VA_LEVEL_NR);
-
-	return
-		level == VA_LEVEL0 ? varr_l0_free(arr, addr) :
-		level == VA_LEVEL1 ? varr_l1_free(arr, addr) :
-		varr_l2_free(arr, addr);
-}
-
-static int varr_l0_alloc(struct m0_varr *arr,
-			     unsigned long   *addr)
-{
-	M0_PRE(arr  != NULL);
-	M0_PRE(addr != NULL);
-
-	*addr = (unsigned long)get_zeroed_page(GFP_KERNEL);
-	if ((unsigned long *)*addr != NULL) {
-		++arr->va_alloc;
-		return 0;
-	}
-	return -ENOMEM;
-}
-
-static int varr_l1_alloc(struct m0_varr *arr,
-			     unsigned long   *addr,
-			     unsigned long    nr)
-{
-	int            rc;
-	unsigned long  id;
-	unsigned long *pg;
-
-	M0_PRE(arr  != NULL);
-	M0_PRE(addr != NULL);
-	M0_PRE(nr    > 0);
-
-	*addr = (unsigned long)get_zeroed_page(GFP_KERNEL);
-	if ((unsigned long *)*addr == NULL)
-		return -ENOMEM;
-	++arr->va_alloc;
-	pg = (unsigned long *)*addr;
-	for (id = 0; id < nr; ++id, ++pg) {
-		rc = varr_l0_alloc(arr, pg);
-		if (rc != 0)
-			break;
-	}
-	return rc;
-}
-
-static int varr_l2_alloc(struct m0_varr *arr,
-			     unsigned long   *addr,
-			     unsigned long    nr)
-{
-	int            rc;
-	unsigned long  n;
-	unsigned long  total;
-	unsigned long *pg;
-
-	M0_PRE(arr  != NULL);
-	M0_PRE(addr != NULL);
-	M0_PRE(nr    > 0);
-
-	*addr = (unsigned long)get_zeroed_page(GFP_KERNEL);
-	if ((unsigned long *)*addr == NULL)
-		return -ENOMEM;
-
-	++arr->va_alloc;
-	pg = (unsigned long *)*addr;
-	for (total = 0; total < nr; total += n, ++pg) {
-		M0_ASSERT((unsigned long *)*pg == NULL);
-		n  = min64u(nr - total, PAGE_NR_IN_LEVEL1);
-		rc = varr_l1_alloc(arr, pg, n);
-		if (rc != 0)
-			break;
-	}
-	return rc;
-}
-
-static int varr_ln_alloc(struct m0_varr *arr,
-			     unsigned long    nr,
-			     int              level)
-{
-	M0_PRE(arr != NULL);
-	M0_PRE(nr   > 0);
-	M0_PRE(level >= VA_LEVEL0 && level <= VA_LEVEL_NR);
-
-	return
-		level == VA_LEVEL0 ?
-			varr_l0_alloc(arr, &arr->va_levels[VA_LEVEL0]) :
-		level == VA_LEVEL1 ?
-			varr_l1_alloc(arr, &arr->va_levels[VA_LEVEL1], nr) :
-		varr_l2_alloc(arr, &arr->va_levels[VA_LEVEL2], nr);
-}
-
-static uint64_t varr_obj_nr_in_l0(const struct m0_varr *arr)
-{
-	return min64u(arr->va_nr, varr_obj_nr_in_buff(arr));
-}
-
-static uint64_t varr_obj_nr_in_l1(const struct m0_varr *arr)
-{
-	return min64u(arr->va_nr - varr_obj_nr_in_l0(arr),
-		      PAGE_NR_IN_LEVEL1 * varr_obj_nr_in_buff(arr));
-}
-
-static uint64_t varr_obj_nr_till_l1(const struct m0_varr *arr)
+M0_EXTERN void *m0_varr_buf_alloc(int bufsize)
 {
-	return varr_obj_nr_in_l0(arr) + varr_obj_nr_in_l1(arr);
-}
+	M0_PRE(bufsize == PAGE_CACHE_SIZE);
 
-static uint64_t varr_obj_nr_in_l2(const struct m0_varr *arr)
-{
-	return min64u(arr->va_nr - varr_obj_nr_till_l1(arr),
-		      PAGE_NR_IN_LEVEL2 * varr_obj_nr_in_buff(arr));
+	return (void *)get_zeroed_page(GFP_KERNEL);
 }
 
-M0_INTERNAL uint64_t varr_obj_nr_in_ln(const struct m0_varr *arr,
-					   int                    level)
+M0_EXTERN void m0_varr_buf_free(void *buf)
 {
-	M0_PRE(arr != NULL);
-	M0_PRE(level >= VA_LEVEL0 && level <= VA_LEVEL2);
+	M0_PRE(buf != NULL);
 
-	return
-		level == 0 ? varr_obj_nr_in_l0(arr) :
-		level == 1 ? varr_obj_nr_in_l1(arr) :
-		varr_obj_nr_in_l2(arr);
+	free_page((unsigned long)buf);
 }
 
-static unsigned long buff_nr_in_ln(const struct m0_varr *arr,
-				   int                    level)
+M0_EXTERN bool m0_varr_size_is_valid(const struct m0_varr *arr)
 {
 	M0_PRE(arr != NULL);
-	M0_PRE(level >= VA_LEVEL0 && level <= VA_LEVEL_NR);
 
 	return
-		level == VA_LEVEL0 ? PAGE_NR_IN_LEVEL0 :
-		level == VA_LEVEL1 ?
-		min64u(cont_nr_for_objs(varr_obj_nr_in_l1(arr),
-				        varr_obj_nr_in_buff(arr)),
-		       PAGE_NR_IN_LEVEL1) :
-		min64u(cont_nr_for_objs(varr_obj_nr_in_l2(arr),
-				        varr_obj_nr_in_buff(arr)),
-		       PAGE_NR_IN_LEVEL2);
+		arr->va_sizeof  <  arr->va_bufsize &&
+		arr->va_bufsize == PAGE_CACHE_SIZE &&
+		arr->va_bufsize %  arr->va_sizeof == 0;
 }
 
-M0_INTERNAL unsigned long buff_index_in_level_n(unsigned long    pg,
-						int              level)
-{
-	M0_PRE(level > 0 && level < VA_LEVEL_NR);
-
-	return
-		level == VA_LEVEL1 ? pg > 0 ? pg - PAGE_NR_IN_LEVEL0 : pg :
-		(pg - PAGE_NR_TILL_LEVEL1) / PAGE_NR_IN_LEVEL1;
-}
-
-static unsigned long buff_nr_left_after_level_n(unsigned long pg,
-						int           level)
-{
-	M0_PRE(level > VA_LEVEL0 && level < VA_LEVEL_NR);
-
-	return level == VA_LEVEL2 ?
-		((pg - PAGE_NR_TILL_LEVEL1) % PAGE_NR_IN_LEVEL1) + 1 : pg;
-}
-
-M0_INTERNAL int level_find(unsigned long pg)
-{
-	M0_PRE(pg < PAGE_NR_TILL_LEVEL2);
-
-	return
-	       pg < PAGE_NR_IN_LEVEL0 ? VA_LEVEL0 :
-	       pg < PAGE_NR_TILL_LEVEL1 ? VA_LEVEL1 : VA_LEVEL2;
-}
+/*
+ *  Local variables:
+ *  c-indentation-style: "K&R"
+ *  c-basic-offset: 8
+ *  tab-width: 8
+ *  fill-column: 80
+ *  scroll-step: 1
+ *  End:
+ */
diff --git a/lib/linux_kernel/varr.h b/lib/linux_kernel/varr.h
deleted file mode 100644
index a6867ee..0000000
--- a/lib/linux_kernel/varr.h
+++ /dev/null
@@ -1,292 +0,0 @@
-/* -*- C -*- */
-/*
- * COPYRIGHT 2013 XYRATEX TECHNOLOGY LIMITED
- *
- * THIS DRAWING/DOCUMENT, ITS SPECIFICATIONS, AND THE DATA CONTAINED
- * HEREIN, ARE THE EXCLUSIVE PROPERTY OF XYRATEX TECHNOLOGY
- * LIMITED, ISSUED IN STRICT CONFIDENCE AND SHALL NOT, WITHOUT
- * THE PRIOR WRITTEN PERMISSION OF XYRATEX TECHNOLOGY LIMITED,
- * BE REPRODUCED, COPIED, OR DISCLOSED TO A THIRD PARTY, OR
- * USED FOR ANY PURPOSE WHATSOEVER, OR STORED IN A RETRIEVAL SYSTEM
- * EXCEPT AS ALLOWED BY THE TERMS OF XYRATEX LICENSES AND AGREEMENTS.
- *
- * YOU SHOULD HAVE RECEIVED A COPY OF XYRATEX'S LICENSE ALONG WITH
- * THIS RELEASE. IF NOT PLEASE CONTACT A XYRATEX REPRESENTATIVE
- * http://www.xyratex.com/contact
- *
- * Original author: Anand Vidwansa <anand_vidwansa@xyratex.com>
- * Original creation date: 12/17/2012
- */
-
-#pragma once
-
-#ifndef __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__
-#define __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__
-
-#include "lib/vec.h"
-
-#include <linux/pagemap.h> /* PAGE_CACHE_SIZE */
-
-/**
- * Represents a virtual array which is meant to be used in kernel space.
- * Typically, linux kernel page allocator does not guarantee allocation
- * of contiguous pages since the address space could be fragmented.
- *
- * It is left to applications to work around this problem.
- * There are 2 possible approaches to resolve this issue.
- * - use vmalloc() instead of kmalloc() OR
- * - use a virtual array by  maintaining set of pages
- *   (contiguous/discontiguous) and doing the indexing manually.
- *
- * Often, using vmalloc() is expensive since process page tables have to be
- * rearranged in order to to make the memory look like it is contiguous
- * even if it is not.
- * This can mess with processor TLB (translation lookaside buffer) cache
- * which maintains the latest mapping of virtual to physical addresses,
- * which adds to performance degradation.
- *
- * The second approach is to use a virtual array with a set of kernel pages
- * (physical contiguity of pages does not matter) and using manual indexing
- * to iterate over the set of pages.
- *
- * Advantages:
- * - Fairly inexpensive as compared to vmalloc() since no page tables need
- *   to be updated.
- * - An indexing scheme desirable to application can be used.
- * - Can do boundary checking (due to use of metadata about array)
- *
- * Disadvantages:
- * - Existing code which assumes contiguous allocation can not be used as is.
- * - New APIs have to be written to read/write from/to array.
- * - Mimimum one page must be allocated since granularity of allocation is
- *   a page.
- *
- * Here the virtual array approach is adopted since it's lightweight and
- * flexible.
- *
- * The structure of virtual array is kept something similar to a block map
- * from an on-disk inode which multiple indirections.
- *
- * The current implementation uses 2 levels of indirections as follows.
- * - Maintains array of 3 pages where
- *   - No indirection: First page is used as is, for memory worth
- *     PAGE_CACHE_SIZE (total 4K).
- *
- *   - First indirection: Second page is used to store addresses of pages,
- *     each of which will store PAGE_CACHE_SIZE worth of data (total 2M).
- *
- *   - Second indirection: Third page is used to store pages, each of which
- *     can hold data worth first indirection (total 1G).
- *
- * In all, this implementation can allocate up to 262657 pages.
- * For any allocation requests bigger than this number, an error code
- * is returned.
- */
-
-struct m0_varr;
-
-enum {
-	VA_LEVEL0 = 0,
-	VA_LEVEL1,
-	VA_LEVEL2,
-	VA_LEVEL_NR,
-	PAGE_NR_IN_LEVEL0   = 1,
-	PAGE_NR_IN_LEVEL1   = PAGE_CACHE_SIZE / sizeof (unsigned long),
-	//PAGE_NR_IN_LEVEL1   = M0_0VEC_ALIGN / sizeof (unsigned long),
-	PAGE_NR_IN_LEVEL2   = PAGE_NR_IN_LEVEL1 * PAGE_NR_IN_LEVEL1,
-
-	/** Number of buffers till end of level 1. */
-	PAGE_NR_TILL_LEVEL1 = PAGE_NR_IN_LEVEL0 + PAGE_NR_IN_LEVEL1,
-
-	/** Number of buffers till end of level 2. */
-	PAGE_NR_TILL_LEVEL2 = PAGE_NR_TILL_LEVEL1 + PAGE_NR_IN_LEVEL2,
-};
-
-struct m0_varr {
-	/** Number of elements in array. */
-	uint64_t      va_nr;
-
-	/** Size of object type stored in m0_varr. */
-	size_t        va_sizeof;
-
-	/**
-	 * Page levels according to size of array.
-	 * Only the levels which fall within size of array are allocated.
-	 */
-	unsigned long va_levels[VA_LEVEL_NR];
-
-	/** Debug field - Number of buffers allocated. */
-	unsigned long va_alloc;
-
-	/** Debug field - Number of buffers deallocated. */
-	unsigned long va_dealloc;
-
-	/** Magic field to cross check sanity of structure. */
-	uint64_t      va_magic;
-};
-
-/**
- * Initialises a virtual array.
- * @param nr   Length of array.
- * @param size Size of object to be stored in array.
- * @pre   arr != NULL && nr > 0.
- * @post  varr_invariant(arr) == true.
- */
-M0_INTERNAL int varr_init(struct m0_varr *arr,
-			  uint64_t        nr,
-			  size_t          size);
-
-/** Returns number of objects that can fit in one buffer. */
-M0_INTERNAL unsigned long varr_obj_nr_in_buff(const struct m0_varr *arr);
-
-/** Returns the level till which objects are accommodated in m0_varr. */
-M0_INTERNAL int level_find(unsigned long pg);
-
-/** Finds out index in given level for given buffer index. */
-M0_INTERNAL unsigned long buff_index_in_level_n(unsigned long    pg,
-						int              level);
-
-/** Returns number of objects contained in given level. */
-M0_INTERNAL uint64_t varr_obj_nr_in_ln(const struct m0_varr *arr,
-					   int                    level);
-
-/**
- * Returns the buffer in which given object with index id falls.
- * @pre  arr != NULL && id < arr->va_nr.
- * @post varr_invariant(arr).
- */
-M0_INTERNAL unsigned long *varr_buff(const struct m0_varr *arr,
-					 unsigned long          id);
-
-#define varr_ele_get(arr, type, index)				\
-({								\
-	type          *__ptr;					\
-								\
-	__ptr  = (type *)varr_buff(arr, index);			\
-	__ptr += index % varr_obj_nr_in_buff(arr);		\
-	M0_POST(__ptr != NULL);					\
-	__ptr;							\
-})
-
-/**
- * Returns the number of container buffers needed to accommodate nr
- * objects from virtual array.
- * @pre obj_nr_in_1_cont > 0.
- */
-M0_INTERNAL unsigned long cont_nr_for_objs(unsigned long nr,
-					   unsigned long obj_nr_in_1_cont);
-
-#define varr_buffs_for_obj_n(arr, nr, level)		\
-cont_nr_for_objs(nr, varr_obj_nr_in_ln(arr, level))
-
-#define varr_for_l0(arr, var, addr, nr, ...)				\
-({									\
-	uint64_t    id;							\
-	typeof(var) var;						\
-									\
-	var = (typeof(var))(addr);					\
-	for (id = 0; id < (nr); ++id, ++var) {				\
-		if (!({ __VA_ARGS__ ; }))				\
-			break;						\
-	}								\
-	id == (nr);							\
-})
-
-#define varr_for_l1(arr, var, addr, nr, ...)			\
-({									\
-	uint64_t       l   = 0;						\
-	uint64_t       _t1 = 0;						\
-	uint64_t       _nr = 0;						\
-	unsigned long *_l0 = (unsigned long *)(addr);			\
-	unsigned long *_l1;						\
-									\
-	for (; l < varr_buffs_for_obj_n(arr, nr, VA_LEVEL0); ++l, ++_l0) {	\
-		_nr = min64u(nr - _t1, varr_obj_nr_in_ln(arr, VA_LEVEL0));\
-		_l1 = (unsigned long *)*_l0;\
-		if (!varr_for_l0(arr, var, _l1, _nr, __VA_ARGS__))	\
-			break;						\
-		_t1 += _nr;						\
-	}								\
-	_t1 == (nr);							\
-})
-
-#define varr_for_l2(arr, var, addr, nr, ...)			\
-({									\
-	uint64_t       l   = 0;						\
-	uint64_t       _nr = 0;						\
-	uint64_t       _t2 = 0;						\
-	unsigned long *_l1 = (unsigned long *)(addr);			\
-	unsigned long *_l2;						\
-									\
-	for (; l < varr_buffs_for_obj_n(arr, nr, VA_LEVEL1); ++l, ++_l1) {\
-		_nr = min64u(nr - _t2, varr_obj_nr_in_ln(arr, VA_LEVEL1));\
-		_l2 = (unsigned long *)*_l1;				\
-		if (!varr_for_l1(arr, var, _l2, _nr, __VA_ARGS__))	\
-			break;						\
-		_t2 += _nr;						\
-	}								\
-	_t2 == (nr);							\
-})
-
-/**
- * Iterates over all objects stored in m0_varr performing same
- * operations for every object.
- * This can be used in invariant checking.
- */
-#define varr_forall(arr, var, type, ...)				\
-({									\
-	bool           cond;						\
-	uint64_t       nr;						\
-	uint64_t       total = 0;					\
-	unsigned long  lev;						\
-	unsigned long *addr;						\
-	type          *var;						\
-	struct m0_varr *a = (arr);					\
-									\
-	lev  = level_find(a->va_nr / varr_obj_nr_in_buff(a));	\
-	addr = (unsigned long *)a->va_levels[VA_LEVEL0];		\
-	nr   = varr_obj_nr_in_ln(a, VA_LEVEL0);			\
-	if (VA_LEVEL0 <= lev) {						\
-		cond = varr_for_l0(a, var, addr, nr,		\
-				       ({ __VA_ARGS__; }));		\
-		if (cond)						\
-			total += nr;					\
-	}								\
-									\
-	addr = (unsigned long *)a->va_levels[VA_LEVEL1];		\
-	nr   = varr_obj_nr_in_ln(a, VA_LEVEL1);			\
-	if (cond && VA_LEVEL1 <= lev) {					\
-		cond = varr_for_l1(a, var, addr, nr,		\
-				       ({ __VA_ARGS__; }));		\
-		if (cond)						\
-			total += nr;					\
-	}								\
-									\
-	addr = (unsigned long *)a->va_levels[VA_LEVEL2];		\
-	nr   = varr_obj_nr_in_ln(a, VA_LEVEL2);			\
-	if (cond && VA_LEVEL2 <= lev) {					\
-		cond = varr_for_l2(a, var, addr, nr,		\
-				       ({ __VA_ARGS__; }));		\
-		if (cond)						\
-			total += nr;					\
-	}								\
-	total == a->va_nr;						\
-})
-
-/**
- * Finalises a virtual array.
- * @pre varr_invariant(arr) == true
- */
-M0_INTERNAL void varr_fini(struct m0_varr *arr);
-
-#endif /* __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__ */
-
-/*
- *  Local variables:
- *  c-indentation-style: "K&R"
- *  c-basic-offset: 8
- *  tab-width: 8
- *  fill-column: 80
- *  scroll-step: 1
- *  End:
- */
diff --git a/lib/misc.c b/lib/misc.c
index 8c672b8..8882884 100644
--- a/lib/misc.c
+++ b/lib/misc.c
@@ -204,6 +204,18 @@ M0_INTERNAL const char *m0_short_file_name(const char *fname)
 	return fname;
 }
 
+M0_INTERNAL uint64_t m0_pow(uint64_t x, uint64_t y)
+{
+	uint64_t i;
+	uint64_t pow = 1;
+
+	M0_PRE(x > 0);
+
+	for (i = 0; i < y; ++i)
+		pow = pow * x;
+	return pow;
+}
+
 M0_INTERNAL const char *m0_failed_condition;
 M0_EXPORTED(m0_failed_condition);
 
diff --git a/lib/user_space/varr.c b/lib/user_space/varr.c
new file mode 100644
index 0000000..bdb4673
--- /dev/null
+++ b/lib/user_space/varr.c
@@ -0,0 +1,57 @@
+/* -*- C -*- */
+/*
+ * COPYRIGHT 2013 XYRATEX TECHNOLOGY LIMITED
+ *
+ * THIS DRAWING/DOCUMENT, ITS SPECIFICATIONS, AND THE DATA CONTAINED
+ * HEREIN, ARE THE EXCLUSIVE PROPERTY OF XYRATEX TECHNOLOGY
+ * LIMITED, ISSUED IN STRICT CONFIDENCE AND SHALL NOT, WITHOUT
+ * THE PRIOR WRITTEN PERMISSION OF XYRATEX TECHNOLOGY LIMITED,
+ * BE REPRODUCED, COPIED, OR DISCLOSED TO A THIRD PARTY, OR
+ * USED FOR ANY PURPOSE WHATSOEVER, OR STORED IN A RETRIEVAL SYSTEM
+ * EXCEPT AS ALLOWED BY THE TERMS OF XYRATEX LICENSES AND AGREEMENTS.
+ *
+ * YOU SHOULD HAVE RECEIVED A COPY OF XYRATEX'S LICENSE ALONG WITH
+ * THIS RELEASE. IF NOT PLEASE CONTACT A XYRATEX REPRESENTATIVE
+ * http://www.xyratex.com/contact
+ *
+ * Original author: Anand Vidwansa <anand_vidwansa@xyratex.com>
+ * Original creation date: 12/17/2012
+ */
+
+#include "lib/varr.h"	/* m0_varr */
+#include "lib/bob.h"	/* m0_bob_type */
+#include "lib/memory.h" /* m0_alloc, m0_free */
+#include "lib/types.h"	/* Includes appropriate types header. */
+
+M0_EXTERN void *m0_varr_buf_alloc(int bufsize)
+{
+	M0_PRE(bufsize > 0);
+
+	return m0_alloc(bufsize);
+}
+
+M0_EXTERN void m0_varr_buf_free(void *buf)
+{
+	M0_PRE(buf != NULL);
+
+	m0_free(buf);
+}
+
+M0_EXTERN bool m0_varr_size_is_valid(const struct m0_varr *arr)
+{
+	M0_PRE(arr != NULL);
+
+	return
+		arr->va_sizeof  < arr->va_bufsize &&
+		arr->va_bufsize % arr->va_sizeof == 0;
+}
+
+/*
+ *  Local variables:
+ *  c-indentation-style: "K&R"
+ *  c-basic-offset: 8
+ *  tab-width: 8
+ *  fill-column: 80
+ *  scroll-step: 1
+ *  End:
+ */
diff --git a/lib/ut/varr.c b/lib/ut/varr.c
index 6df1922..be504f6 100644
--- a/lib/ut/varr.c
+++ b/lib/ut/varr.c
@@ -18,7 +18,7 @@
  * Original creation date: 01/07/2013
  */
 
-#include "lib/linux_kernel/varr.h"
+#include "lib/varr.h"
 #include "lib/vec.h"
 #include "lib/memory.h"
 #include "ut/ut.h"
@@ -74,41 +74,33 @@ void test_varr(void)
 {
 	int                  rc;
 	unsigned long        id;
-	unsigned long        t2;
-	unsigned long        buff_nr;
+	//unsigned long        t2;
+	//unsigned long        buff_nr;
 	unsigned long        ele_nr_in_buff;
-	unsigned long        buff_id_in_l2;
-	unsigned long       *tier1;
-	unsigned long       *tier2;
-	unsigned long       *addr;
+	//unsigned long        buff_id_in_l2;
+	//unsigned long       *tier1;
+	//unsigned long       *tier2;
+	//unsigned long       *addr;
 	unsigned long       *val;
 	unsigned long        cnt;
 	struct foo          *f;
 	struct m0_varr       arr;
 
 	/* varr used to store ELEMENT_NR, 8 byte long objects. */
-	rc = varr_init(&arr, ELEMENT_NR, sizeof(unsigned long));
+	rc = varr_init(&arr, ELEMENT_NR, sizeof(unsigned long), M0_0VEC_ALIGN);
 
 	M0_UT_ASSERT(rc == 0);
 	M0_UT_ASSERT(arr.va_magic  == M0_LIB_GENARRAY_MAGIC);
 	M0_UT_ASSERT(arr.va_nr     == ELEMENT_NR);
 	M0_UT_ASSERT(arr.va_sizeof == sizeof(unsigned long));
 
-	M0_UT_ASSERT(arr.va_levels[VA_LEVEL0] != 0);
-	M0_UT_ASSERT(arr.va_levels[VA_LEVEL1] != 0);
-
 	ele_nr_in_buff = M0_0VEC_ALIGN / sizeof(unsigned long);
+	/*
 	for (id = 0; id < ele_nr_in_buff; ++id) {
 		tier1 = (unsigned long *)arr.va_levels[VA_LEVEL1] + id;
 		M0_UT_ASSERT(*tier1 != 0);
 	}
 
-	M0_UT_ASSERT(arr.va_levels[VA_LEVEL2] != 0);
-
-	buff_nr        = ELEMENT_NR / ele_nr_in_buff;
-	ELEMENT_NR % ele_nr_in_buff != 0 ?: ++buff_nr;
-	buff_id_in_l2  = (buff_nr - PAGE_NR_TILL_LEVEL1) / PAGE_NR_IN_LEVEL1;
-
 	for (id = 0; id <= buff_id_in_l2; ++id) {
 		tier1  = (unsigned long *)arr.va_levels[VA_LEVEL2];
 		tier1 += id;
@@ -129,6 +121,7 @@ void test_varr(void)
 		tier1 += id;
 		M0_UT_ASSERT(*tier1 == 0);
 	}
+	*/
 
 	for (id = 0; id < ELEMENT_NR; ++id) {
 		val = varr_ele_get(&arr, unsigned long, id);
@@ -140,6 +133,7 @@ void test_varr(void)
 		M0_UT_ASSERT(*val == id);
 	}
 
+	/*
 	tier1  = (unsigned long *)arr.va_levels[VA_LEVEL2];
 	tier1 += buff_index_in_level_n(buff_nr, VA_LEVEL2);
 	tier2  = (unsigned long *)*tier1;
@@ -147,10 +141,11 @@ void test_varr(void)
 	addr   = (unsigned long *)(*tier2 + (sizeof(unsigned long) *
 		 (ELEMENT_NR % varr_obj_nr_in_buff(&arr))));
 	M0_UT_ASSERT(*addr == 0);
+	*/
 	varr_fini(&arr);
 
 	/* Genarray used to store array of structures. */
-	rc = varr_init(&arr, struct_nr, sizeof *f);
+	rc = varr_init(&arr, struct_nr, sizeof *f, M0_0VEC_ALIGN);
 
 	/*
 	 * Total size to accommodate array of struct foo worth 64 bytes
@@ -161,7 +156,6 @@ void test_varr(void)
 	 * Pages till level 1 = 513.
 	 * Pages till level 2, sublevel (0 + 1 + 2) = 1929.
 	 * Number of meta buffers allocated = 5.
-	 */
 	M0_UT_ASSERT(rc == 0);
 	M0_UT_ASSERT(arr.va_nr == struct_nr);
 	M0_UT_ASSERT(arr.va_sizeof == sizeof *f);
@@ -169,6 +163,7 @@ void test_varr(void)
 	M0_UT_ASSERT((unsigned long *)arr.va_levels[1] != NULL);
 	M0_UT_ASSERT((unsigned long *)arr.va_levels[2] != NULL);
 	M0_UT_ASSERT(arr.va_alloc == 1929 + 5);
+	*/
 
 	M0_ALLOC_ARR(address_tracker, struct_nr);
 	M0_UT_ASSERT(address_tracker != NULL);
@@ -180,14 +175,14 @@ void test_varr(void)
 		address_tracker[id] = f;
 	}
 
-	M0_UT_ASSERT(!varr_forall(&arr, fint, struct foo,
-				      fint->f_8b == 12345));
+	/*M0_UT_ASSERT(!varr_forall(&arr, fint, struct foo,
+				      fint->f_8b == 12345));*/
 
 	for (id = 0; id < struct_nr; ++id)
 		M0_UT_ASSERT(address_tracker[id]->f_magic == FOO_MAGIC);
 
-	M0_UT_ASSERT(varr_forall(&arr, fint, struct foo,
-				     fint->f_magic == FOO_MAGIC));
+	/*M0_UT_ASSERT(varr_forall(&arr, fint, struct foo,
+				     fint->f_magic == FOO_MAGIC));*/
 
 	for (id = struct_nr - 1; id != 0; --id) {
 		f = varr_ele_get(&arr, struct foo, id);
diff --git a/lib/varr.c b/lib/varr.c
new file mode 100644
index 0000000..7b5091f
--- /dev/null
+++ b/lib/varr.c
@@ -0,0 +1,545 @@
+/* -*- C -*- */
+/*
+ * COPYRIGHT 2013 XYRATEX TECHNOLOGY LIMITED
+ *
+ * THIS DRAWING/DOCUMENT, ITS SPECIFICATIONS, AND THE DATA CONTAINED
+ * HEREIN, ARE THE EXCLUSIVE PROPERTY OF XYRATEX TECHNOLOGY
+ * LIMITED, ISSUED IN STRICT CONFIDENCE AND SHALL NOT, WITHOUT
+ * THE PRIOR WRITTEN PERMISSION OF XYRATEX TECHNOLOGY LIMITED,
+ * BE REPRODUCED, COPIED, OR DISCLOSED TO A THIRD PARTY, OR
+ * USED FOR ANY PURPOSE WHATSOEVER, OR STORED IN A RETRIEVAL SYSTEM
+ * EXCEPT AS ALLOWED BY THE TERMS OF XYRATEX LICENSES AND AGREEMENTS.
+ *
+ * YOU SHOULD HAVE RECEIVED A COPY OF XYRATEX'S LICENSE ALONG WITH
+ * THIS RELEASE. IF NOT PLEASE CONTACT A XYRATEX REPRESENTATIVE
+ * http://www.xyratex.com/contact
+ *
+ * Original author: Anand Vidwansa <anand_vidwansa@xyratex.com>
+ * Original creation date: 12/17/2012
+ */
+
+#include "lib/varr.h"	/* m0_varr */
+#include "lib/bob.h"	/* m0_bob_type */
+#include "lib/memory.h" /* M0_ALLOC_ARR */
+#include "lib/misc.h"	/* m0_forall */
+#include "lib/errno.h"	/* Includes appropriate errno header. */
+#include "lib/types.h"	/* Includes appropriate types header. */
+#include "lib/trace.h"	/* M0_ENTRY() */
+
+static const struct m0_bob_type varr_bobtype;
+
+M0_BOB_DEFINE(static, &varr_bobtype, m0_varr);
+
+static const struct m0_bob_type varr_bobtype = {
+	.bt_name         = "generic_array_bobtype",
+	.bt_magix_offset = offsetof(struct m0_varr, va_magic),
+	.bt_magix        = M0_LIB_GENARRAY_MAGIC,
+	.bt_check        = NULL,
+};
+
+M0_INTERNAL uint64_t index_in_level_n_pn(const struct m0_varr *arr,
+					 uint64_t              done,
+					 uint32_t              level);
+M0_INTERNAL uint32_t level_find(const struct m0_varr *arr, uint64_t pg);
+static bool varr_invariant(const struct m0_varr *arr);
+static int varr_buffers_alloc(struct m0_varr *arr,
+			      uint64_t        buff_nr);
+static void varr_buffers_dealloc(struct m0_varr *arr,
+				 uint64_t        buff_nr);
+static uint64_t max_buff_nr_till_lev_n_pn(const struct m0_varr *arr,
+					  uint32_t              level);
+M0_INTERNAL void varr_fini(struct m0_varr *arr);
+M0_INTERNAL uint64_t cont_nr_for_objs(unsigned long nr,
+				      unsigned long obj_nr_in_1_cont);
+
+/*
+ * A LIFO stack which maintains the backlink to parent node as well as
+ * the index of parent node in given buffer.
+ * The parent backlink helps traverse up the tree (from child to parent)
+ * and the index of parent node in given buffer helps identify the
+ * number of buffer beneath parent node in current traversal.
+ * User of this data structure typically maintains array of such structures
+ * to help move from leaf node to root node.
+ * Index of an element in the array is used an identifier to represent the
+ * level in the tree.
+ */
+struct varr_lifo_stack {
+	/* Backlink to parent node. */
+	void     *ls_parent;
+
+	/* Index of parent node in the buffer. */
+	uint64_t  ls_index;
+};
+
+/* Enumeration for action to be taken on a set of buffers. */
+enum buffer_action {
+	BA_ALLOC,
+	BA_DEALLOC,
+	BA_NR
+};
+
+M0_INTERNAL unsigned long varr_obj_nr_in_buff(const struct m0_varr *arr)
+{
+	M0_PRE(arr != NULL);
+	M0_PRE(arr->va_sizeof > 0);
+
+	return arr->va_bufsize / arr->va_sizeof;
+}
+
+M0_INTERNAL uint64_t cont_nr_for_objs(unsigned long nr,
+				      unsigned long obj_nr_in_1_cont)
+{
+	M0_PRE(obj_nr_in_1_cont > 0);
+
+	return nr / obj_nr_in_1_cont + (nr % obj_nr_in_1_cont == 0 ? 0 : 1);
+}
+
+static bool varr_invariant(const struct m0_varr *arr)
+{
+	return
+		m0_varr_bob_check(arr) &&
+		arr->va_nr > 0 &&
+		arr->va_alloc > 0 &&
+		arr->va_sizeof > 0 &&
+		M0_0VEC_ALIGN % arr->va_sizeof == 0;
+}
+
+static int m0_lookup_table_init(struct m0_lookup_table *table,
+				uint64_t                base,
+				uint64_t                len)
+{
+	int      rc = 0;
+	uint64_t exp;
+
+	M0_PRE(table != NULL);
+	M0_PRE(base > 0);
+	M0_PRE(len  > 0);
+
+	table->lt_base = base;
+	table->lt_len  = len;
+	M0_ALLOC_ARR(table->lt_values, len);
+	if (table->lt_values == NULL)
+		rc = -ENOMEM;
+
+	table->lt_values[0] = 1;
+	for (exp = 1; exp <= len; ++exp)
+		table->lt_values[exp] = m0_pow(table->lt_base, exp);
+	return rc;
+}
+
+static uint64_t m0_lookup_table_lookup(const struct m0_lookup_table *table,
+				       uint64_t                      base,
+				       uint64_t                      index)
+{
+	M0_PRE(table != NULL);
+	M0_PRE(base  == table->lt_base);
+	M0_PRE(index <= table->lt_len);
+	M0_PRE(table->lt_values != NULL);
+
+	return table->lt_values[index];
+}
+
+static void m0_lookup_table_fini(struct m0_lookup_table *table)
+{
+	m0_free(table->lt_values);
+	table->lt_base   = 0;
+	table->lt_len    = 0;
+	table->lt_values = NULL;
+}
+
+M0_INTERNAL int varr_init(struct m0_varr *arr,
+			  uint64_t        nr,
+			  size_t          size,
+			  int             bufsize)
+{
+	int      i;
+	int      rc = 0;
+	uint64_t buff_nr;
+
+	M0_PRE(arr != NULL);
+	M0_PRE(nr   > 0);
+	M0_PRE(size > 0);
+	M0_PRE(bufsize > 0);
+	M0_PRE(bufsize % size == 0);
+
+	/*
+	 * Since two successive buffs are not guaranteed to be contiguous,
+	 * structures bigger than page size can't fit in such array since
+	 * any attempt to dereference structure members can go over a
+	 * page size and can fault the program.
+	 */
+	if (!m0_varr_size_is_valid(arr))
+		return -EINVAL;
+
+	arr->va_nr        = nr;
+	arr->va_alloc     = arr->va_dealloc = 0;
+	arr->va_sizeof    = size;
+	arr->va_bufsize   = bufsize;
+	arr->va_bufptr_nr = arr->va_bufsize / VA_TNODEPTR_SIZE;
+	m0_varr_bob_init(arr);
+	for (i = 0; i < VA_TNODE_NR; ++i)
+		arr->va_tree[i] = NULL;
+
+	buff_nr         = cont_nr_for_objs(nr, varr_obj_nr_in_buff(arr));
+	arr->va_depth   = level_find(arr, buff_nr);
+
+	/* Builds the stack only if it is needed. */
+	if (arr->va_depth - 1 > 0) {
+		M0_ALLOC_ARR(arr->va_stack, arr->va_depth - 1);
+		if (arr->va_stack == NULL)
+			rc = -ENOMEM;
+	}
+
+	if (rc == 0) {
+		rc = m0_lookup_table_init(&arr->va_lktable, arr->va_bufptr_nr,
+					  arr->va_depth - 1);
+		if (rc == 0)
+			rc = varr_buffers_alloc(arr, buff_nr);
+	}
+
+	if (rc != 0)
+		varr_fini(arr);
+	else
+		M0_POST_EX(varr_invariant(arr));
+	return rc;
+}
+
+M0_INTERNAL void varr_fini(struct m0_varr *arr)
+{
+	M0_PRE(arr != NULL);
+	M0_PRE_EX(varr_invariant(arr));
+
+	varr_buffers_dealloc(arr, cont_nr_for_objs(arr->va_nr,
+			     varr_obj_nr_in_buff(arr)));
+
+	m0_lookup_table_fini(&arr->va_lktable);
+	M0_POST(arr->va_alloc == arr->va_dealloc);
+	m0_varr_bob_fini(arr);
+	arr->va_nr     = arr->va_bufsize = 0;
+	arr->va_depth  = 0;
+	arr->va_sizeof = 0;
+	arr->va_alloc  = arr->va_dealloc = 0;
+}
+
+M0_INTERNAL unsigned long *varr_buffer(const struct m0_varr *arr,
+				       uint64_t              index)
+{
+	uint32_t       level;
+	uint64_t       id;
+	uint64_t       buf;
+	unsigned long *holder;
+
+	M0_PRE(arr   != NULL);
+	M0_PRE(index <  arr->va_nr);
+
+	buf    = index / varr_obj_nr_in_buff(arr);
+	holder = arr->va_tree[buf / max_buff_nr_till_lev_n_pn(arr, 1)];
+	M0_ASSERT(holder != NULL);
+
+	for (level = 1; level <= arr->va_depth && buf != 0; ++level) {
+		id = index_in_level_n_pn(arr, buf, level);
+		holder += id;
+		if (level != 1)
+			buf -= max_buff_nr_till_lev_n_pn(arr, level) * id;
+		/* Dereferences the buffer pointer at given offset. */
+		M0_ASSERT((unsigned long *)*holder!= NULL);
+		holder = (unsigned long *)*holder;
+	}
+
+	M0_POST_EX(varr_invariant(arr));
+	return holder;
+}
+
+/* A helper function to factor out one or multiple buffer (de)allocation(s). */
+static int buffers_helper(struct m0_varr     *arr,
+			  void               *holder,
+			  uint64_t            nr,
+			  enum buffer_action  act)
+{
+	uint64_t       i;
+	unsigned long *h;
+
+	M0_PRE(arr    != NULL);
+	M0_PRE(holder != NULL);
+	M0_PRE(nr  > 0);
+	M0_PRE(act < BA_NR);
+	M0_CASSERT(sizeof h == sizeof holder);
+
+	for (i = 0; i < nr; ++i) {
+		h  = holder;
+		h += i;
+		if (act == BA_ALLOC) {
+			*h = (unsigned long)m0_varr_buf_alloc(arr->va_bufsize);
+			if ((unsigned long *)*h == NULL)
+				break;
+		} else {
+			M0_ASSERT((unsigned long *)*h != NULL);
+			m0_varr_buf_free((void *)*h);
+			*h = (unsigned long)NULL;
+		}
+	}
+	return i == nr ? 0 : -ENOMEM;
+}
+
+/*
+ * While deallocating buffers, the leaf node buffers are deallocated first
+ * since during indexing these pages account for initial members of
+ * virtual array.
+ * The approach is centered towards maintaining a LIFO stack per iteration 
+ * which will store parent of a given level while traversing from top down.
+ * The whole approach is taken in order to implement tree traversal in an
+ * iterative manner rather than using recursive functions.
+ * This approach is a bit similar to inorder traversal of a tree but
+ * not exactly the same.
+ */
+static void varr_buffers_dealloc(struct m0_varr *arr,
+				 uint64_t        buff_nr)
+{
+	int            rc;
+	int            node;
+	uint32_t       level;
+	uint64_t       nr;
+	uint64_t       done;
+	uint64_t       buff_nr_pn;
+	unsigned long *holder;
+
+	M0_PRE(arr     != NULL);
+	M0_PRE(buff_nr  > 0);
+
+	for (done = 0, node = 0; node < VA_TNODE_NR && done < buff_nr;
+	     ++node) {
+		holder = arr->va_tree[node];
+		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 0),
+				    buff_nr - done);
+		for (; done < buff_nr_pn; ) {
+
+			for (level = 1; level <= arr->va_depth &&
+			     done < buff_nr_pn; ++level) {
+
+				nr = min64u(level != arr->va_depth ? 1 :
+					    arr->va_bufptr_nr,
+					    buff_nr_pn - done);
+				if (level != arr->va_depth) {
+					arr->va_stack[level].ls_index  =
+						index_in_level_n_pn(arr, done,
+								level);
+					holder += arr->va_stack[level].ls_index;
+					arr->va_stack[level].ls_parent = holder;
+				}
+
+				/* We respect your privacy! */
+				if (level != arr->va_depth)
+					holder = (unsigned long *)*holder;
+			}
+
+			/* Deallocates the leaf node buffer(s). */
+			rc = buffers_helper(arr, holder, nr, BA_DEALLOC);
+			M0_ASSERT(rc == 0);
+			done += nr;
+
+			/* If the stack was built, start popping the stack. */
+			for (level = arr->va_depth - 1;
+			     level >= 1 && arr->va_stack != NULL; ++level) {
+
+				/*
+				 * Deallocates the holder buffer if all
+				 * buffers beneath this buffer in this trail
+				 * are already deallocated.
+				 */
+				if (max_buff_nr_till_lev_n_pn(arr, level) *
+				    (arr->va_stack[level].ls_index + 1) ==
+				    done % max_buff_nr_till_lev_n_pn(arr, 0)) {
+					rc = buffers_helper(arr, arr->
+							    va_stack[level].
+							    ls_parent, 1,
+							    BA_DEALLOC);
+					M0_ASSERT(rc == 0);
+				}
+				arr->va_stack[level].ls_parent = NULL;
+				arr->va_stack[level].ls_index  = 0;
+			}
+		}
+	}
+}
+
+/* Allocates buffers for given virtual array from level 0 to varr::va_depth. */
+static int varr_buffers_alloc(struct m0_varr *arr,
+			      uint64_t        buff_nr)
+{
+	int            rc;
+	int            node;
+	uint32_t       level;
+	uint64_t       nr;
+	/* Number of buffer allocated at any given moment. */
+	uint64_t       done;
+	/* Number of buffers per tree node. */
+	uint64_t       buff_nr_pn;
+	/*
+	 * Entity which holds address of some buffer, typically a part
+	 * of some meta-buffer.
+	 */
+	unsigned long *holder;
+
+	M0_PRE(arr     != NULL);
+	M0_PRE(buff_nr  > 0);
+
+	/*
+	 * While allocating a structure with multiple indirections,
+	 * the buffer allocations should be done in iterative manner
+	 * (as compared to recursive manner, which could lead to issues
+	 *  with limited stack size in kernel).
+	 * Only the buffers at last level (m0_varr::va_depth) contain
+	 * actual objects requested by end-user.
+	 * Buffers from rest of levels are meta-buffers, containing
+	 * pointers to other buffers from level beneath.
+	 * While traversing the tree, a trail is followed by dereferencing
+	 * a pointer on each level, except for the last level.
+	 */
+	for (done = 0, node = 0; node < VA_TNODE_NR && done < buff_nr;
+	     ++node) {
+		holder = arr->va_tree[node];
+		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 1),
+				    buff_nr - done);
+		for (; done < buff_nr_pn; ) {
+
+			for (level = 1; level <= arr->va_depth &&
+			     done < buff_nr_pn; ++level) {
+
+				/*
+				 * For all levels except the last one,
+				 * only 1 buffer will be allocated in
+				 * each level. Since only last level
+				 * holds data buffers, it can allocate
+				 * multiple buffers.
+				 */
+				nr = min64u(level != arr->va_depth ? 1 :
+					    arr->va_bufptr_nr,
+					    buff_nr_pn - done);
+
+				/*
+				 * For last level, the holding address is
+				 * fixed and need not be computed.
+				 */
+				if (level != arr->va_depth)
+					holder += index_in_level_n_pn(arr,
+							done, level);
+
+				/*
+				 * The loop travels same trail multiple times
+				 * until all buffers which fall in this trail
+				 * are allocated.
+				 * Meta-buffers in this trail could be already
+				 * allocated and no allocation is needed on
+				 * that level.
+				 */
+				if ((unsigned long *)*holder == NULL) {
+					rc = buffers_helper(arr, holder, nr,
+							    BA_ALLOC);
+					if (rc != 0)
+						goto end;
+					/*
+					 * Input buff_nr number is number of
+					 * _data_ buffers.
+					 */
+					if (level == arr->va_depth)
+						done += nr;
+				}
+				M0_ASSERT((unsigned long *)*holder != NULL);
+
+				/* We respect your privacy! */
+				if (level != arr->va_depth)
+					holder = (unsigned long *)*holder;
+			}
+		}
+	}
+end:
+	M0_POST(ergo(rc == 0, done == buff_nr));
+	return rc;
+}
+
+/*
+ * Returns the index in given buffer at given level when the tree traversal
+ * has reached @done nodes.
+ * The acronym _pn in API name stands for "per node".
+ */
+M0_INTERNAL uint64_t index_in_level_n_pn(const struct m0_varr *arr,
+					 uint64_t              done,
+					 uint32_t              level)
+{
+	M0_PRE(arr  != NULL);
+	M0_PRE(level < arr->va_depth);
+
+	return done / max_buff_nr_till_lev_n_pn(arr, level);
+}
+
+/*
+ * Returns max possible number of buffers for only a single tree node,
+ * that can fit till given level in virtual array.
+ * The acronym _pn in API name stands for "per node".
+ * Lower level number contains more buffers than higher level number.
+ * Level 0 is ancestor of level n (n > 0).
+ * TODO: A lookup table per m0_varr object would be helpful since
+ * during tree traversals and initialization, m0_pow() will be invoked
+ * every time which would be expensive.
+ */
+static uint64_t max_buff_nr_till_lev_n_pn(const struct m0_varr *arr,
+					  uint32_t              level)
+{
+	M0_PRE(level <= arr->va_depth);
+
+	/*return m0_pow(arr->va_bufsize / VA_TNODEPTR_SIZE,
+		      arr->va_depth - level);*/
+	return m0_lookup_table_lookup(&arr->va_lktable, arr->va_bufptr_nr,
+				      arr->va_depth - level);
+}
+
+/*
+ * Returns max possible number of _total_ buffers (for all, VA_TNODE_NR
+ * sibling nodes) that can fit till given level in virtual array.
+ */
+static uint64_t max_buff_nr_till_lev_n(const struct m0_varr *arr,
+				       uint32_t              level)
+{
+	M0_PRE(arr != NULL);
+	M0_PRE(arr->va_sizeof  > 0);
+	M0_PRE(arr->va_bufsize > 0);
+
+	return VA_TNODE_NR * max_buff_nr_till_lev_n_pn(arr, level);
+}
+
+/*
+ * Although, the radix tree deployed to hold buffers, is a fixed height
+ * tree, the number of levels of indirection within buffers can grow
+ * with increasing number of objects stored in array.
+ *
+ * For instance, these levels can be described as follows.
+ * - If cumulative size of array of objects falls within VA_TNODE_NR buffers
+ *   of m0_varr:va_bufsize each, m0_varr::va_depth becomes 1.
+ *
+ * - Level 2 holds VA_TNODE_NR * varr::va_bufsize / VA_TNODEPTR_SIZE.
+ *   If cumulative size of array falls within this limit, m0_varr:va_depth
+ *   becomes 2.
+ *
+ * - Level 3 holds VA_TNODE_NR tree nodes holding a buffer, which stores
+ *   multiple buffer pointers each, which in turn hold multiple
+ *   buffer pointers.
+ *   Level 3 holds VA_TNODE_NR * varr::va_bufsize * varr::va_bufsize /
+ *   (VA_TNODEPTR_SIZE * VA_TNODEPTR_SIZE).
+ *   If cumulative size of array of objects fall within this limit,
+ *   m0_varr:va_depth becomes 3.
+ * and so on.
+ */
+M0_INTERNAL uint32_t level_find(const struct m0_varr *arr,
+				uint64_t              pg)
+{
+	bool     found = false;
+	uint32_t level;
+
+	M0_PRE(arr != NULL);
+	M0_PRE(pg   > 0);
+
+	for (level = 1; !found; ++level)
+		if (pg < max_buff_nr_till_lev_n(arr, level))
+			found = true;
+	return level;
+}
diff --git a/lib/varr.h b/lib/varr.h
new file mode 100644
index 0000000..f2ebb55
--- /dev/null
+++ b/lib/varr.h
@@ -0,0 +1,227 @@
+/* -*- C -*- */
+/*
+ * COPYRIGHT 2013 XYRATEX TECHNOLOGY LIMITED
+ *
+ * THIS DRAWING/DOCUMENT, ITS SPECIFICATIONS, AND THE DATA CONTAINED
+ * HEREIN, ARE THE EXCLUSIVE PROPERTY OF XYRATEX TECHNOLOGY
+ * LIMITED, ISSUED IN STRICT CONFIDENCE AND SHALL NOT, WITHOUT
+ * THE PRIOR WRITTEN PERMISSION OF XYRATEX TECHNOLOGY LIMITED,
+ * BE REPRODUCED, COPIED, OR DISCLOSED TO A THIRD PARTY, OR
+ * USED FOR ANY PURPOSE WHATSOEVER, OR STORED IN A RETRIEVAL SYSTEM
+ * EXCEPT AS ALLOWED BY THE TERMS OF XYRATEX LICENSES AND AGREEMENTS.
+ *
+ * YOU SHOULD HAVE RECEIVED A COPY OF XYRATEX'S LICENSE ALONG WITH
+ * THIS RELEASE. IF NOT PLEASE CONTACT A XYRATEX REPRESENTATIVE
+ * http://www.xyratex.com/contact
+ *
+ * Original author: Anand Vidwansa <anand_vidwansa@xyratex.com>
+ * Original creation date: 12/17/2012
+ */
+
+#pragma once
+
+#ifndef __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__
+#define __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__
+
+#include "lib/vec.h"
+
+/**
+ * Represents a virtual array which is meant to be used in kernel space.
+ * Typically, linux kernel page allocator does not guarantee allocation
+ * of contiguous pages since the address space could be fragmented.
+ *
+ * It is left to applications to work around this problem.
+ * There are 2 possible approaches to resolve this issue.
+ * - use vmalloc() instead of kmalloc() OR
+ * - use a virtual array by  maintaining set of pages
+ *   (contiguous/discontiguous) and doing the indexing manually.
+ *
+ * Often, using vmalloc() is expensive since process page tables have to be
+ * rearranged in order to to make the memory look like it is contiguous
+ * even if it is not.
+ * This can mess with processor TLB (translation lookaside buffer) cache
+ * which maintains the latest mapping of virtual to physical addresses,
+ * which adds to performance degradation.
+ *
+ * The second approach is to use a virtual array with a set of kernel pages
+ * (physical contiguity of pages does not matter) and using manual indexing
+ * to iterate over the set of pages.
+ *
+ * Advantages:
+ * - Fairly inexpensive as compared to vmalloc() since no page tables need
+ *   to be updated.
+ * - An indexing scheme desirable to application can be used.
+ * - Can do boundary checking (due to use of metadata about array)
+ *
+ * Disadvantages:
+ * - Existing code which assumes contiguous allocation can not be used as is.
+ * - New APIs have to be written to read/write from/to array.
+ * - Mimimum one page must be allocated since granularity of allocation is
+ *   a page.
+ *
+ * Here the virtual array approach is adopted since it's lightweight and
+ * flexible.
+ *
+ * The structure of virtual array is kept something similar to a block map
+ * from an on-disk inode which multiple indirections.
+ *
+ * The current implementation uses 2 levels of indirections as follows.
+ * - Maintains array of 3 pages where
+ *   - No indirection: First page is used as is, for memory worth
+ *     PAGE_CACHE_SIZE (total 4K).
+ *
+ *   - First indirection: Second page is used to store addresses of pages,
+ *     each of which will store PAGE_CACHE_SIZE worth of data (total 2M).
+ *
+ *   - Second indirection: Third page is used to store pages, each of which
+ *     can hold data worth first indirection (total 1G).
+ *
+ * In all, this implementation can allocate up to 262657 pages.
+ * For any allocation requests bigger than this number, an error code
+ * is returned.
+ */
+
+struct m0_varr;
+struct varr_lifo_stack;
+
+M0_EXTERN void *m0_varr_buf_alloc(int   bufsize);
+M0_EXTERN void  m0_varr_buf_free(void *buf);
+M0_EXTERN bool  m0_varr_size_is_valid(const struct m0_varr *arr);
+
+enum {
+	/* Number of nodes which originate from root of radix tree. */
+	VA_TNODE_NR = 64,
+
+	/* Size of pointer to a tree node. */
+	VA_TNODEPTR_SIZE = sizeof(void *),
+};
+
+/**
+ * While initializing the virtual array and while iterating the radix tree
+ * in it, m0_pow() function is invoked multiple times which could prove to
+ * be expensive. A lookup table is maintained per m0_varr structure to
+ * remedy this problem.
+ */
+struct m0_lookup_table {
+	/* Base number to which exponent will be calculated. */
+	uint64_t  lt_base;
+
+	/* Length of exponent array. */
+	uint32_t  lt_len;
+
+	/*
+	 * Array of resultant numbers which are equivalent to m0_pow()
+	 * function. Index of element in this array is considered as
+	 * the exponent number.
+	 * Length of array is equal to associated m0_varr::va_depth.
+	 */
+	uint64_t *lt_values;
+};
+
+struct m0_varr {
+	/** Number of elements in array. */
+	uint64_t                va_nr;
+
+	/** Size of object type stored in m0_varr. */
+	size_t                  va_sizeof;
+
+	/** Size of buffer which is used to store objects from array. */
+	uint64_t                va_bufsize;
+
+	/** Level depth of tree proportional to number of objects stored. */
+	uint32_t                va_depth;
+
+	/**
+	 * Number of pointer to buffer that can be accommodated in one
+	 * meta buffer.
+	 * This number is easy to calculate and need not be stored
+	 * as a member of structure. However, during tree traversal, this
+	 * number is calculated multiple times in each trail, owing to
+	 * significant and _exactly same_ compute operations which can be
+	 * easily avoided by maintaining it as a member.
+	 */
+	uint64_t                va_bufptr_nr;
+
+	/**
+	 * Array of radix tree nodes, each of which represents an abstraction
+	 * of buffer containing multitude of objects.
+	 * The arrangement is such that there could be n levels within any
+	 * tree node before a leaf node is reached.
+	 * Such arrangement provisions constant height radix tree which eases
+	 * up the lookups.
+	 */
+	void                   *va_tree[VA_TNODE_NR];
+
+	/**
+	 * Array of varr_lifo_stack structures to maintain backlink
+	 * to parents in a tree traversal.
+	 */
+	struct varr_lifo_stack *va_stack;
+
+	/** Lookup table to store m0_pow() values for this array. */
+	struct m0_lookup_table  va_lktable;
+
+	/** Debug field - Number of buffers allocated. */
+	uint64_t                va_alloc;
+
+	/** Debug field - Number of buffers deallocated. */
+	uint64_t                va_dealloc;
+
+	/** Magic field to cross check sanity of structure. */
+	uint64_t                va_magic;
+};
+
+/**
+ * Initialises a virtual array.
+ * @param nr   Length of array.
+ * @param size Size of object to be stored in array.
+ * @param bufsize Size of each buffer which stores the objects.
+ * @pre   arr != NULL && nr > 0.
+ * @post  varr_invariant(arr) == true.
+ */
+M0_INTERNAL int varr_init(struct m0_varr *arr,
+			  uint64_t        nr,
+			  size_t          size,
+			  int             bufsize);
+
+/** Returns number of objects that can fit in one buffer. */
+M0_INTERNAL unsigned long varr_obj_nr_in_buff(const struct m0_varr *arr);
+
+/** Returns the level till which objects are accommodated in m0_varr. */
+M0_INTERNAL uint32_t level_find(const struct m0_varr *arr, uint64_t pg);
+
+/**
+ * Returns the buffer in which given object with index id falls.
+ * @pre  arr != NULL && index < arr->va_nr.
+ * @post varr_invariant(arr).
+ */
+M0_INTERNAL unsigned long *varr_buffer(const struct m0_varr *arr,
+				       uint64_t              index);
+
+#define varr_ele_get(arr, type, index)				\
+({								\
+	type          *__ptr;					\
+								\
+	__ptr  = (type *)varr_buffer(arr, index);		\
+	__ptr += index % varr_obj_nr_in_buff(arr);		\
+	M0_POST(__ptr != NULL);					\
+	__ptr;							\
+})
+
+/**
+ * Finalises a virtual array.
+ * @pre varr_invariant(arr) == true
+ */
+M0_INTERNAL void varr_fini(struct m0_varr *arr);
+
+#endif /* __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__ */
+
+/*
+ *  Local variables:
+ *  c-indentation-style: "K&R"
+ *  c-basic-offset: 8
+ *  tab-width: 8
+ *  fill-column: 80
+ *  scroll-step: 1
+ *  End:
+ */
-- 
1.8.3.2

