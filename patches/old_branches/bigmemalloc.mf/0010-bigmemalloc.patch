From 127558cb609124e7e0c2f1b6cfa983e53d65e8f9 Mon Sep 17 00:00:00 2001
From: "anand.vidwansa" <anand_vidwansa@xyratex.com>
Date: Wed, 14 Aug 2013 04:28:48 -0700
Subject: [PATCH 10/50] bigmemalloc: - Fixed miscellaneous defects.

---
 lib/ut/varr.c |   9 ++---
 lib/varr.c    | 106 ++++++++++++++++++++++++++--------------------------------
 lib/varr.h    |  34 +++++++++----------
 3 files changed, 70 insertions(+), 79 deletions(-)

diff --git a/lib/ut/varr.c b/lib/ut/varr.c
index be504f6..0a1e847 100644
--- a/lib/ut/varr.c
+++ b/lib/ut/varr.c
@@ -87,7 +87,8 @@ void test_varr(void)
 	struct m0_varr       arr;
 
 	/* varr used to store ELEMENT_NR, 8 byte long objects. */
-	rc = varr_init(&arr, ELEMENT_NR, sizeof(unsigned long), M0_0VEC_ALIGN);
+	rc = m0_varr_init(&arr, ELEMENT_NR, sizeof(unsigned long),
+			  M0_0VEC_ALIGN);
 
 	M0_UT_ASSERT(rc == 0);
 	M0_UT_ASSERT(arr.va_magic  == M0_LIB_GENARRAY_MAGIC);
@@ -142,10 +143,10 @@ void test_varr(void)
 		 (ELEMENT_NR % varr_obj_nr_in_buff(&arr))));
 	M0_UT_ASSERT(*addr == 0);
 	*/
-	varr_fini(&arr);
+	m0_varr_fini(&arr);
 
 	/* Genarray used to store array of structures. */
-	rc = varr_init(&arr, struct_nr, sizeof *f, M0_0VEC_ALIGN);
+	rc = m0_varr_init(&arr, struct_nr, sizeof *f, M0_0VEC_ALIGN);
 
 	/*
 	 * Total size to accommodate array of struct foo worth 64 bytes
@@ -190,5 +191,5 @@ void test_varr(void)
 		M0_UT_ASSERT(foo_check(f, id));
 	}
 	m0_free(address_tracker);
-	varr_fini(&arr);
+	m0_varr_fini(&arr);
 }
diff --git a/lib/varr.c b/lib/varr.c
index 7b5091f..6455a7b 100644
--- a/lib/varr.c
+++ b/lib/varr.c
@@ -48,7 +48,7 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 				 uint64_t        buff_nr);
 static uint64_t max_buff_nr_till_lev_n_pn(const struct m0_varr *arr,
 					  uint32_t              level);
-M0_INTERNAL void varr_fini(struct m0_varr *arr);
+M0_INTERNAL void m0_varr_fini(struct m0_varr *arr);
 M0_INTERNAL uint64_t cont_nr_for_objs(unsigned long nr,
 				      unsigned long obj_nr_in_1_cont);
 
@@ -104,9 +104,9 @@ static bool varr_invariant(const struct m0_varr *arr)
 		M0_0VEC_ALIGN % arr->va_sizeof == 0;
 }
 
-static int m0_lookup_table_init(struct m0_lookup_table *table,
-				uint64_t                base,
-				uint64_t                len)
+static int varr_lookup_table_init(struct varr_lookup_table *table,
+				  uint64_t                  base,
+				  uint64_t                  len)
 {
 	int      rc = 0;
 	uint64_t exp;
@@ -122,24 +122,25 @@ static int m0_lookup_table_init(struct m0_lookup_table *table,
 		rc = -ENOMEM;
 
 	table->lt_values[0] = 1;
-	for (exp = 1; exp <= len; ++exp)
+	table->lt_values[1] = base;
+	for (exp = 2; exp < len; ++exp)
 		table->lt_values[exp] = m0_pow(table->lt_base, exp);
 	return rc;
 }
 
-static uint64_t m0_lookup_table_lookup(const struct m0_lookup_table *table,
-				       uint64_t                      base,
-				       uint64_t                      index)
+static uint64_t varr_lookup_table_lookup(const struct varr_lookup_table *table,
+				         uint64_t                        base,
+				         uint64_t                        index)
 {
 	M0_PRE(table != NULL);
 	M0_PRE(base  == table->lt_base);
-	M0_PRE(index <= table->lt_len);
+	M0_PRE(index <  table->lt_len);
 	M0_PRE(table->lt_values != NULL);
 
 	return table->lt_values[index];
 }
 
-static void m0_lookup_table_fini(struct m0_lookup_table *table)
+static void m0_lookup_table_fini(struct varr_lookup_table *table)
 {
 	m0_free(table->lt_values);
 	table->lt_base   = 0;
@@ -147,10 +148,10 @@ static void m0_lookup_table_fini(struct m0_lookup_table *table)
 	table->lt_values = NULL;
 }
 
-M0_INTERNAL int varr_init(struct m0_varr *arr,
-			  uint64_t        nr,
-			  size_t          size,
-			  int             bufsize)
+M0_INTERNAL int m0_varr_init(struct m0_varr *arr,
+			     uint64_t        nr,
+			     size_t          size,
+			     int             bufsize)
 {
 	int      i;
 	int      rc = 0;
@@ -183,7 +184,7 @@ M0_INTERNAL int varr_init(struct m0_varr *arr,
 	buff_nr         = cont_nr_for_objs(nr, varr_obj_nr_in_buff(arr));
 	arr->va_depth   = level_find(arr, buff_nr);
 
-	/* Builds the stack only if it is needed. */
+	/* Allocates the stack only if it is needed. */
 	if (arr->va_depth - 1 > 0) {
 		M0_ALLOC_ARR(arr->va_stack, arr->va_depth - 1);
 		if (arr->va_stack == NULL)
@@ -191,20 +192,20 @@ M0_INTERNAL int varr_init(struct m0_varr *arr,
 	}
 
 	if (rc == 0) {
-		rc = m0_lookup_table_init(&arr->va_lktable, arr->va_bufptr_nr,
-					  arr->va_depth - 1);
+		rc = varr_lookup_table_init(&arr->va_lktable, arr->va_bufptr_nr,
+					    arr->va_depth);
 		if (rc == 0)
 			rc = varr_buffers_alloc(arr, buff_nr);
 	}
 
 	if (rc != 0)
-		varr_fini(arr);
+		m0_varr_fini(arr);
 	else
 		M0_POST_EX(varr_invariant(arr));
 	return rc;
 }
 
-M0_INTERNAL void varr_fini(struct m0_varr *arr)
+M0_INTERNAL void m0_varr_fini(struct m0_varr *arr)
 {
 	M0_PRE(arr != NULL);
 	M0_PRE_EX(varr_invariant(arr));
@@ -309,9 +310,9 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 	for (done = 0, node = 0; node < VA_TNODE_NR && done < buff_nr;
 	     ++node) {
 		holder = arr->va_tree[node];
-		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 0),
+		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 1),
 				    buff_nr - done);
-		for (; done < buff_nr_pn; ) {
+		while (done < buff_nr_pn) {
 
 			for (level = 1; level <= arr->va_depth &&
 			     done < buff_nr_pn; ++level) {
@@ -320,16 +321,15 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 					    arr->va_bufptr_nr,
 					    buff_nr_pn - done);
 				if (level != arr->va_depth) {
-					arr->va_stack[level].ls_index  =
+					arr->va_stack[level - 1].ls_index  =
 						index_in_level_n_pn(arr, done,
-								level);
-					holder += arr->va_stack[level].ls_index;
-					arr->va_stack[level].ls_parent = holder;
-				}
-
-				/* We respect your privacy! */
-				if (level != arr->va_depth)
+								    level);
+					holder += arr->va_stack[level - 1].
+						  ls_index;
+					arr->va_stack[level - 1].ls_parent =
+						holder;
 					holder = (unsigned long *)*holder;
+				}
 			}
 
 			/* Deallocates the leaf node buffer(s). */
@@ -339,24 +339,25 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 
 			/* If the stack was built, start popping the stack. */
 			for (level = arr->va_depth - 1;
-			     level >= 1 && arr->va_stack != NULL; ++level) {
+			     level >= 1 && arr->va_stack != NULL; --level) {
 
 				/*
 				 * Deallocates the holder buffer if all
 				 * buffers beneath this buffer in this trail
 				 * are already deallocated.
 				 */
-				if (max_buff_nr_till_lev_n_pn(arr, level) *
-				    (arr->va_stack[level].ls_index + 1) ==
-				    done % max_buff_nr_till_lev_n_pn(arr, 0)) {
+				if (min64u(buff_nr_pn,
+				    max_buff_nr_till_lev_n_pn(arr, level) *
+				    (arr->va_stack[level - 1].ls_index + 1)) ==
+				    done % max_buff_nr_till_lev_n_pn(arr, 1)) {
 					rc = buffers_helper(arr, arr->
-							    va_stack[level].
+							    va_stack[level - 1].
 							    ls_parent, 1,
 							    BA_DEALLOC);
 					M0_ASSERT(rc == 0);
 				}
-				arr->va_stack[level].ls_parent = NULL;
-				arr->va_stack[level].ls_index  = 0;
+				arr->va_stack[level - 1].ls_parent = NULL;
+				arr->va_stack[level - 1].ls_index  = 0;
 			}
 		}
 	}
@@ -397,10 +398,16 @@ static int varr_buffers_alloc(struct m0_varr *arr,
 	 */
 	for (done = 0, node = 0; node < VA_TNODE_NR && done < buff_nr;
 	     ++node) {
+		if (arr->va_tree[node] == NULL) {
+			rc = buffers_helper(arr, arr->va_tree[node],
+					    1, BA_ALLOC);
+			if (rc != 0)
+				goto end;
+		}
 		holder = arr->va_tree[node];
 		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 1),
 				    buff_nr - done);
-		for (; done < buff_nr_pn; ) {
+		while (done < buff_nr_pn) {
 
 			for (level = 1; level <= arr->va_depth &&
 			     done < buff_nr_pn; ++level) {
@@ -478,9 +485,6 @@ M0_INTERNAL uint64_t index_in_level_n_pn(const struct m0_varr *arr,
  * The acronym _pn in API name stands for "per node".
  * Lower level number contains more buffers than higher level number.
  * Level 0 is ancestor of level n (n > 0).
- * TODO: A lookup table per m0_varr object would be helpful since
- * during tree traversals and initialization, m0_pow() will be invoked
- * every time which would be expensive.
  */
 static uint64_t max_buff_nr_till_lev_n_pn(const struct m0_varr *arr,
 					  uint32_t              level)
@@ -489,22 +493,8 @@ static uint64_t max_buff_nr_till_lev_n_pn(const struct m0_varr *arr,
 
 	/*return m0_pow(arr->va_bufsize / VA_TNODEPTR_SIZE,
 		      arr->va_depth - level);*/
-	return m0_lookup_table_lookup(&arr->va_lktable, arr->va_bufptr_nr,
-				      arr->va_depth - level);
-}
-
-/*
- * Returns max possible number of _total_ buffers (for all, VA_TNODE_NR
- * sibling nodes) that can fit till given level in virtual array.
- */
-static uint64_t max_buff_nr_till_lev_n(const struct m0_varr *arr,
-				       uint32_t              level)
-{
-	M0_PRE(arr != NULL);
-	M0_PRE(arr->va_sizeof  > 0);
-	M0_PRE(arr->va_bufsize > 0);
-
-	return VA_TNODE_NR * max_buff_nr_till_lev_n_pn(arr, level);
+	return varr_lookup_table_lookup(&arr->va_lktable, arr->va_bufptr_nr,
+				        arr->va_depth - level);
 }
 
 /*
@@ -539,7 +529,7 @@ M0_INTERNAL uint32_t level_find(const struct m0_varr *arr,
 	M0_PRE(pg   > 0);
 
 	for (level = 1; !found; ++level)
-		if (pg < max_buff_nr_till_lev_n(arr, level))
+		if (pg <= VA_TNODE_NR * m0_pow(arr->va_bufptr_nr, level))
 			found = true;
-	return level;
+	return level + 1;
 }
diff --git a/lib/varr.h b/lib/varr.h
index f2ebb55..14ff882 100644
--- a/lib/varr.h
+++ b/lib/varr.h
@@ -102,7 +102,7 @@ enum {
  * be expensive. A lookup table is maintained per m0_varr structure to
  * remedy this problem.
  */
-struct m0_lookup_table {
+struct varr_lookup_table {
 	/* Base number to which exponent will be calculated. */
 	uint64_t  lt_base;
 
@@ -120,16 +120,16 @@ struct m0_lookup_table {
 
 struct m0_varr {
 	/** Number of elements in array. */
-	uint64_t                va_nr;
+	uint64_t                  va_nr;
 
 	/** Size of object type stored in m0_varr. */
-	size_t                  va_sizeof;
+	size_t                    va_sizeof;
 
 	/** Size of buffer which is used to store objects from array. */
-	uint64_t                va_bufsize;
+	uint64_t                  va_bufsize;
 
 	/** Level depth of tree proportional to number of objects stored. */
-	uint32_t                va_depth;
+	uint32_t                  va_depth;
 
 	/**
 	 * Number of pointer to buffer that can be accommodated in one
@@ -140,7 +140,7 @@ struct m0_varr {
 	 * significant and _exactly same_ compute operations which can be
 	 * easily avoided by maintaining it as a member.
 	 */
-	uint64_t                va_bufptr_nr;
+	uint64_t                  va_bufptr_nr;
 
 	/**
 	 * Array of radix tree nodes, each of which represents an abstraction
@@ -150,25 +150,25 @@ struct m0_varr {
 	 * Such arrangement provisions constant height radix tree which eases
 	 * up the lookups.
 	 */
-	void                   *va_tree[VA_TNODE_NR];
+	void                     *va_tree[VA_TNODE_NR];
 
 	/**
 	 * Array of varr_lifo_stack structures to maintain backlink
 	 * to parents in a tree traversal.
 	 */
-	struct varr_lifo_stack *va_stack;
+	struct varr_lifo_stack   *va_stack;
 
 	/** Lookup table to store m0_pow() values for this array. */
-	struct m0_lookup_table  va_lktable;
+	struct varr_lookup_table  va_lktable;
 
 	/** Debug field - Number of buffers allocated. */
-	uint64_t                va_alloc;
+	uint64_t                  va_alloc;
 
 	/** Debug field - Number of buffers deallocated. */
-	uint64_t                va_dealloc;
+	uint64_t                  va_dealloc;
 
 	/** Magic field to cross check sanity of structure. */
-	uint64_t                va_magic;
+	uint64_t                  va_magic;
 };
 
 /**
@@ -179,10 +179,10 @@ struct m0_varr {
  * @pre   arr != NULL && nr > 0.
  * @post  varr_invariant(arr) == true.
  */
-M0_INTERNAL int varr_init(struct m0_varr *arr,
-			  uint64_t        nr,
-			  size_t          size,
-			  int             bufsize);
+M0_INTERNAL int m0_varr_init(struct m0_varr *arr,
+			     uint64_t        nr,
+			     size_t          size,
+			     int             bufsize);
 
 /** Returns number of objects that can fit in one buffer. */
 M0_INTERNAL unsigned long varr_obj_nr_in_buff(const struct m0_varr *arr);
@@ -212,7 +212,7 @@ M0_INTERNAL unsigned long *varr_buffer(const struct m0_varr *arr,
  * Finalises a virtual array.
  * @pre varr_invariant(arr) == true
  */
-M0_INTERNAL void varr_fini(struct m0_varr *arr);
+M0_INTERNAL void m0_varr_fini(struct m0_varr *arr);
 
 #endif /* __MERO_LIB_LINUX_KERNEL_GEN_ARRAY_H__ */
 
-- 
1.8.3.2

