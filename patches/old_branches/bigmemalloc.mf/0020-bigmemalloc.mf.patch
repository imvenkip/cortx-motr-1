From 25ba1d52a40eee0d75e4c79f66dd8fe2a0b60453 Mon Sep 17 00:00:00 2001
From: Nachiket Sahasrabuddhe <nachiket_sahasrabuddhe@xyratex.com>
Date: Thu, 5 Sep 2013 19:08:15 +0530
Subject: [PATCH 20/50] bigmemalloc.mf

-All modifications over except caching functions.
---
 lib/linux_kernel/varr.c |  14 +-
 lib/varr.c              | 334 +++++++++++++++++++++---------------------------
 lib/varr.h              |  51 +++-----
 3 files changed, 171 insertions(+), 228 deletions(-)

diff --git a/lib/linux_kernel/varr.c b/lib/linux_kernel/varr.c
index fda883f..c6e2190 100644
--- a/lib/linux_kernel/varr.c
+++ b/lib/linux_kernel/varr.c
@@ -25,16 +25,20 @@
 
 M0_EXTERN void *m0_varr_buf_alloc(int bufsize)
 {
-	M0_PRE(bufsize == PAGE_CACHE_SIZE);
-
-	return (void *)get_zeroed_page(GFP_KERNEL);
+	if (bufsize == PAGE_CACHE_SIZE)
+		return (void *)get_zeroed_page(GFP_KERNEL);
+	else
+		return m0_alloc(bufsize);
 }
 
-M0_EXTERN void m0_varr_buf_free(void *buf)
+M0_EXTERN void m0_varr_buf_free(void *buf, int bufsize)
 {
 	M0_PRE(buf != NULL);
 
-	free_page((unsigned long)buf);
+	if (bufsize == PAGE_CACHE_SIZE)
+		free_page((unsigned long)buf);
+	else
+		m0_free(buf);
 }
 
 M0_EXTERN bool m0_varr_size_is_valid(const struct m0_varr *arr)
diff --git a/lib/varr.c b/lib/varr.c
index 290aee5..1334c8e 100644
--- a/lib/varr.c
+++ b/lib/varr.c
@@ -37,9 +37,6 @@ static const struct m0_bob_type varr_bobtype = {
 	.bt_check        = NULL,
 };
 
-M0_INTERNAL uint64_t index_in_level_n_pn(const struct m0_varr *arr,
-					 uint64_t              done,
-					 uint32_t              level);
 M0_INTERNAL uint32_t level_find(const struct m0_varr *arr, uint64_t pg);
 static bool varr_invariant(const struct m0_varr *arr);
 static int varr_buffers_alloc(struct m0_varr *arr,
@@ -63,19 +60,26 @@ M0_INTERNAL uint64_t cont_nr_for_objs(unsigned long nr,
  * Index of an element in the array is used an identifier to represent the
  * level in the tree.
  */
-struct varr_lifo_stack_rec {
+struct varr_stack_rec {
 	/* Backlink to parent node. */
-	void     *lsr_addr;
+	void     *sr_addr;
 
 	/* Index of parent node in the buffer. */
-	uint64_t  lsr_index;
+	uint64_t  sr_index;
 };
 
-struct varr_lifo_stack {
-	struct varr_lifo_stack_rec *ls_rec;
-	int			    ls_sp;
+struct varr_stack {
+	struct varr_stack_rec *s_rec;
+	int		       s_sp;
 };
 
+/* Cache that holds pointer to recently accessed buffer along with range of
+ * objects that reside in it.
+ */
+struct varr_cache {
+	void    *buff;
+	uint64_t start_index;
+}
 /* Enumeration for action to be taken on a set of buffers. */
 enum buffer_action {
 	BA_ALLOC,
@@ -83,32 +87,44 @@ enum buffer_action {
 	BA_NR
 };
 
-int push(struct m0_varr *arr, void *addr, uint64_t index)
+M0_INTERNAL int push(struct m0_varr *arr, void *addr, uint64_t index)
 {
 	++arr->va_stack->ls_sp;
-	if (arr->va_stack->ls_sp == arr->va_depth)
-		return -ENOMEM;
+	if (arr->va_stack->ls_sp == arr->va_depth - 1)
+		return -EPERM;
 	arr->va_stack->ls_rec[arr->va_stack->ls_sp].lsr_addr   = addr;
 	arr->va_stack->ls_rec[arr->va_stack->ls_sp].lsr_index  = index;
 	return 0;
 }
 
-int pop(struct m0_varr *arr, void **addr, uint64_t *index)
+M0_INTERNAL int pop(struct m0_varr *arr, void **addr, uint64_t *index)
 {
 	if (arr->va_stack->ls_sp == -1)
-		return -ENOMEM;
+		return -EPERM;
 	*addr  = arr->va_stack->ls_rec[arr->va_stack->ls_sp].lsr_addr;
 	*index = arr->va_stack->ls_rec[arr->va_stack->ls_sp].lsr_index;
 	--arr->va_stack->ls_sp;
 	return 0;
 }
 
+/* Returns logarithm to the base two, for nearest power of two
+ * which is not lesser than 'size'*/
+M0_INTERNAL uint8_t nearest_power_of_two(size_t size)
+{
+	uint32_t aligned_size = 1;
+	uint8_t  aligned_shift = 0;
+
+	while (size <= aligned_size) {
+		aligned_size<<1;
+		++aligned_shift;
+	}
+	return aligned_shift;
+}
+
 M0_INTERNAL unsigned long varr_obj_nr_in_buff(const struct m0_varr *arr)
 {
 	M0_PRE(arr != NULL);
-	M0_PRE(arr->va_sizeof > 0);
-
-	return arr->va_bufsize / arr->va_sizeof;
+	return 1<<(arr->va_buf_shift - arr->va_obj_shift);
 }
 
 M0_INTERNAL uint64_t cont_nr_for_objs(unsigned long nr,
@@ -119,64 +135,35 @@ M0_INTERNAL uint64_t cont_nr_for_objs(unsigned long nr,
 	return nr / obj_nr_in_1_cont + (nr % obj_nr_in_1_cont == 0 ? 0 : 1);
 }
 
-static bool varr_invariant(const struct m0_varr *arr)
+M0_INTERNAL bool within_tree_height(const struct m0_varr *arr, int level)
 {
-	return
-		m0_varr_bob_check(arr) &&
-		arr->va_nr > 0 &&
-		arr->va_alloc > 0 &&
-		arr->va_sizeof > 0 &&
-		M0_0VEC_ALIGN % arr->va_sizeof == 0;
+	return level > 0 && level < arr->va_depth;
 }
 
-static int varr_lookup_table_init(struct varr_lookup_table *table,
-				  uint64_t                  base,
-				  uint64_t                  len)
+M0_INTERNAL bool within_tree_width(const struct m0_varr *arr, uint64_t child_id)
 {
-	int      rc = 0;
-	uint64_t exp;
-
-	M0_PRE(table != NULL);
-	M0_PRE(base > 0);
-	M0_PRE(len  > 0);
-
-	table->lt_base = base;
-	table->lt_len  = len;
-	M0_ALLOC_ARR(table->lt_values, len);
-	if (table->lt_values == NULL)
-		rc = -ENOMEM;
-
-	table->lt_values[0] = 1;
-	table->lt_values[1] = base;
-	for (exp = 2; exp < len; ++exp)
-		table->lt_values[exp] = m0_pow(table->lt_base, exp);
-	return rc;
+	return child_id < arr->va_buf_ptr_nr;
 }
 
-static uint64_t varr_lookup_table_lookup(const struct varr_lookup_table *table,
-				         uint64_t                        base,
-				         uint64_t                        index)
+M0_INTERNAL uint64_t last_nbits_set(uint8_t n)
 {
-	M0_PRE(table != NULL);
-	M0_PRE(base  == table->lt_base);
-	M0_PRE(index <  table->lt_len);
-	M0_PRE(table->lt_values != NULL);
-
-	return table->lt_values[index];
+	return n < 64 ? ~(~0<<n) : ~0;
 }
 
-static void m0_lookup_table_fini(struct varr_lookup_table *table)
+static bool varr_invariant(const struct m0_varr *arr)
 {
-	m0_free(table->lt_values);
-	table->lt_base   = 0;
-	table->lt_len    = 0;
-	table->lt_values = NULL;
+	return
+		m0_varr_bob_check(arr) &&
+		arr->va_nr > 0 &&
+		arr->va_alloc > 0 &&
+		arr->va_sizeof > 0 &&
+		M0_0VEC_ALIGN % arr->va_sizeof == 0;
 }
 
 M0_INTERNAL int m0_varr_init(struct m0_varr *arr,
 			     uint64_t        nr,
 			     size_t          size,
-			     int             bufsize)
+			     size_t          bufsize)
 {
 	int      i;
 	int      rc = 0;
@@ -197,33 +184,31 @@ M0_INTERNAL int m0_varr_init(struct m0_varr *arr,
 	if (!m0_varr_size_is_valid(arr))
 		return -EINVAL;
 
-	arr->va_nr        = nr;
-	arr->va_alloc     = arr->va_dealloc = 0;
-	arr->va_sizeof    = size;
-	arr->va_bufsize   = bufsize;
-	arr->va_bufptr_nr = arr->va_bufsize / VA_TNODEPTR_SIZE;
+	arr->va_nr              = nr;
+	arr->va_alloc           = arr->va_dealloc = 0;
+	arr->va_size_shift      = nearest_power_of_two(size);
+	arr->va_buf_shift       = nearest_power_of_two(bufsize);
+	arr->va_bufptr_nr_shift = arr->va_buf_shift -
+		neares_power_of_two(VA_TNODEPTR_SIZE);
+	arr->va_bufptr_nr       = 1<<arr->va_bufptr_nr_shift;
 	m0_varr_bob_init(arr);
 	for (i = 0; i < VA_TNODE_NR; ++i)
 		arr->va_tree[i] = NULL;
 
 	buff_nr         = cont_nr_for_objs(nr, varr_obj_nr_in_buff(arr));
 	arr->va_depth   = level_find(arr, buff_nr);
-
-	/* Allocates the stack only if it is needed. */
-	if (arr->va_depth > 0) {
+	if (arr->va_depth > 1) {
 		M0_ALLOC_PTR(arr->va_stack);
-		M0_ALLOC_ARR(arr->va_stack->ls_rec, arr->va_depth);
 		if (arr->va_stack == NULL)
 			rc = -ENOMEM;
-		arr->va_stack->ls_sp = 0;
+		M0_ALLOC_ARR(arr->va_stack->ls_rec, arr->va_depth - 1);
+		if (arr->va_stack->ls_rec == NULL)
+			rc = -ENOMEM;
+		arr->va_stack->ls_sp = -1;
 	}
 
-	if (rc == 0) {
-		rc = varr_lookup_table_init(&arr->va_lktable, arr->va_bufptr_nr,
-					    arr->va_depth);
-		if (rc == 0)
-			rc = varr_buffers_alloc(arr, buff_nr);
-	}
+	if (rc == 0)
+		rc = varr_buffers_alloc(arr, buff_nr);
 
 	if (rc != 0)
 		m0_varr_fini(arr);
@@ -250,31 +235,38 @@ M0_INTERNAL void m0_varr_fini(struct m0_varr *arr)
 	arr->va_alloc  = arr->va_dealloc = 0;
 }
 
-M0_INTERNAL unsigned long *varr_buffer(const struct m0_varr *arr,
-				       uint64_t              index)
+M0_INTERNAL unsigned long *m0_varr_buffer(const struct m0_varr *arr,
+					  uint64_t index)
 {
 	uint32_t       level;
-	uint64_t       id;
-	uint64_t       buf;
+	uint64_t       obj_mask;
+	uint64_t       buff_mask;
 	unsigned long *holder;
+	uint64_t       index_local;
 
 	M0_PRE(arr   != NULL);
 	M0_PRE(index <  arr->va_nr);
 
-	buf    = index / varr_obj_nr_in_buff(arr);
-	id     = buf / max_buff_nr_till_lev_n_pn(arr, 1);
-	holder = arr->va_tree[id];
-	M0_ASSERT(holder != NULL);
-	buf -= max_buff_nr_till_lev_n_pn(arr, 1) * id;
-	for (level = 1; level <= arr->va_depth && buf != 0; ++level) {
-		id = index_in_level_n_pn(arr, buf, level);
-		holder += id;
-		buf -= max_buff_nr_till_lev_n_pn(arr, level) * id;
+	if (cache_fetch(arr, index, &holder))
+		return holder;
+	obj_mask  = last_nbits_set(arr->va_obj_shift);
+	buff_mask = last_nbits_set(arr->va_buff_shift);
+	index_local = index;
+	index_local >>= arr->va_obj_shift + (arr->va_depth - 1) *
+		arr->va_buf_shift;
+	holder = arr->va_tree[index_local];
+	index_local = index;
+	for (level = 1; level <= arr->va_depth - 1; ++level,
+	     index_local = index) {
+		index_local >>= arr->va_obj_shif + (arr->va_depth - level) *
+			arr->va_buf_shift;
+		index_local &= buff_mask;
+		holder += index;
 		/* Dereferences the buffer pointer at given offset. */
 		M0_ASSERT((unsigned long *)*holder!= NULL);
 		holder = (unsigned long *)*holder;
 	}
-
+	cache_update(arr, holder, index & (~obj_mask));
 	M0_POST_EX(varr_invariant(arr));
 	return holder;
 }
@@ -326,7 +318,7 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 {
 	int            rc;
 	int            node;
-	int            level;
+	uint64_t       level;
 	uint64_t       nr;
 	uint64_t       done;
 	uint64_t       done_pt;
@@ -337,18 +329,27 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 	M0_PRE(arr     != NULL);
 	M0_PRE(buff_nr  > 0);
 
+	arr->va_stack->ls_sp == -1;
 	for (done = 0, done_pt = 0, node = 0, child_id = 0; node < VA_TNODE_NR
-	     && done < buff_nr; ++node, done_pt = 0, level = 1) {
+	     && done < buff_nr; ++node, done_pt = 0, level = 1, child_id = 0) {
 		holder = arr->va_tree[node];
 		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 1),
 				    buff_nr - done);
-		while (level > -1) {
-			while (child_id < arr->va_bufptr_nr &&
+		if (arr->va_depth == 1) {
+			done_pt = 1;
+			continue;
+		} else {
+			rc = push(arr, &arr->va_tree[node], 0);
+			M0_ASSERT(rc == 0);
+		}
+		while (within_tree_height(arr, level)) {
+			while (done_pt < buff_nr_pn &&
+			       within_tree_width(arr, child_id) &&
 			       (unsigned long *)*holder != NULL) {
-				nr = min64u(level != arr->va_depth ? 1 :
+				nr = min64u(level != arr->va_depth - 1? 1 :
 					    arr->va_bufptr_nr,
 					    buff_nr_pn - done_pt);
-				if (level == arr->va_depth) {
+				if (level == arr->va_depth - 1) {
 					rc = buffers_helper(arr, holder, nr,
 							    BA_DEALLOC);
 					M0_ASSERT(rc == 0);
@@ -370,107 +371,82 @@ static void varr_buffers_dealloc(struct m0_varr *arr,
 			--level;
 			++holder;
 			++child_id;
+			M0_ASSERT(arr->va_stack->ls_sp == level - 1);
 		}
 		done += done_pt;
+		M0_ASSERT(arr->va_stack->ls_sp == -1);
 	}
 }
 
 /* Allocates buffers for given virtual array from level 0 to varr::va_depth. */
 static int varr_buffers_alloc(struct m0_varr *arr,
-			      uint64_t        buff_nr)
+			       uint64_t buff_nr)
 {
 	int            rc;
 	int            node;
-	uint32_t       level;
+	int            level;
 	uint64_t       nr;
-	/* Number of buffer allocated at any given moment. */
 	uint64_t       done;
-	/* Number of buffers allocated per tree at given moment */
 	uint64_t       done_pt;
-	/* Number of buffers per tree node. */
+	uint64_t       child_id;
 	uint64_t       buff_nr_pn;
-	/*
-	 * Entity which holds address of some buffer, typically a part
-	 * of some meta-buffer.
-	 */
 	unsigned long *holder;
 
 	M0_PRE(arr     != NULL);
 	M0_PRE(buff_nr  > 0);
 
-	/*
-	 * While allocating a structure with multiple indirections,
-	 * the buffer allocations should be done in iterative manner
-	 * (as compared to recursive manner, which could lead to issues
-	 *  with limited stack size in kernel).
-	 * Only the buffers at last level (m0_varr::va_depth) contain
-	 * actual objects requested by end-user.
-	 * Buffers from rest of levels are meta-buffers, containing
-	 * pointers to other buffers from level beneath.
-	 * While traversing the tree, a trail is followed by dereferencing
-	 * a pointer on each level, except for the last level.
-	 */
-	for (done = 0, node = 0, done_pt = 0; node < VA_TNODE_NR && done < buff_nr;
-	     ++node, done_pt = 0) {
-		if (arr->va_tree[node] == NULL) {
-			rc = buffers_helper(arr, arr->va_tree[node],
-					    1, BA_ALLOC);
+	for (done = 0, done_pt = 0, node = 0, child_id = 0; node < VA_TNODE_NR
+	     && done < buff_nr; ++node, done_pt = 0, level = 1) {
+		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 1),
+				    buff_nr - done);
+		rc = buffers_helper(arr, arr->va_tree[node], 1, BA_ALLOC);
+		if (rc != 0)
+			goto end;
+		holder = arr->va_tree[node];
+		/* If tree-depth is unity we do not use stack hence we continue
+		 * to the next tree in forest.*/
+		if (arr->va_depth == 1) {
+			done_pt = 1;
+			continue;
+		} else {
+			rc = push(arr, &arr->va_tree[node], 0);
 			if (rc != 0)
 				goto end;
 		}
-		buff_nr_pn = min64u(max_buff_nr_till_lev_n_pn(arr, 1),
-				    buff_nr - done);
-		while (done_pt < buff_nr_pn) {
-			holder = arr->va_tree[node];
-			for (level = 1; level <= arr->va_depth &&
-			     done_pt < buff_nr_pn; ++level) {
-
-				/*
-				 * For all levels except the last one,
-				 * only 1 buffer will be allocated in
-				 * each level. Since only last level
-				 * holds data buffers, it can allocate
-				 * multiple buffers.
-				 */
-				nr = min64u(level != arr->va_depth ? 1 :
+		while (done_pt < buff_nr_pn &&
+		       within_tree_height(arr, level)) {
+			while (within_tree_width(arr, child_id)) {
+				nr = min64u(level != arr->va_depth - 1? 1 :
 					    arr->va_bufptr_nr,
 					    buff_nr_pn - done_pt);
-
-				/*
-				 * For last level, the holding address is
-				 * fixed and need not be computed.
-				 */
-					holder += index_in_level_n_pn(arr,
-							done_pt, level);
-
-				/*
-				 * The loop travels same trail multiple times
-				 * until all buffers which fall in this trail
-				 * are allocated.
-				 * Meta-buffers in this trail could be already
-				 * allocated and no allocation is needed on
-				 * that level.
-				 */
-				if ((unsigned long *)*holder == NULL) {
-					rc = buffers_helper(arr, holder, nr,
-							    BA_ALLOC);
+				rc = buffers_helper(arr, holder, nr,
+						    BA_ALLOC);
+				if (rc != 0)
+					goto end;
+				if (level == arr->va_depth - 1) {
+					done_pt += nr;
+					break;
+				} else {
+					rc = push(arr, holder,
+					          child_id);
 					if (rc != 0)
 						goto end;
-					/*
-					 * Input buff_nr number is number of
-					 * _data_ buffers.
-					 */
-					if (level == arr->va_depth)
-						done_pt += nr;
-				}
-				M0_ASSERT((unsigned long *)*holder != NULL);
-
-				/* We respect your privacy! */
-				if (level != arr->va_depth)
 					holder = (unsigned long *)*holder;
+					child_id = 0;
+					++level;
+				}
 			}
+			rc = pop(arr, (void **)&holder, &child_id);
+			if (rc != 0)
+				goto end;
+			--level;
+			++holder;
+			++child_id;
+			M0_ASSERT(arr->va_stack->ls_sp == level - 1);
 		}
-		done += buff_nr_pn;
+		done += done_pt;
+		/* Reset the stack pointer before parsing next tree. */
+		arr->va_stack->ls_sp = -1;
 	}
 end:
 	M0_POST(ergo(rc == 0, done == buff_nr));
@@ -478,21 +454,6 @@ end:
 }
 
 /*
- * Returns the index in given buffer at given level when the tree traversal
- * has reached @done nodes.
- * The acronym _pn in API name stands for "per node".
- */
-M0_INTERNAL uint64_t index_in_level_n_pn(const struct m0_varr *arr,
-					 uint64_t              done,
-					 uint32_t              level)
-{
-	M0_PRE(arr  != NULL);
-	M0_PRE(level < arr->va_depth);
-
-	return done / max_buff_nr_till_lev_n_pn(arr, level);
-}
-
-/*
  * Returns max possible number of buffers for only a single tree node,
  * that can fit till given level in virtual array.
  * The acronym _pn in API name stands for "per node".
@@ -506,8 +467,7 @@ static uint64_t max_buff_nr_till_lev_n_pn(const struct m0_varr *arr,
 
 	/*return m0_pow(arr->va_bufsize / VA_TNODEPTR_SIZE,
 		      arr->va_depth - level);*/
-	return varr_lookup_table_lookup(&arr->va_lktable, arr->va_bufptr_nr,
-				        arr->va_depth - level);
+	return 1<<(arr->va_bufptr_nr_shift * (arr->va_depth - level));
 }
 
 /*
@@ -541,8 +501,8 @@ M0_INTERNAL uint32_t level_find(const struct m0_varr *arr,
 	M0_PRE(arr != NULL);
 	M0_PRE(pg   > 0);
 
-	for (level = 1; !found; ++level)
-		if (pg <= VA_TNODE_NR * m0_pow(arr->va_bufptr_nr, level))
+	for (level = 0; !found; ++level)
+		if (pg <= 1<<(arr->va_bufptr_nr_shift * level + VA_NODE_SHIFT))
 			found = true;
 	return level + 1;
 }
diff --git a/lib/varr.h b/lib/varr.h
index 61956f3..0b7f786 100644
--- a/lib/varr.h
+++ b/lib/varr.h
@@ -77,10 +77,10 @@
  */
 
 struct m0_varr;
-struct varr_lifo_stack;
+struct varr_stack;
 
-M0_EXTERN void *m0_varr_buf_alloc(int   bufsize);
-M0_EXTERN void  m0_varr_buf_free(void *buf);
+M0_EXTERN void *m0_varr_buf_alloc(int bufsize);
+M0_EXTERN void  m0_varr_buf_free(void *buf, int bufsize);
 M0_EXTERN bool  m0_varr_size_is_valid(const struct m0_varr *arr);
 
 enum {
@@ -89,42 +89,22 @@ enum {
 
 	/* Size of pointer to a tree node. */
 	VA_TNODEPTR_SIZE = sizeof(void *),
-};
 
-/**
- * While initializing the virtual array and while iterating the radix tree
- * in it, m0_pow() function is invoked multiple times which could prove to
- * be expensive. A lookup table is maintained per m0_varr structure to
- * remedy this problem.
- */
-struct varr_lookup_table {
-	/* Base number to which exponent will be calculated. */
-	uint64_t  lt_base;
-
-	/* Length of exponent array. */
-	uint32_t  lt_len;
-
-	/*
-	 * Array of resultant numbers which are equivalent to m0_pow()
-	 * function. Index of element in this array is considered as
-	 * the exponent number.
-	 * Length of array is equal to associated m0_varr::va_depth.
-	 */
-	uint64_t *lt_values;
+	VA_TNODE_NR_SHIFT = 6,
 };
 
 struct m0_varr {
 	/** Number of elements in array. */
-	uint64_t                  va_nr;
+	uint64_t         va_nr;
 
 	/** Size of object type stored in m0_varr. */
-	size_t                    va_sizeof;
+	uint8_t          va_obj_shift;
 
 	/** Size of buffer which is used to store objects from array. */
-	uint64_t                  va_bufsize;
+	uint8_t          va_buf_shift;
 
 	/** Level depth of tree proportional to number of objects stored. */
-	uint32_t                  va_depth;
+	uint32_t         va_depth;
 
 	/**
 	 * Number of pointer to buffer that can be accommodated in one
@@ -135,7 +115,7 @@ struct m0_varr {
 	 * significant and _exactly same_ compute operations which can be
 	 * easily avoided by maintaining it as a member.
 	 */
-	uint64_t                  va_bufptr_nr;
+	uint64_t          va_bufptr_nr_shift;
 
 	/**
 	 * Array of radix tree nodes, each of which represents an abstraction
@@ -145,24 +125,23 @@ struct m0_varr {
 	 * Such arrangement provisions constant height radix tree which eases
 	 * up the lookups.
 	 */
-	void                     *va_tree[VA_TNODE_NR];
+	void              *va_tree[VA_TNODE_NR];
 
 	/**
 	 * maintains backlink to parents in a tree traversal.
 	 */
-	struct varr_lifo_stack    *va_stack;
+	struct varr_stack *va_stack;
 
-	/** Lookup table to store m0_pow() values for this array. */
-	struct varr_lookup_table  va_lktable;
+	struct varr_cache *va_cache;
 
 	/** Debug field - Number of buffers allocated. */
-	uint64_t                  va_alloc;
+	uint64_t           va_alloc;
 
 	/** Debug field - Number of buffers deallocated. */
-	uint64_t                  va_dealloc;
+	uint64_t           va_dealloc;
 
 	/** Magic field to cross check sanity of structure. */
-	uint64_t                  va_magic;
+	uint64_t           va_magic;
 };
 
 /**
-- 
1.8.3.2

