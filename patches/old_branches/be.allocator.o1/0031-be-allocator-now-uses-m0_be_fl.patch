From bf72abb9620dbc3af8a8498953fe0e5fb734f511 Mon Sep 17 00:00:00 2001
From: Maxim Medved <max_medved@xyratex.com>
Date: Fri, 13 Dec 2013 03:07:59 +0200
Subject: [PATCH 31/96] be/allocator: now uses m0_be_fl

---
 be/alloc.c | 171 +++++++++++++++++++++----------------------------------------
 be/alloc.h |   3 --
 2 files changed, 59 insertions(+), 115 deletions(-)

diff --git a/be/alloc.c b/be/alloc.c
index 63ce003..cee149b 100644
--- a/be/alloc.c
+++ b/be/alloc.c
@@ -34,6 +34,7 @@
 /**
  * @addtogroup be
  * @todo make a doxygen page
+ * @todo UPDATEME
  *
  * Overview
  *
@@ -258,7 +259,8 @@ M0_INTERNAL void m0_be_fl_destroy(struct m0_be_fl *fl, struct m0_be_tx *tx)
 	M0_BE_OP_SYNC(op, m0_be_list_destroy(&fl->bfl_free_list, tx, &op));
 }
 
-M0_INTERNAL unsigned long m0_be_fl_index(struct m0_be_fl *fl, m0_bcount_t size)
+M0_INTERNAL unsigned long m0_be_fl_index_round_up(struct m0_be_fl *fl,
+						  m0_bcount_t size)
 {
 	unsigned long index;
 
@@ -266,10 +268,13 @@ M0_INTERNAL unsigned long m0_be_fl_index(struct m0_be_fl *fl, m0_bcount_t size)
 	return index > M0_BE_FL_NR ? M0_BE_FL_NR : index;
 }
 
-M0_INTERNAL unsigned long m0_be_fl_index_chunk(struct m0_be_fl *fl,
-					       struct be_alloc_chunk *chunk)
+M0_INTERNAL unsigned long m0_be_fl_index_round_down_chunk(struct m0_be_fl *fl,
+						  struct be_alloc_chunk *chunk)
 {
-	return m0_be_fl_index(fl, chunk->bac_size);
+	unsigned long index;
+
+	index = chunk->bac_size / M0_BE_FL_STEP;
+	return index > M0_BE_FL_NR ? M0_BE_FL_NR : index;
 }
 
 M0_INTERNAL bool m0_be_fl__invariant(struct m0_be_fl *fl)
@@ -299,7 +304,8 @@ M0_INTERNAL bool m0_be_fl__invariant(struct m0_be_fl *fl)
 	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
 		m0_tlist_for(&chunks_free_tl,
 			     &be_fl_list(fl, i)->bl_list, chunk) {
-			if (_0C(m0_be_fl_index_chunk(fl, chunk) != i))
+			if (_0C(m0_be_fl_index_round_down_chunk(fl,
+								chunk) != i))
 				return false;
 		} m0_tlist_endfor;
 	}
@@ -340,7 +346,7 @@ M0_INTERNAL void m0_be_fl_add(struct m0_be_fl *fl,
 			      struct m0_be_tx *tx,
 			      struct be_alloc_chunk *chunk)
 {
-	unsigned long index = m0_be_fl_index_chunk(fl, chunk);
+	unsigned long index = m0_be_fl_index_round_down_chunk(fl, chunk);
 	bool	      empty;
 
 	M0_PRE_EX(m0_be_fl__invariant(fl));
@@ -360,7 +366,7 @@ M0_INTERNAL void m0_be_fl_del(struct m0_be_fl *fl,
 			      struct m0_be_tx *tx,
 			      struct be_alloc_chunk *chunk)
 {
-	unsigned long index = m0_be_fl_index_chunk(fl, chunk);
+	unsigned long index = m0_be_fl_index_round_down_chunk(fl, chunk);
 	bool	      empty;
 
 	M0_PRE_EX(m0_be_fl__invariant(fl));
@@ -391,14 +397,14 @@ M0_INTERNAL struct be_alloc_chunk *m0_be_fl_pick(struct m0_be_fl *fl,
 						 m0_bcount_t size)
 {
 	struct be_alloc_chunk *chunk;
-	struct be_alloc_chunk *next;
+	struct be_alloc_chunk *iter;
 	unsigned long	       index;
 	struct m0_be_list     *flist;
 	int		       i;
 
 	M0_PRE_EX(m0_be_fl__invariant(fl));
 
-	for (index = m0_be_fl_index(fl, size);
+	for (index = m0_be_fl_index_round_up(fl, size);
 	     index < ARRAY_SIZE(fl->bfl_free); ++index) {
 		if (!be_fl_list_is_empty(fl, index))
 			break;
@@ -408,16 +414,18 @@ M0_INTERNAL struct be_alloc_chunk *m0_be_fl_pick(struct m0_be_fl *fl,
 						    be_fl_list(fl, index);
 	chunk = flist == NULL ? NULL : chunks_free_tlist_head(&flist->bl_list);
 	if (index == M0_BE_FL_NR && chunk != NULL) {
-		next = chunk;
-		for (i = 0; i < M0_BE_FL_PICK_SCAN_LIMIT; ++i) {
-			if (chunk->bac_size == size)
+		chunk = NULL;
+		i = 0;
+		m0_tlist_for(&chunks_free_tl, &flist->bl_list, iter) {
+			if (iter->bac_size > size &&
+			    ergo(chunk != NULL,
+				 chunk->bac_size > iter->bac_size)) {
+				chunk = iter;
+			}
+			++i;
+			if (i >= M0_BE_FL_PICK_SCAN_LIMIT && chunk != NULL)
 				break;
-			next = chunks_free_tlist_next(&flist->bl_list, chunk);
-			if (next == NULL)
-				break;
-			if (next->bac_size < chunk->bac_size)
-				chunk = next;
-		}
+		} m0_tlist_endfor;
 	}
 	M0_POST(ergo(chunk != NULL, chunk->bac_size >= size));
 	return chunk;
@@ -436,8 +444,6 @@ M0_INTERNAL void m0_be_fl_credit(struct m0_be_fl *fl,
 	case M0_BFL_ADD:
 	case M0_BFL_DEL:
 	case M0_BFL_RESIZE:
-	case M0_BFL_TLINK_CREATE:
-	case M0_BFL_TLINK_DESTROY:
 		m0_be_tx_credit_add(accum, &M0_BE_TX_CREDIT(20, 0x100));
 		break;
 	default:
@@ -451,11 +457,6 @@ static struct m0_be_list *be_alloc_list_chunks(struct m0_be_allocator *a)
 	return &a->ba_chunks;
 }
 
-static struct m0_be_list *be_alloc_list_free(struct m0_be_allocator *a)
-{
-	return &a->ba_free;
-}
-
 static struct m0_be_fl *be_fl(struct m0_be_allocator *a)
 {
 	return &a->ba_fl;
@@ -521,18 +522,9 @@ static bool be_alloc_chunk_invariant(struct m0_be_allocator *a,
 {
 	struct be_alloc_chunk *cprev;
 	struct be_alloc_chunk *cnext;
-	struct be_alloc_chunk *fprev;
-	struct be_alloc_chunk *fnext;
 
 	cprev = chunks_all_tlist_prev(&a->ba_chunks.bl_list, c);
 	cnext = chunks_all_tlist_next(&a->ba_chunks.bl_list, c);
-	if (c->bac_free) {
-		fprev = chunks_free_tlist_prev(&a->ba_free.bl_list, c);
-		fnext = chunks_free_tlist_next(&a->ba_free.bl_list, c);
-	} else {
-		fprev = NULL;
-		fnext = NULL;
-	}
 
 	return _0C(c != NULL) &&
 	       _0C(be_alloc_is_chunk_in_allocator(a, c)) &&
@@ -540,20 +532,10 @@ static bool be_alloc_chunk_invariant(struct m0_be_allocator *a,
 			be_alloc_is_chunk_in_allocator(a, cnext))) &&
 	       _0C(ergo(cprev != NULL,
 			be_alloc_is_chunk_in_allocator(a, cprev))) &&
-	       _0C(ergo(fnext != NULL,
-			be_alloc_is_chunk_in_allocator(a, fnext))) &&
-	       _0C(ergo(fprev != NULL,
-			be_alloc_is_chunk_in_allocator(a, fprev))) &&
 	       _0C(c->bac_magic0 == M0_BE_ALLOC_MAGIC0) &&
 	       _0C(c->bac_magic1 == M0_BE_ALLOC_MAGIC1) &&
 	       _0C(be_alloc_chunk_is_not_overlapping(cprev, c)) &&
-	       _0C(be_alloc_chunk_is_not_overlapping(c, cnext)) &&
-	       _0C(be_alloc_chunk_is_not_overlapping(fprev, c)) &&
-	       _0C(be_alloc_chunk_is_not_overlapping(c, fnext)) &&
-	       _0C(ergo(fprev != cprev,
-		    be_alloc_chunk_is_not_overlapping(fprev, cprev))) &&
-	       _0C(ergo(cnext != fnext,
-		    be_alloc_chunk_is_not_overlapping(cnext, fnext)));
+	       _0C(be_alloc_chunk_is_not_overlapping(c, cnext));
 }
 
 static void be_alloc_chunk_init(struct m0_be_allocator *a,
@@ -569,8 +551,6 @@ static void be_alloc_chunk_init(struct m0_be_allocator *a,
 	};
 	M0_BE_OP_SYNC(op,
 		      m0_be_tlink_create(c, tx, &op, be_alloc_list_chunks(a)));
-	M0_BE_OP_SYNC(op,
-		      m0_be_tlink_create(c, tx, &op, be_alloc_list_free(a)));
 	/*
 	 * Move this right before m0_be_tlink_create() to optimize capturing
 	 * size. Chunk capturing at the end of the function will help with
@@ -586,11 +566,10 @@ static void be_alloc_chunk_del_fini(struct m0_be_allocator *a,
 {
 	M0_PRE(be_alloc_chunk_invariant(a, c));
 
-	M0_BE_OP_SYNC(op, m0_be_list_del(be_alloc_list_free(a),	  &op, tx, c));
+	m0_be_fl_del(be_fl(a), tx, c);
+
 	M0_BE_OP_SYNC(op, m0_be_list_del(be_alloc_list_chunks(a), &op, tx, c));
 	M0_BE_OP_SYNC(op, m0_be_tlink_destroy(c, tx, &op,
-					      be_alloc_list_free(a)));
-	M0_BE_OP_SYNC(op, m0_be_tlink_destroy(c, tx, &op,
 					      be_alloc_list_chunks(a)));
 }
 
@@ -623,24 +602,11 @@ static void be_alloc_chunk_mark_free(struct m0_be_allocator *a,
 				     struct m0_be_tx *tx,
 				     struct be_alloc_chunk *c)
 {
-	struct be_alloc_chunk *next;
-	struct m0_be_list     *cfree = be_alloc_list_free(a);
-
 	M0_PRE(be_alloc_chunk_invariant(a, c));
 	M0_PRE(!c->bac_free);
-	/* scan forward until free chunk found */
-	for (next = c; next != NULL; next = be_alloc_chunk_next(a, next)) {
-		if (next->bac_free)
-			break;
-	}
+	m0_be_fl_add(be_fl(a), tx, c);
 	c->bac_free = true;
 	be_alloc_free_flag_capture(a, tx, c);
-	if (next == NULL) {
-		M0_BE_OP_SYNC(op, m0_be_list_add_tail(cfree, &op, tx, c));
-	} else {
-		M0_BE_OP_SYNC(op,
-			      m0_be_list_add_before(cfree, &op, tx, next, c));
-	}
 	M0_POST(c->bac_free);
 	M0_POST(be_alloc_chunk_invariant(a, c));
 }
@@ -657,16 +623,13 @@ static struct be_alloc_chunk *
 be_alloc_chunk_add_after(struct m0_be_allocator *a,
 			 struct m0_be_tx *tx,
 			 struct be_alloc_chunk *c,
-			 struct be_alloc_chunk *f,
 			 uintptr_t offset,
 			 m0_bcount_t size_total, bool free)
 {
 	struct m0_be_list     *chunks = be_alloc_list_chunks(a);
-	struct m0_be_list     *cfree  = be_alloc_list_free(a);
 	struct be_alloc_chunk *new;
 
 	M0_PRE(ergo(c != NULL, be_alloc_chunk_invariant(a, c)));
-	M0_PRE(ergo(free && f != NULL, be_alloc_chunk_invariant(a, f)));
 	M0_PRE(size_total > sizeof *new);
 
 	new = c == NULL ? (struct be_alloc_chunk *)
@@ -682,16 +645,9 @@ be_alloc_chunk_add_after(struct m0_be_allocator *a,
 		M0_BE_OP_SYNC(op, m0_be_list_add(chunks, &op, tx, new));
 	}
 	if (free) {
-		/** add chunk to m0_be_allocator.bac_free list */
-		if (f != NULL) {
-			M0_BE_OP_SYNC(op, m0_be_list_add_after(cfree, &op,
-							       tx, f, new));
-		} else {
-			M0_BE_OP_SYNC(op, m0_be_list_add(cfree, &op, tx, new));
-		}
+		m0_be_fl_add(be_fl(a), tx, new);
 	}
 	M0_POST(be_alloc_chunk_invariant(a, new));
-	M0_POST(ergo(free && f != NULL, be_alloc_chunk_invariant(a, f)));
 	M0_PRE(ergo(c != NULL, be_alloc_chunk_invariant(a, c)));
 	return new;
 }
@@ -703,7 +659,6 @@ be_alloc_chunk_split(struct m0_be_allocator *a,
 		     uintptr_t start_new, m0_bcount_t size)
 {
 	struct be_alloc_chunk *prev;
-	struct be_alloc_chunk *prev_free;
 	struct be_alloc_chunk *new;
 	const m0_bcount_t      chunk_size = sizeof *c;
 	uintptr_t	       start0;
@@ -715,8 +670,7 @@ be_alloc_chunk_split(struct m0_be_allocator *a,
 	M0_PRE(be_alloc_chunk_invariant(a, c));
 	M0_PRE(c->bac_free);
 
-	prev	  = be_alloc_chunk_prev(a, c);
-	prev_free = chunks_free_tlist_prev(&a->ba_free.bl_list, c);
+	prev	    = be_alloc_chunk_prev(a, c);
 
 	start0	    = be_alloc_chunk_after(a, prev);
 	start1	    = start_new + chunk_size + size;
@@ -733,18 +687,23 @@ be_alloc_chunk_split(struct m0_be_allocator *a,
 	if (chunk0_size <= chunk_size) {
 		/* no space for chunk0 */
 		if (prev != NULL) {
-			prev->bac_size += chunk0_size;
+			/* XXX refactor next if as func */
+			if (prev->bac_free) {
+				m0_be_fl_resize(be_fl(a), tx, prev,
+						prev->bac_size + chunk0_size);
+			} else {
+				prev->bac_size += chunk0_size;
+			}
 			be_alloc_size_capture(a, tx, prev);
 		} else
 			; /* space before the first chunk is temporary lost */
 	} else {
-		prev_free = be_alloc_chunk_add_after(a, tx, prev, prev_free,
-						     0, chunk0_size, true);
-		prev = prev_free;
+		prev = be_alloc_chunk_add_after(a, tx, prev,
+						0, chunk0_size, true);
 	}
 
 	/* add the new chunk */
-	new = be_alloc_chunk_add_after(a, tx, prev, NULL,
+	new = be_alloc_chunk_add_after(a, tx, prev,
 				       prev == NULL ? chunk0_size : 0,
 				       chunk_size + size, false);
 	M0_ASSERT(new != NULL);
@@ -754,7 +713,7 @@ be_alloc_chunk_split(struct m0_be_allocator *a,
 		new->bac_size += chunk1_size;
 		be_alloc_size_capture(a, tx, new);
 	} else {
-		be_alloc_chunk_add_after(a, tx, new, prev_free,
+		be_alloc_chunk_add_after(a, tx, new,
 					 0, chunk1_size, true);
 	}
 
@@ -805,12 +764,11 @@ static bool be_alloc_chunk_trymerge(struct m0_be_allocator *a,
 	M0_PRE(ergo(x != NULL, be_alloc_chunk_invariant(a, x)));
 	M0_PRE(ergo(y != NULL, be_alloc_chunk_invariant(a, y)));
 	M0_PRE(ergo(x != NULL && y != NULL, (char *) x < (char *) y));
-	M0_PRE(ergo(x != NULL, chunks_free_tlink_is_in(x)) ||
-	       ergo(y != NULL, chunks_free_tlink_is_in(y)));
+	M0_PRE(ergo(x != NULL, x->bac_free) || ergo(y != NULL, y->bac_free));
 	if (x != NULL && y != NULL && x->bac_free && y->bac_free) {
 		y_size_total = sizeof(*y) + y->bac_size;
 		be_alloc_chunk_del_fini(a, tx, y);
-		x->bac_size += y_size_total;
+		m0_be_fl_resize(be_fl(a), tx, x, x->bac_size + y_size_total);
 		be_alloc_size_capture(a, tx, x);
 		chunks_were_merged = true;
 	}
@@ -827,7 +785,6 @@ M0_INTERNAL int m0_be_allocator_init(struct m0_be_allocator *a,
 
 	a->ba_seg = seg;
 	m0_be_list_init(be_alloc_list_chunks(a), seg);
-	m0_be_list_init(be_alloc_list_free(a), seg);
 	m0_be_fl_init(be_fl(a), seg);
 
 	/* XXX temporary solution to make capturing checkers pass */
@@ -839,7 +796,6 @@ M0_INTERNAL int m0_be_allocator_init(struct m0_be_allocator *a,
 M0_INTERNAL void m0_be_allocator_fini(struct m0_be_allocator *a)
 {
 	m0_be_fl_fini(be_fl(a));
-	m0_be_list_fini(be_alloc_list_free(a));
 	m0_be_list_fini(be_alloc_list_chunks(a));
 	m0_mutex_fini(&a->ba_lock);
 
@@ -860,12 +816,6 @@ M0_INTERNAL bool m0_be_allocator__invariant(struct m0_be_allocator *a)
 			break;
 		}
 	} m0_tl_endfor;
-	m0_tl_for(chunks_free, &a->ba_free.bl_list, iter) {
-		if (!be_alloc_chunk_invariant(a, iter) && !success) {
-			success = false;
-			break;
-		}
-	} m0_tl_endfor;
 
 	m0_mutex_unlock(&a->ba_lock);
 
@@ -899,11 +849,9 @@ M0_INTERNAL int m0_be_allocator_create(struct m0_be_allocator *a,
 	};
 	M0_BE_OP_SYNC(op, m0_be_list_create(be_alloc_list_chunks(a), tx, &op,
 					    seg, &chunks_all_tl));
-	M0_BE_OP_SYNC(op, m0_be_list_create(be_alloc_list_free(a), tx, &op,
-					    seg, &chunks_free_tl));
 	m0_be_fl_create(be_fl(a), tx, seg);
 	/* init main chunk */
-	c = be_alloc_chunk_add_after(a, tx, NULL, NULL, 0, free_space, true);
+	c = be_alloc_chunk_add_after(a, tx, NULL, 0, free_space, true);
 	M0_ASSERT(c != NULL);
 
 	be_alloc_head_capture(a, tx);
@@ -925,13 +873,12 @@ M0_INTERNAL void m0_be_allocator_destroy(struct m0_be_allocator *a,
 
 	/** @todo GET_PTR h */
 	m0_mutex_lock(&a->ba_lock);
-	m0_be_fl_destroy(be_fl(a), tx);
 	c = chunks_all_tlist_head(&a->ba_chunks.bl_list);
 
 	be_alloc_chunk_del_fini(a, tx, c);
 
+	m0_be_fl_destroy(be_fl(a), tx);
 	M0_BE_OP_SYNC(op, m0_be_list_destroy(be_alloc_list_chunks(a), tx, &op));
-	M0_BE_OP_SYNC(op, m0_be_list_destroy(be_alloc_list_free(a), tx, &op));
 
 	a->ba_size = 0;
 	be_alloc_head_capture(a, tx);
@@ -961,7 +908,6 @@ M0_INTERNAL void m0_be_allocator_credit(struct m0_be_allocator *a,
 	struct m0_be_tx_credit tmp;
 	struct be_alloc_chunk  chunk;
 	struct m0_be_list      *chunks = be_alloc_list_chunks(a);
-	struct m0_be_list      *free   = be_alloc_list_free(a);
 
 	chunk_credit	= M0_BE_TX_CREDIT_TYPE(struct be_alloc_chunk);
 	cred_free_flag	= M0_BE_TX_CREDIT_PTR(&chunk.bac_free);
@@ -979,37 +925,38 @@ M0_INTERNAL void m0_be_allocator_credit(struct m0_be_allocator *a,
 	m0_be_list_credit(NULL, M0_BLO_DESTROY, 1, &cred_list_destroy);
 
 	tmp = M0_BE_TX_CREDIT(0, 0);
-	m0_be_list_credit(free,	  M0_BLO_TLINK_CREATE, 1, &tmp);
 	m0_be_list_credit(chunks, M0_BLO_TLINK_CREATE, 1, &tmp);
 	m0_be_tx_credit_add(&tmp, &chunk_credit);
-	m0_be_list_credit(free,	  M0_BLO_INSERT,       1, &tmp);
+	m0_be_fl_credit(be_fl(a), M0_BFL_ADD, &tmp);
 	m0_be_list_credit(chunks, M0_BLO_INSERT,       1, &tmp);
 	chunk_add_after_credit = tmp;
 
 	tmp = M0_BE_TX_CREDIT(0, 0);
-	m0_be_list_credit(free,	  M0_BLO_DELETE,	1, &tmp);
 	m0_be_list_credit(chunks, M0_BLO_DELETE,	1, &tmp);
-	m0_be_list_credit(free,	  M0_BLO_TLINK_DESTROY, 1, &tmp);
 	m0_be_list_credit(chunks, M0_BLO_TLINK_DESTROY, 1, &tmp);
+	m0_be_fl_credit(be_fl(a), M0_BFL_DEL, &tmp);
 	chunk_del_fini_credit = tmp;
 
 	m0_be_tx_credit_add(&chunk_trymerge_credit, &chunk_del_fini_credit);
+	m0_be_fl_credit(be_fl(a), M0_BFL_RESIZE, &chunk_trymerge_credit);
 	m0_be_tx_credit_add(&chunk_trymerge_credit, &cred_chunk_size);
 
 	m0_be_tx_credit_add(&cred_split, &chunk_del_fini_credit);
+	m0_be_fl_credit(be_fl(a), M0_BFL_RESIZE, &cred_split);
 	m0_be_tx_credit_mac(&cred_split, &cred_chunk_size, 2);
 	m0_be_tx_credit_mac(&cred_split, &chunk_add_after_credit, 3);
 
-	m0_be_list_credit(free, M0_BLO_INSERT, 1, &cred_mark_free);
 	m0_be_tx_credit_add(&cred_mark_free, &cred_free_flag);
 
 	switch (optype) {
 		case M0_BAO_CREATE:
 			m0_be_tx_credit_mac(accum, &cred_list_create, 2);
+			m0_be_fl_credit(be_fl(a), M0_BFL_CREATE, accum);
 			m0_be_tx_credit_add(accum, &chunk_add_after_credit);
 			m0_be_tx_credit_add(accum, &cred_allocator);
 			break;
 		case M0_BAO_DESTROY:
+			m0_be_fl_credit(be_fl(a), M0_BFL_DESTROY, accum);
 			m0_be_tx_credit_add(accum, &chunk_del_fini_credit);
 			m0_be_tx_credit_mac(accum, &cred_list_destroy, 2);
 			m0_be_tx_credit_add(accum, &cred_allocator);
@@ -1045,8 +992,8 @@ M0_INTERNAL void m0_be_alloc_aligned(struct m0_be_allocator *a,
 				     m0_bcount_t size,
 				     unsigned shift)
 {
-	struct be_alloc_chunk *iter;
 	struct be_alloc_chunk *c = NULL;
+	m0_bcount_t	       aligned_size;
 
 	M0_PRE_EX(m0_be_allocator__invariant(a));
 	shift = max_check(shift, (unsigned) M0_BE_ALLOC_SHIFT_MIN);
@@ -1056,11 +1003,11 @@ M0_INTERNAL void m0_be_alloc_aligned(struct m0_be_allocator *a,
 
 	m0_mutex_lock(&a->ba_lock);
 	/* algorithm starts here */
-	m0_tl_for(chunks_free, &a->ba_free.bl_list, iter) {
-		c = be_alloc_chunk_trysplit(a, tx, iter, size, shift);
-		if (c != NULL)
-			break;
-	} m0_tl_endfor;
+	aligned_size = shift == M0_BE_ALLOC_SHIFT_MIN ? size :
+		       size * 2 + (1U << shift);
+	c = m0_be_fl_pick(be_fl(a), aligned_size);
+	c = be_alloc_chunk_trysplit(a, tx, c, size, shift);
+	M0_ASSERT(c != NULL);	/* XXX */
 	if (c != NULL && tx != NULL) {
 		memset(&c->bac_mem, 0, size);
 		m0_be_tx_capture(tx, &M0_BE_REG(a->ba_seg, size, &c->bac_mem));
diff --git a/be/alloc.h b/be/alloc.h
index 2509891..db59218 100644
--- a/be/alloc.h
+++ b/be/alloc.h
@@ -50,8 +50,6 @@ enum {
 enum m0_be_fl_op {
 	M0_BFL_CREATE,
 	M0_BFL_DESTROY,
-	M0_BFL_TLINK_CREATE,
-	M0_BFL_TLINK_DESTROY,
 	M0_BFL_ADD,
 	M0_BFL_DEL,
 	M0_BFL_RESIZE,
@@ -137,7 +135,6 @@ struct m0_be_allocator {
 	 */
 	struct m0_mutex		       ba_lock;
 	struct m0_be_list	       ba_chunks;	/**< all chunks */
-	struct m0_be_list	       ba_free;		/**< free chunks */
 	struct m0_be_allocator_stats   ba_stats;	/**< XXX not used now */
 	m0_bcount_t		       ba_size;		/**< memory size */
 	void			      *ba_addr;		/**< memory address */
-- 
1.8.3.2

