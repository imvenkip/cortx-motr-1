From fb6a11007484eaa2e56a997679cb81add8319175 Mon Sep 17 00:00:00 2001
From: Maxim Medved <max_medved@xyratex.com>
Date: Thu, 9 Jan 2014 06:55:46 +0200
Subject: [PATCH 43/96] be/alloc: invariants should take const params

---
 be/alloc.c | 33 ++++++++++++++-------------------
 be/alloc.h |  4 +++-
 2 files changed, 17 insertions(+), 20 deletions(-)

diff --git a/be/alloc.c b/be/alloc.c
index 94aea9d..7fed892 100644
--- a/be/alloc.c
+++ b/be/alloc.c
@@ -508,14 +508,14 @@ static void be_alloc_size_capture(struct m0_be_allocator *a,
 	M0_BE_TX_CAPTURE_PTR(a->ba_seg, tx, &c->bac_size);
 }
 
-static bool be_alloc_is_mem_in_allocator(struct m0_be_allocator *a,
+static bool be_alloc_is_mem_in_allocator(const struct m0_be_allocator *a,
 					 const void *ptr, m0_bcount_t size)
 {
 	return ptr >= a->ba_addr &&
 	       ptr + size <= a->ba_addr + a->ba_size;
 }
 
-static bool be_alloc_is_chunk_in_allocator(struct m0_be_allocator *a,
+static bool be_alloc_is_chunk_in_allocator(const struct m0_be_allocator *a,
 					   const struct be_alloc_chunk *c)
 {
 	return be_alloc_is_mem_in_allocator(a, c, sizeof *c + c->bac_size);
@@ -528,7 +528,7 @@ static bool be_alloc_chunk_is_not_overlapping(const struct be_alloc_chunk *a,
 	       (a < b && &a->bac_mem[a->bac_size] <= (char *) b);
 }
 
-static bool be_alloc_chunk_invariant(struct m0_be_allocator *a,
+static bool be_alloc_chunk_invariant(const struct m0_be_allocator *a,
 				     const struct be_alloc_chunk *c)
 {
 	struct be_alloc_chunk *cprev;
@@ -812,13 +812,11 @@ M0_INTERNAL void m0_be_allocator_fini(struct m0_be_allocator *a)
 	m0_be_reg__write(&M0_BE_REG_PTR(a->ba_seg, a));	/* XXX */
 }
 
-M0_INTERNAL bool m0_be_allocator__invariant(struct m0_be_allocator *a)
+M0_INTERNAL bool m0_be_allocator__invariant(const struct m0_be_allocator *a)
 {
 	struct be_alloc_chunk *iter;
 	bool		       success = true;
 
-	m0_mutex_lock(&a->ba_lock);
-
 	m0_tl_for(chunks_all, &a->ba_chunks.bl_list, iter) {
 		if (!be_alloc_chunk_invariant(a, iter)) {
 			success = false;
@@ -826,8 +824,6 @@ M0_INTERNAL bool m0_be_allocator__invariant(struct m0_be_allocator *a)
 		}
 	} m0_tl_endfor;
 
-	m0_mutex_unlock(&a->ba_lock);
-
 	return success;
 }
 
@@ -863,11 +859,10 @@ M0_INTERNAL int m0_be_allocator_create(struct m0_be_allocator *a,
 
 	be_alloc_head_capture(a, tx);
 
-	m0_mutex_unlock(&a->ba_lock);
-
 	/** @todo PUT_PTR h */
-
 	M0_POST_EX(m0_be_allocator__invariant(a));
+	m0_mutex_unlock(&a->ba_lock);
+
 	return 0;
 }
 
@@ -876,10 +871,10 @@ M0_INTERNAL void m0_be_allocator_destroy(struct m0_be_allocator *a,
 {
 	struct be_alloc_chunk *c;
 
-	M0_PRE_EX(m0_be_allocator__invariant(a));
-
 	/** @todo GET_PTR h */
 	m0_mutex_lock(&a->ba_lock);
+	M0_PRE_EX(m0_be_allocator__invariant(a));
+
 	c = chunks_all_tlist_head(&a->ba_chunks.bl_list);
 
 	be_alloc_chunk_del_fini(a, tx, c);
@@ -1002,13 +997,14 @@ M0_INTERNAL void m0_be_alloc_aligned(struct m0_be_allocator *a,
 	struct be_alloc_chunk *c = NULL;
 	m0_bcount_t	       aligned_size;
 
-	M0_PRE_EX(m0_be_allocator__invariant(a));
 	shift = max_check(shift, (unsigned) M0_BE_ALLOC_SHIFT_MIN);
 
 	/* XXX */
 	m0_be_op_state_set(op, M0_BOS_ACTIVE);
 
 	m0_mutex_lock(&a->ba_lock);
+	M0_PRE_EX(m0_be_allocator__invariant(a));
+
 	/* algorithm starts here */
 	aligned_size = shift == M0_BE_ALLOC_SHIFT_MIN ? size :
 		       size * 2 + (1U << shift);
@@ -1034,9 +1030,9 @@ M0_INTERNAL void m0_be_alloc_aligned(struct m0_be_allocator *a,
 	 * unlock mutex after post-conditions which are using allocator
 	 * internals
 	 */
+	M0_POST_EX(m0_be_allocator__invariant(a));
 	m0_mutex_unlock(&a->ba_lock);
 
-	M0_POST_EX(m0_be_allocator__invariant(a));
 	M0_POST(equi(op->bo_u.u_allocator.a_ptr != NULL,
 		     op->bo_u.u_allocator.a_rc == 0));
 
@@ -1068,13 +1064,13 @@ M0_INTERNAL void m0_be_free_aligned(struct m0_be_allocator *a,
 	struct be_alloc_chunk *next;
 	bool		       chunks_were_merged;
 
-	M0_PRE_EX(m0_be_allocator__invariant(a));
 	M0_PRE(ergo(ptr != NULL, be_alloc_is_mem_in_allocator(a, ptr, 1)));
 
 	m0_be_op_state_set(op, M0_BOS_ACTIVE);
 
 	if (ptr != NULL) {
 		m0_mutex_lock(&a->ba_lock);
+		M0_PRE_EX(m0_be_allocator__invariant(a));
 
 		c = be_alloc_chunk_addr(ptr);
 		M0_PRE(be_alloc_chunk_invariant(a, c));
@@ -1092,12 +1088,12 @@ M0_INTERNAL void m0_be_free_aligned(struct m0_be_allocator *a,
 		M0_POST(c->bac_size > 0);
 		M0_POST(be_alloc_chunk_invariant(a, c));
 
+		M0_POST_EX(m0_be_allocator__invariant(a));
 		m0_mutex_unlock(&a->ba_lock);
 	}
 
 	m0_be_op_state_set(op, M0_BOS_SUCCESS);
 
-	M0_POST_EX(m0_be_allocator__invariant(a));
 }
 
 M0_INTERNAL void m0_be_free(struct m0_be_allocator *a,
@@ -1111,10 +1107,9 @@ M0_INTERNAL void m0_be_free(struct m0_be_allocator *a,
 M0_INTERNAL void m0_be_alloc_stats(struct m0_be_allocator *a,
 				   struct m0_be_allocator_stats *out)
 {
-	M0_PRE_EX(m0_be_allocator__invariant(a));
-
 	/** @todo GET_PTR a->ba_h */
 	m0_mutex_lock(&a->ba_lock);
+	M0_PRE_EX(m0_be_allocator__invariant(a));
 	*out = a->ba_stats;
 	m0_mutex_unlock(&a->ba_lock);
 	/** @todo PUT_PTR a->ba_h */
diff --git a/be/alloc.h b/be/alloc.h
index 168a453..c7e2b20 100644
--- a/be/alloc.h
+++ b/be/alloc.h
@@ -161,8 +161,10 @@ M0_INTERNAL void m0_be_allocator_fini(struct m0_be_allocator *a);
  *
  * It performs detailed verification of allocator data structures.
  * It ignores all user data.
+ *
+ * @note It doesn't take allocator lock.
  */
-M0_INTERNAL bool m0_be_allocator__invariant(struct m0_be_allocator *a);
+M0_INTERNAL bool m0_be_allocator__invariant(const struct m0_be_allocator *a);
 
 /**
  * Create allocator on the segment.
-- 
1.8.3.2

