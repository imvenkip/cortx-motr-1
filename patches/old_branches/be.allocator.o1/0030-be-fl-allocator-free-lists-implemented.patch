From 3fe58ce7954274770c02793e3542b1a5364e6733 Mon Sep 17 00:00:00 2001
From: Maxim Medved <max_medved@xyratex.com>
Date: Fri, 13 Dec 2013 01:20:02 +0200
Subject: [PATCH 30/96] be/fl: allocator free lists implemented

---
 be/alloc.c    | 280 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++-
 be/alloc.h    |  60 +++++++++++++
 be/ut/alloc.c | 117 ++++++++++++++++++++++++
 be/ut/main.c  |   2 +
 4 files changed, 458 insertions(+), 1 deletion(-)

diff --git a/be/alloc.c b/be/alloc.c
index 532204b..63ce003 100644
--- a/be/alloc.c
+++ b/be/alloc.c
@@ -170,12 +170,281 @@ M0_TL_DESCR_DEFINE(chunks_all, "list of all chunks in m0_be_allocator",
 		   M0_BE_ALLOC_ALL_LINK_MAGIC, M0_BE_ALLOC_ALL_MAGIC);
 M0_TL_DEFINE(chunks_all, static, struct be_alloc_chunk);
 
-M0_TL_DESCR_DEFINE(chunks_free, "list of free chunks in m0_be_allocator",
+M0_TL_DESCR_DEFINE(chunks_free, "XXX list of free chunks in m0_be_allocator",
 		   static, struct be_alloc_chunk,
 		   bac_linkage_free, bac_magic_free,
 		   M0_BE_ALLOC_FREE_LINK_MAGIC, M0_BE_ALLOC_FREE_MAGIC);
 M0_TL_DEFINE(chunks_free, static, struct be_alloc_chunk);
 
+M0_TL_DESCR_DEFINE(chunk_sizes, "DOCUMENTME",
+		   static, struct m0_be_fl_size,
+		   bfs_size_link, bfs_size_link_magic,
+	/* XXX */  M0_BE_ALLOC_FREE_LINK_MAGIC + 1, M0_BE_ALLOC_FREE_MAGIC + 1);
+M0_TL_DEFINE(chunk_sizes, static, struct be_alloc_chunk);
+
+static struct m0_be_list *be_fl_list(struct m0_be_fl *fl, unsigned long index)
+{
+	M0_PRE(index < ARRAY_SIZE(fl->bfl_free));
+
+	return &fl->bfl_free[index].bfs_list;
+};
+
+static bool be_fl_list_is_empty(struct m0_be_fl *fl, unsigned long index)
+{
+	bool empty;
+
+	M0_BE_OP_SYNC(op,
+		      empty = m0_be_list_is_empty(be_fl_list(fl, index), &op));
+	return empty;
+}
+
+M0_INTERNAL void m0_be_fl_init(struct m0_be_fl *fl, struct m0_be_seg *seg)
+{
+	int i;
+
+	m0_be_list_init(&fl->bfl_free_list, seg);
+	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
+		m0_be_list_init(be_fl_list(fl, i), seg);
+		m0_be_tlink_init(be_fl_list(fl, i), &fl->bfl_free_list);
+	}
+
+	/* XXX temporary solution to make capturing checkers pass */
+	m0_be_reg__write(&M0_BE_REG_PTR(seg, fl));	/* XXX */
+}
+
+M0_INTERNAL void m0_be_fl_fini(struct m0_be_fl *fl)
+{
+	struct m0_be_seg *seg = fl->bfl_free_list.bl_seg;
+	int		  i;
+
+	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
+		m0_be_tlink_fini(be_fl_list(fl, i), &fl->bfl_free_list);
+		m0_be_list_fini(be_fl_list(fl, i));
+	}
+	m0_be_list_fini(&fl->bfl_free_list);
+
+	/* XXX temporary solution to make capturing checkers pass */
+	m0_be_reg__write(&M0_BE_REG_PTR(seg, fl));	/* XXX */
+}
+
+M0_INTERNAL void m0_be_fl_create(struct m0_be_fl *fl,
+				 struct m0_be_tx *tx,
+				 struct m0_be_seg *seg)
+{
+	int i;
+
+	M0_BE_OP_SYNC(op, m0_be_list_create(
+		    &fl->bfl_free_list, tx, &op, seg, &chunk_sizes_tl));
+	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
+		M0_BE_OP_SYNC(op, m0_be_list_create(
+			be_fl_list(fl, i), tx, &op, seg, &chunks_free_tl));
+		M0_BE_OP_SYNC(op,
+		      m0_be_tlink_create(be_fl_list(fl, i), tx, &op,
+					 &fl->bfl_free_list));
+	}
+}
+
+M0_INTERNAL void m0_be_fl_destroy(struct m0_be_fl *fl, struct m0_be_tx *tx)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
+		M0_BE_OP_SYNC(op,
+		      m0_be_tlink_destroy(be_fl_list(fl, i), tx, &op,
+					  &fl->bfl_free_list));
+		M0_BE_OP_SYNC(op, m0_be_list_destroy(be_fl_list(fl, i),
+						     tx, &op));
+	}
+	M0_BE_OP_SYNC(op, m0_be_list_destroy(&fl->bfl_free_list, tx, &op));
+}
+
+M0_INTERNAL unsigned long m0_be_fl_index(struct m0_be_fl *fl, m0_bcount_t size)
+{
+	unsigned long index;
+
+	index = m0_align(size, M0_BE_FL_STEP) / M0_BE_FL_STEP;
+	return index > M0_BE_FL_NR ? M0_BE_FL_NR : index;
+}
+
+M0_INTERNAL unsigned long m0_be_fl_index_chunk(struct m0_be_fl *fl,
+					       struct be_alloc_chunk *chunk)
+{
+	return m0_be_fl_index(fl, chunk->bac_size);
+}
+
+M0_INTERNAL bool m0_be_fl__invariant(struct m0_be_fl *fl)
+{
+	struct be_alloc_chunk *chunk;
+	struct m0_be_fl_size  *fl_size;
+	struct m0_be_fl_size  *prev;
+	bool		       empty;
+	int		       i;
+
+	/* check if there is only empty lists in bfl_free_list */
+	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
+		M0_BE_OP_SYNC(op, empty =
+			      m0_be_list_is_empty(be_fl_list(fl, i), &op));
+		if (_0C(equi(m0_tlink_is_in(&chunk_sizes_tl, &fl->bfl_free[i]),
+			     empty)))
+			return false;
+	}
+	/* check if bfl_free_list is ordered */
+	prev = NULL;
+	m0_tlist_for(&chunk_sizes_tl, &fl->bfl_free_list.bl_list, fl_size) {
+		if (_0C(prev >= fl_size))
+			return false;
+		prev = fl_size;
+	} m0_tlist_endfor;
+	/* check if each bfs_list contains chunks with appropriate size */
+	for (i = 0; i < ARRAY_SIZE(fl->bfl_free); ++i) {
+		m0_tlist_for(&chunks_free_tl,
+			     &be_fl_list(fl, i)->bl_list, chunk) {
+			if (_0C(m0_be_fl_index_chunk(fl, chunk) != i))
+				return false;
+		} m0_tlist_endfor;
+	}
+	return true;
+}
+
+/* XXX naming */
+M0_INTERNAL void m0_be_fl_add_size(struct m0_be_fl *fl,
+				   struct m0_be_tx *tx,
+				   unsigned long index)
+{
+	struct m0_be_fl_size *fl_size;
+	struct m0_be_fl_size *fl_size_new = &fl->bfl_free[index];
+
+	m0_tlist_for(&chunk_sizes_tl, &fl->bfl_free_list.bl_list, fl_size) {
+		if (fl_size > fl_size_new)
+			break;
+	} m0_tlist_endfor;
+	if (fl_size > fl_size_new) {
+		M0_BE_OP_SYNC(op, m0_be_list_add_before(&fl->bfl_free_list,
+							&op, tx, fl_size,
+							fl_size_new));
+	} else {
+		M0_BE_OP_SYNC(op, m0_be_list_add_tail(&fl->bfl_free_list,
+						      &op, tx, fl_size_new));
+	}
+}
+
+M0_INTERNAL void m0_be_fl_del_size(struct m0_be_fl *fl,
+				   struct m0_be_tx *tx,
+				   unsigned long index)
+{
+	M0_BE_OP_SYNC(op, m0_be_list_del(&fl->bfl_free_list,
+					 &op, tx, &fl->bfl_free[index]));
+}
+
+M0_INTERNAL void m0_be_fl_add(struct m0_be_fl *fl,
+			      struct m0_be_tx *tx,
+			      struct be_alloc_chunk *chunk)
+{
+	unsigned long index = m0_be_fl_index_chunk(fl, chunk);
+	bool	      empty;
+
+	M0_PRE_EX(m0_be_fl__invariant(fl));
+
+	empty = be_fl_list_is_empty(fl, index);
+	if (empty)
+		m0_be_fl_add_size(fl, tx, index);
+	M0_BE_OP_SYNC(op, m0_be_tlink_create(chunk, tx, &op,
+					     be_fl_list(fl, index)));
+	M0_BE_OP_SYNC(op, m0_be_list_add(be_fl_list(fl, index),
+					 &op, tx, chunk));
+
+	M0_POST_EX(m0_be_fl__invariant(fl));
+}
+
+M0_INTERNAL void m0_be_fl_del(struct m0_be_fl *fl,
+			      struct m0_be_tx *tx,
+			      struct be_alloc_chunk *chunk)
+{
+	unsigned long index = m0_be_fl_index_chunk(fl, chunk);
+	bool	      empty;
+
+	M0_PRE_EX(m0_be_fl__invariant(fl));
+
+	M0_BE_OP_SYNC(op, m0_be_list_del(be_fl_list(fl, index),
+					 &op, tx, chunk));
+	M0_BE_OP_SYNC(op, m0_be_tlink_destroy(chunk, tx, &op,
+					      be_fl_list(fl, index)));
+	empty = be_fl_list_is_empty(fl, index);
+	if (empty)
+		m0_be_fl_del_size(fl, tx, index);
+
+	M0_POST_EX(m0_be_fl__invariant(fl));
+}
+
+M0_INTERNAL void m0_be_fl_resize(struct m0_be_fl *fl,
+				 struct m0_be_tx *tx,
+				 struct be_alloc_chunk *chunk,
+				 m0_bcount_t size)
+{
+	m0_be_fl_del(fl, tx, chunk);
+	chunk->bac_size = size;
+	m0_be_fl_add(fl, tx, chunk);
+}
+
+/** find empty chunk with size at least 'size' */
+M0_INTERNAL struct be_alloc_chunk *m0_be_fl_pick(struct m0_be_fl *fl,
+						 m0_bcount_t size)
+{
+	struct be_alloc_chunk *chunk;
+	struct be_alloc_chunk *next;
+	unsigned long	       index;
+	struct m0_be_list     *flist;
+	int		       i;
+
+	M0_PRE_EX(m0_be_fl__invariant(fl));
+
+	for (index = m0_be_fl_index(fl, size);
+	     index < ARRAY_SIZE(fl->bfl_free); ++index) {
+		if (!be_fl_list_is_empty(fl, index))
+			break;
+	}
+
+	flist = index == ARRAY_SIZE(fl->bfl_free) ? NULL :
+						    be_fl_list(fl, index);
+	chunk = flist == NULL ? NULL : chunks_free_tlist_head(&flist->bl_list);
+	if (index == M0_BE_FL_NR && chunk != NULL) {
+		next = chunk;
+		for (i = 0; i < M0_BE_FL_PICK_SCAN_LIMIT; ++i) {
+			if (chunk->bac_size == size)
+				break;
+			next = chunks_free_tlist_next(&flist->bl_list, chunk);
+			if (next == NULL)
+				break;
+			if (next->bac_size < chunk->bac_size)
+				chunk = next;
+		}
+	}
+	M0_POST(ergo(chunk != NULL, chunk->bac_size >= size));
+	return chunk;
+}
+
+M0_INTERNAL void m0_be_fl_credit(struct m0_be_fl *fl,
+				 enum m0_be_fl_op fl_op,
+				 struct m0_be_tx_credit *accum)
+{
+	/* XXX make proper credit calculation */
+	switch (fl_op) {
+	case M0_BFL_CREATE:
+	case M0_BFL_DESTROY:
+		m0_be_tx_credit_add(accum, &M0_BE_TX_CREDIT(0x1000, 0x10000));
+		break;
+	case M0_BFL_ADD:
+	case M0_BFL_DEL:
+	case M0_BFL_RESIZE:
+	case M0_BFL_TLINK_CREATE:
+	case M0_BFL_TLINK_DESTROY:
+		m0_be_tx_credit_add(accum, &M0_BE_TX_CREDIT(20, 0x100));
+		break;
+	default:
+		M0_ASSERT_INFO(false, "fl_op = %d", fl_op);
+	}
+}
+
 /* XXX use these accessors everywhere */
 static struct m0_be_list *be_alloc_list_chunks(struct m0_be_allocator *a)
 {
@@ -187,6 +456,11 @@ static struct m0_be_list *be_alloc_list_free(struct m0_be_allocator *a)
 	return &a->ba_free;
 }
 
+static struct m0_be_fl *be_fl(struct m0_be_allocator *a)
+{
+	return &a->ba_fl;
+}
+
 static void be_alloc_chunk_capture(struct m0_be_allocator *a,
 				   struct m0_be_tx *tx,
 				   struct be_alloc_chunk *c)
@@ -554,6 +828,7 @@ M0_INTERNAL int m0_be_allocator_init(struct m0_be_allocator *a,
 	a->ba_seg = seg;
 	m0_be_list_init(be_alloc_list_chunks(a), seg);
 	m0_be_list_init(be_alloc_list_free(a), seg);
+	m0_be_fl_init(be_fl(a), seg);
 
 	/* XXX temporary solution to make capturing checkers pass */
 	m0_be_reg__write(&M0_BE_REG_PTR(a->ba_seg, a));	/* XXX */
@@ -563,6 +838,7 @@ M0_INTERNAL int m0_be_allocator_init(struct m0_be_allocator *a,
 
 M0_INTERNAL void m0_be_allocator_fini(struct m0_be_allocator *a)
 {
+	m0_be_fl_fini(be_fl(a));
 	m0_be_list_fini(be_alloc_list_free(a));
 	m0_be_list_fini(be_alloc_list_chunks(a));
 	m0_mutex_fini(&a->ba_lock);
@@ -625,6 +901,7 @@ M0_INTERNAL int m0_be_allocator_create(struct m0_be_allocator *a,
 					    seg, &chunks_all_tl));
 	M0_BE_OP_SYNC(op, m0_be_list_create(be_alloc_list_free(a), tx, &op,
 					    seg, &chunks_free_tl));
+	m0_be_fl_create(be_fl(a), tx, seg);
 	/* init main chunk */
 	c = be_alloc_chunk_add_after(a, tx, NULL, NULL, 0, free_space, true);
 	M0_ASSERT(c != NULL);
@@ -648,6 +925,7 @@ M0_INTERNAL void m0_be_allocator_destroy(struct m0_be_allocator *a,
 
 	/** @todo GET_PTR h */
 	m0_mutex_lock(&a->ba_lock);
+	m0_be_fl_destroy(be_fl(a), tx);
 	c = chunks_all_tlist_head(&a->ba_chunks.bl_list);
 
 	be_alloc_chunk_del_fini(a, tx, c);
diff --git a/be/alloc.h b/be/alloc.h
index 9e11c2f..2509891 100644
--- a/be/alloc.h
+++ b/be/alloc.h
@@ -40,6 +40,65 @@ struct m0_be_tx_credit;
  */
 
 enum {
+	M0_BE_FL_STEP = 8,	/**< each size is increased to this boundary */
+	M0_BE_FL_NR = 128,	/*<* number of free lists for each size */
+	M0_BE_FL_PICK_SCAN_LIMIT = 0x10, /**< scan limit for searching in
+					      chunks with size >=
+					      M0_BE_FL_NR * M0_BE_FL_STEP */
+};
+
+enum m0_be_fl_op {
+	M0_BFL_CREATE,
+	M0_BFL_DESTROY,
+	M0_BFL_TLINK_CREATE,
+	M0_BFL_TLINK_DESTROY,
+	M0_BFL_ADD,
+	M0_BFL_DEL,
+	M0_BFL_RESIZE,
+};
+
+/** m0_be free list for some size of chunks */
+struct m0_be_fl_size {
+	struct m0_be_list bfs_list;
+	struct m0_tlink	  bfs_size_link;
+	uint64_t	  bfs_size_link_magic;
+};
+
+/** m0_be free list */
+struct m0_be_fl {
+	struct m0_be_fl_size bfl_free[M0_BE_FL_NR + 1];
+	struct m0_be_list    bfl_free_list;
+};
+
+struct be_alloc_chunk;
+
+M0_INTERNAL void m0_be_fl_init(struct m0_be_fl *fl, struct m0_be_seg *seg);
+M0_INTERNAL void m0_be_fl_fini(struct m0_be_fl *fl);
+M0_INTERNAL bool m0_be_fl__invariant(struct m0_be_fl *fl);
+
+M0_INTERNAL void m0_be_fl_create(struct m0_be_fl *fl,
+				 struct m0_be_tx *tx,
+				 struct m0_be_seg *seg);
+M0_INTERNAL void m0_be_fl_destroy(struct m0_be_fl *fl, struct m0_be_tx *tx);
+
+M0_INTERNAL void m0_be_fl_add(struct m0_be_fl *fl,
+			      struct m0_be_tx *tx,
+			      struct be_alloc_chunk *chunk);
+M0_INTERNAL void m0_be_fl_del(struct m0_be_fl *fl,
+			      struct m0_be_tx *tx,
+			      struct be_alloc_chunk *chunk);
+M0_INTERNAL void m0_be_fl_resize(struct m0_be_fl *fl,
+				 struct m0_be_tx *tx,
+				 struct be_alloc_chunk *chunk,
+				 m0_bcount_t size);
+M0_INTERNAL struct be_alloc_chunk *m0_be_fl_pick(struct m0_be_fl *fl,
+						 m0_bcount_t size);
+
+M0_INTERNAL void m0_be_fl_credit(struct m0_be_fl *fl,
+				 enum m0_be_fl_op fl_op,
+				 struct m0_be_tx_credit *accum);
+
+enum {
 	/**
 	 * Allocated memory will be aligned using at least this shift.
 	 * @see m0_be_alloc(), m0_be_allocator_credit().
@@ -82,6 +141,7 @@ struct m0_be_allocator {
 	struct m0_be_allocator_stats   ba_stats;	/**< XXX not used now */
 	m0_bcount_t		       ba_size;		/**< memory size */
 	void			      *ba_addr;		/**< memory address */
+	struct m0_be_fl		       ba_fl;
 };
 
 /**
diff --git a/be/ut/alloc.c b/be/ut/alloc.c
index 7ca2c3e..4aea8ec 100644
--- a/be/ut/alloc.c
+++ b/be/ut/alloc.c
@@ -28,6 +28,7 @@
 #include "lib/thread.h"		/* m0_thread */
 #include "ut/ut.h"		/* M0_UT_ASSERT */
 #include "be/ut/helper.h"	/* m0_be_ut_backend */
+#include "be/alloc_internal.h"	/* be_alloc_chunk */
 
 #include <stdlib.h>		/* rand_r */
 #include <string.h>		/* memset */
@@ -229,6 +230,122 @@ M0_INTERNAL void m0_be_ut_alloc_info(void)
 	m0_be_ut_backend_fini(&be_ut_alloc_backend);
 }
 
+enum {
+	BE_UT_FL_CHUNK_NR  = 0x100,
+	BE_UT_FL_ITER	   = 0x800,
+	BE_UT_FL_SEG_SIZE  = 0x10000,
+	BE_UT_FL_OP_PER_TX = 0x10,
+	BE_UT_FL_SIZE_MAX  = M0_BE_FL_STEP * (M0_BE_FL_NR + 0x10),
+};
+
+static struct m0_be_ut_backend be_ut_fl_backend;
+
+static unsigned int be_ut_fl_rand(unsigned int max, unsigned int *seed)
+{
+	return rand_r(seed) % max;
+}
+
+static m0_bcount_t be_ut_fl_rand_size(unsigned int *seed)
+{
+	return be_ut_fl_rand(BE_UT_FL_SIZE_MAX, seed);
+}
+
+void m0_be_ut_fl(void)
+{
+	static struct m0_be_ut_backend *ut_be = &be_ut_fl_backend;
+	static struct m0_be_ut_seg	ut_seg;
+	static struct m0_be_tx		tx;
+	struct be_alloc_chunk	       *chunks;
+	int			       *chunks_used;
+	struct m0_be_tx_credit		cred;
+	struct m0_be_fl		       *fl;
+	struct m0_be_seg	       *seg = &ut_seg.bus_seg;
+	unsigned int			seed = 0;
+	void			       *addr;
+	int				i;
+	int				rc;
+	int				index;
+	bool				move_chunk;
+	m0_bcount_t			size;
+
+	m0_be_ut_backend_init(ut_be);
+	m0_be_ut_seg_init(&ut_seg, ut_be, BE_UT_FL_SEG_SIZE);
+
+	addr	= seg->bs_addr + m0_be_seg_reserved(seg);
+	fl	= (struct m0_be_fl *)addr;
+	addr   += sizeof *fl;
+	chunks	= (struct be_alloc_chunk *)addr;
+
+	m0_be_fl_init(fl, seg);
+	m0_be_ut_seg_check_persistence(&ut_seg);
+
+	M0_BE_UT_TRANSACT(ut_be, NULL, tx, cred,
+			  m0_be_fl_credit(NULL, M0_BFL_CREATE, &cred),
+			  m0_be_fl_create(fl, tx, seg));
+
+	M0_ALLOC_ARR(chunks_used, BE_UT_FL_CHUNK_NR);
+	M0_ASSERT(chunks_used != NULL);
+
+	for (i = 0; i < BE_UT_FL_ITER; ++i) {
+		if ((i % BE_UT_FL_OP_PER_TX) == 0) {
+			M0_SET0(&tx);
+			m0_be_ut_tx_init(&tx, ut_be);
+
+			cred = M0_BE_TX_CREDIT(0, 0);
+			/* XXX don't use the largest possible credit */
+			m0_be_fl_credit(fl, M0_BFL_RESIZE, &cred);
+			m0_be_tx_credit_mul(&cred, BE_UT_FL_OP_PER_TX);
+			m0_be_tx_prep(&tx, &cred);
+
+			rc = m0_be_tx_open_sync(&tx);
+			M0_ASSERT_INFO(rc == 0, "rc = %d", rc);
+		}
+		index = be_ut_fl_rand(BE_UT_FL_CHUNK_NR, &seed);
+		if (chunks_used[index]) {
+			/* del or resize */
+			move_chunk = be_ut_fl_rand(2, &seed) == 0;
+			if (move_chunk) {
+				size = be_ut_fl_rand_size(&seed);
+				m0_be_fl_resize(fl, &tx, &chunks[index], size);
+				M0_UT_ASSERT(chunks[index].bac_size == size);
+			} else {
+				m0_be_fl_del(fl, &tx, &chunks[index]);
+				chunks_used[index] = false;
+			}
+		} else {
+			/* add */
+			chunks[index].bac_size = be_ut_fl_rand_size(&seed);
+			m0_be_fl_add(fl, &tx, &chunks[index]);
+			chunks_used[index] = true;
+		}
+		if (i + 1 == BE_UT_FL_ITER ||
+		    ((i + 1) % BE_UT_FL_OP_PER_TX) == 0) {
+			m0_be_tx_close_sync(&tx);
+			m0_be_tx_fini(&tx);
+		}
+	}
+
+	for (i = 0; i < BE_UT_FL_CHUNK_NR; ++i) {
+		if (chunks_used[i]) {
+			M0_BE_UT_TRANSACT(ut_be, NULL, tx, cred,
+				  m0_be_fl_credit(NULL, M0_BFL_DEL, &cred),
+				  m0_be_fl_del(fl, tx, &chunks[i]));
+		}
+	}
+
+	m0_free(chunks_used);
+
+	M0_BE_UT_TRANSACT(ut_be, NULL, tx, cred,
+			  m0_be_fl_credit(NULL, M0_BFL_DESTROY, &cred),
+			  m0_be_fl_destroy(fl, tx));
+
+	m0_be_fl_fini(fl);
+	/* m0_be_ut_seg_check_persistence(&ut_seg); */
+
+	m0_be_ut_seg_fini(&ut_seg);
+	m0_be_ut_backend_fini(ut_be);
+}
+
 #undef M0_TRACE_SUBSYSTEM
 
 /*
diff --git a/be/ut/main.c b/be/ut/main.c
index b6c85c0..24aa25d 100644
--- a/be/ut/main.c
+++ b/be/ut/main.c
@@ -58,6 +58,7 @@ extern void m0_be_ut_tx_persistence(void);
 extern void m0_be_ut_tx_fast(void);
 extern void m0_be_ut_tx_concurrent(void);
 
+extern void m0_be_ut_fl(void);
 extern void m0_be_ut_alloc_init_fini(void);
 extern void m0_be_ut_alloc_create_destroy(void);
 extern void m0_be_ut_alloc_multiple(void);
@@ -101,6 +102,7 @@ const struct m0_test_suite be_ut = {
 		{ "tx-persistence",      m0_be_ut_tx_persistence       },
 		{ "tx-fast",             m0_be_ut_tx_fast              },
 		{ "tx-concurrent",	 m0_be_ut_tx_concurrent	       },
+		{ "fl",			 m0_be_ut_fl		       },
 		{ "alloc-init",          m0_be_ut_alloc_init_fini      },
 		{ "alloc-create",        m0_be_ut_alloc_create_destroy },
 		{ "alloc-multiple",      m0_be_ut_alloc_multiple       },
-- 
1.8.3.2

