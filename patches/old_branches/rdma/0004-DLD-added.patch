From 5e120056e428d44ad748384ab33a28c0776779a2 Mon Sep 17 00:00:00 2001
From: "jinshan.xiong" <jinshan.xiong@clusterstor.com>
Date: Tue, 13 Jul 2010 22:42:54 -0600
Subject: [PATCH 04/34] - DLD added.

---
 net/usunrpc/rfc5666/svc_rdma.c          |   8 +-
 net/usunrpc/rfc5666/svc_rdma.h          |  66 ++++++-
 net/usunrpc/rfc5666/svc_rdma_internal.h | 299 +++++++++++++++++++++++++++-----
 net/usunrpc/rfc5666/xdr_rdma.c          |   7 +-
 4 files changed, 323 insertions(+), 57 deletions(-)

diff --git a/net/usunrpc/rfc5666/svc_rdma.c b/net/usunrpc/rfc5666/svc_rdma.c
index c975465..f7fe64a 100644
--- a/net/usunrpc/rfc5666/svc_rdma.c
+++ b/net/usunrpc/rfc5666/svc_rdma.c
@@ -9,12 +9,6 @@
 #include <errno.h>
 #include <stdlib.h>
 
-#ifdef USE_IN_LIBIO
-# include <wchar.h>
-# include <libio/iolibio.h>
-#endif
-
-
 static int svcrdma_buffer_create(struct svcrdma_xprt *xprt);
 static void svcrdma_buffer_destroy(struct svcrdma_xprt *xprt);
 static int rdma_recv_completion(struct svcrdma_xprt *xprt,
@@ -64,7 +58,7 @@ static const struct xp_ops svcrdma_rendezvous_op = {
 /**
  * svcrdma_create: Create an rdma export and make it listen on port @port
  */
-SVCXPRT *svcrdma_create (int port, struct svcrdma_config *config)
+SVCXPRT *svcrdma_create (struct sockaddr_in *sa, struct svcrdma_config *cfg)
 {
         SVCXPRT *svcxprt;
         struct svcrndzv_xprt *xp;
diff --git a/net/usunrpc/rfc5666/svc_rdma.h b/net/usunrpc/rfc5666/svc_rdma.h
index 25e86cc..6dbed77 100644
--- a/net/usunrpc/rfc5666/svc_rdma.h
+++ b/net/usunrpc/rfc5666/svc_rdma.h
@@ -1,11 +1,71 @@
+#ifndef _SVC_RDMA_H_
+#define _SVC_RDMA_H_
 
+#include <rpc/rpc.h>
+
+/**
+  @addtogroup svcrdma
+  @{
+ */
+
+/**
+  svcrdma_config contains configuration parameters needed by rdma to set up
+  hardware correct to work. It also contains some configuration items which
+  should be the results of negotiation between client and server.
+
+  More items will be added during implmenting time.
+ */
 struct svcrdma_config {
-        int dummy;
+        /**
+          maximum buffer size of one request. Server should negotiate this
+          value with clients.
+         */
+        int max_buf_size;
 };
 
+/**
+  data buffer description in C2 service.
+
+  Whenever client wants to send a data buffer to server, the data buffer
+  must be encoded as a c2_dbuf. The most important field is @alignment which
+  indicates the alignment requirement of data.
+ */
 struct c2_dbuf {
-        bool_t (*xdr)(XDRS *, char *buffer, int buflen);
-        int alignment;
+        int alignment; /**< alignment of data buffer */
         int buflen;
         char *buffer;
 };
+
+/* external interfaces */
+/**
+  svcrdma_create tries to create a service on RDMA.
+
+  @param[in] sa  - socket address the rdma will listen on; rdma still uses
+                   ipoib to address endpoints
+  @param[in] cfg - rdma configuration values for rdma service
+
+  @retval NULL   - failed to create rdma service
+  @retval others - success
+ */
+SVCXPRT *svcrdma_create(struct sockaddr_in *sa, struct svcrdma_config *cfg);
+/**
+ */
+bool_t   xdr_dbuf      (XDRS *xdrs, struct c2_dbuf *dbuf);
+/**
+ */
+bool_t   xdr_rdma_chunks(XDRS *xdrs, char *buffer, int buflen);
+
+/** @} end group svcrdma */
+
+#endif /* ifndef _SVC_RDMA_H_ */
+
+/*
+ *  Local variables:
+ *  c-indentation-style: "K&R"
+ *  c-basic-offset: 8
+ *  tab-width: 8
+ *  fill-column: 80
+ *  scroll-step: 1
+ *  End:
+ */
+
diff --git a/net/usunrpc/rfc5666/svc_rdma_internal.h b/net/usunrpc/rfc5666/svc_rdma_internal.h
index d569601..75a2450 100644
--- a/net/usunrpc/rfc5666/svc_rdma_internal.h
+++ b/net/usunrpc/rfc5666/svc_rdma_internal.h
@@ -1,4 +1,7 @@
 
+#ifndef _SVC_RDMA_INTERNAL_H_
+#define _SVC_RDMA_INTERNAL_H_
+
 #include <stdio.h>
 #include <unistd.h>
 #include <string.h>
@@ -9,93 +12,264 @@
 #include <errno.h>
 #include <stdlib.h>
 
-#ifdef USE_IN_LIBIO
-# include <wchar.h>
-# include <libio/iolibio.h>
-#endif
+/* internal data structures. */
+
+/**
+  @page svcrdma sunrpc rdma userspace implementation
+
+
+  @section Introduction
+
+  We're going to implement an RDMA server endpoint conforming to rfc5666, in
+  this way, we can talk to clients of linux kernel. This eliminates a lot work
+  since we do not need to design our own protocol and have a proven-to-work
+  infrastructure.
+
+  Similar to svctcp implementation, we're also going to implement a connecting
+  orient with rdma connection management, which provides a series of interfaces
+  such as rdma_create_id, rdma_listen and rdma_accept, etc. These interfaces
+  make it easy to create an RDMA service like socket.
+
+  There're two type of svcxprt in our implementation:
+  @li listener xprt, it represents a listening service like TCP listening
+      socket, which takes the responsibility to monitor new coming connection
+      requests, and new connections will be built after receiving them;
+  @li connection xprt, this is similar to accepted socket in TCP. A connection
+      xprt will be set up after receiving new request from a client. A
+      connection will have enough information to describe the link status,
+      credits and requests and replies pending for this specific client.
+
+  @section logspec Logic Specification
+
+  <b>Connection Management</b>
+
+  Thanks to rdma connection manager library, I can work out a TCP similar
+  connection management scheme.
+
+  When the service is being created by calling svcrdma_create(), a rendevzous
+  export, svcrndzv_xprt, is initialized, A rdma cmid is created and a completion
+  channel fd is used to create SVCXPRT. In this way, the SVCXPRT will be notified
+  when a new connecting request is coming. For convenience, we also put global
+  pararmeters, resource into svcrndzv_xprt because it's system wide only.
+
+  Whenever a client connecting request comes, a new connection export,
+  svcrdma_xprt will be created. After RDMA related things have been initialized,
+  a new SVCXPRT will be created, and export fd will be assigned to the completion
+  channel's fd, in this way, it'll be notified if there is new requests coming
+  to this export. Connection export will be used to store connection related
+  information, such as credits assigned, requests being processed, reply pending,
+  etc.
+
+  <b>Multi-threaded Support</b>
+  At present, svcrdma is implemented based on sunrpc, so that the request handling
+  process is serialized. However, we have to consider multi-threaded support
+  because multi-threaded server is a must to get high performance.
+
+  Fortunately, we don't need to do much job to be multithread safe. Basically:
+  @li request handling cannot rely on SVCXPRT such as sunrpc callbacks do
+  @li rdma buffer operations have to be done under protection
+  @li credits management must be done under protection
+
+  We may need to think about scalability problem in case the # of clients would
+  huge.
+
+  <b>Credits Management</b>
+
+  Credit is a number indicating how many requests from a specific client
+  can be pending on the server. RDMA needs this limitation because it uses
+  preregistered mechanism to receive buffers - and hardware has limitation of
+  how many preregistered buffers is allowed. In case there's no free buffers
+  at the server side, sending request by client will fail and application will
+  be disturbed.
+
+  In RFC5666, whenever a client sends an rdma request to server, it brings a
+  expected credit, and server checks the current available buffers, then
+  returns client a grant credit. Client cannot send requests more than the
+  credit value, otherwise client may be disconnected.
+
+  It's obvious that server has to reserve at least 1 credit for each client.
+  Otherwise, deadlock occurs because client has no way to send request to client
+  and so as to not be able to renew its credit value.
+
+  In our implementation, svc server of course records how many clients are
+  connected and reserves 1 credit for each client. We also track credits for
+  each client in connection export. Thus if there're no any outstanding for a
+  specific client, we'll set the credit to 1 - this is safe and reasonable.
+  Right now, I'm not going to implement a real credit control alogrithm before
+  porting to multi-threaded server.
+ */
+
+/**
+  @defgroup svcrdma SUNRPC userspace RDMA(RFC5666)
 
-#define RDMA_VERS       1
+  Since inifiniband is a popular network architecture to get good performance
+  in HPCS world, also we're building our protocol stack based on sunrpc. So
+  here is a strong intention for sunrpc to use RDMA in user space.
+
+  In linux kernel, there is also a kernel implementation of SUNRPC/RDMA in
+  latest kernel. So our job is just to implement a user space svc sunrpc RDMA
+  which will conform to client implementation of linux kernel.
+
+  However, the problem of current implementation of sunrpc userspace server is
+  that it is a single thread program. Even executing rpc requests from
+  different is also serialized. This is totaly not acceptable for a high
+  performance server. Definitely we're going to revise glibc sunrpc
+  implementation to make it multithreaded. There're two ways:
+  @li ganesha has already had an implementation which solves the problem
+  @li we can implement a multithreaded sunrpc ourselves
+
+  The solution is not determined. Anyway, svcrdma implementation MUST conisder
+  the possibility to be used under multithreaded env, also it will be under
+  minimum possible overhead to port to whatever solution is adopted finally.
+
+  @{
+ */
+
+/** RFC5666 has only version 1 supported. */
+#define RDMA_VERS       (xdr_one)
 
 enum rdma_xprt_status {
-        RDMA_XPRT_INIT   = 0,
+        RDMA_XPRT_INIT   = 0, /**< init state */
         RDMA_XPRT_LISTEN,
         RDMA_XPRT_CONNECTING,
         RDMA_XPRT_ESTABLISHED,
         RDMA_XPRT_DESTROYED
 };
 
-/*
- * svcrdma_export for listener
+/**
+  svcrdma_export for listener
+
+  Listener export represents a service exported by RDMA, it can be used to
+  serve new connection from client. Also, listener export manages global
+  parameters and buffers. 
  */
 struct svcrndzv_xprt {
         SVCXPRT                    svcxprt;
+        enum rdma_xprt_status      status;
+
+        /**
+          rdma connection id the listener is listening on. cmid is similar
+          to a TCP socket in TCP/IP. rdma_listen and rdma_accept will take
+          it as an argument.
+         */
         struct rdma_cm_id         *cmid;
         struct rdma_event_channel *channel;
-        enum rdma_xprt_status      status;
 
-        /* exports have been accepted. */
+        /** exports have been accepted. */
         struct c2_list             xprts;
 
-        /* rdma parameters are stored here. */
-        int trans;
-        int port;
-        int max_send_wr;
-        int max_recv_wr;
-        int max_send_sge;
-        int max_recv_sge;
+        /**
+          Preallocated buffers used by this xprt.
+         */
+        struct rdma_request      *rcv_bufs;
+        struct rdma_reply        *snd_bufs;
+        /**
+          Credits management.
+          At initialisation time, the total avail credits is determined
+          by hardware. Then, for connecting clients, we have to reserve
+          1 credit for each.
+         */
+        int                       credits; /**< total avail credits */
+        int                       clients; /**< # of living clients */
+
+        /**
+           rdma parameters which are assigned at initialization time.
+           @see svcrdma_create
+
+           BTW, server has to negotiate some of these parameters with client.
+           The detailed process of negotiation is out of scope.
+        */
+        int trans;              /**< transport type of rdma, RC, UC, etc */
+        int port;               /**< port of ipoib, infiniband needs it to
+                                     be addressed */
+        int max_send_wr;        /**< how many paralleling sending(reply)
+                                     requests */
+        int max_recv_wr;        /**< how many requests can be received
+                                     simultaneously. This value represents the
+                                     capability of hardware, and should be
+                                     distributed to each clients by credits. */
+        int max_send_sge;       /**< maximum num of sges in a sending rqst */
+        int max_recv_sge;       /**< maximum num of sges for a recv rqst */
         int rc_retry_cnt;
         int rc_rnr_retry_cnt;
-        int max_msg_size;
+        int max_msg_size;       /**< maximum size of each request & reply */
         int page_size;
         int rcv_wr_flags;
         int snd_wr_flags;
 };
 
-/*
- *
+/**
+   A connection to a client.
  */
 struct svcrdma_xprt {
-        struct c2_list_link   link;
+        struct c2_list_link   link;     /**< link list to listener */
         SVCXPRT               svcxprt;
         struct svcrndzv_xprt *rndzv;
         struct rdma_cm_id    *cmid;
 
-        /* queue pair information */
+        /**
+           queue pair information.
+         */
+
+        /**
+          each connection has an independent pair queue and completion
+          channel which will be notified if there is new events coming.
+         */
         struct ibv_comp_channel *channel;
         struct ibv_pd           *pd;
         struct ibv_cq           *cq;
         struct ibv_qp           *qp;
 
-        /* the rdma request is processing */
+        /**
+          the following information records the requests and replies this
+          export is handling. */
+        /**
+          the rdma request is being processed.
+          XXX: this is a workaround for single-thread sunrpc since we have
+          no way to get the current request from SVCXPRT
+         */
         struct rdma_request     *curreq;
+        /**
+          requests which are being processed, prepared for future use
+         */
         struct c2_list          *requests;
-        struct c2_list          *replis;
+        /**
+          Pending replies for this export.
+          We need to track replies because of async attribute of RDMA
+         */
+        struct c2_list          *replies;
 
-        /* preallocated buffers used by this xprt. */
-        struct rdma_request  *rcv_bufs;
-        struct rdma_reply    *snd_bufs;
+        /**
+          credits management
+         */
+        int                      credits;   /**< credits occupied */
+        int                      cred_used; /**< credits being used */
 
-        /* attribute of this xprt */
+        /** attribute of this xprt */
         enum rdma_xprt_status    status;
 };
 
+/**
+  rdma_reply describes a reply message of request.
+ */
 struct rdma_reply {
-        struct rdma_reply *next;
+        struct rdma_reply    *next; /**< global linked list */
         struct svcrdma_xprt  *xprt;
 
+        struct c2_list_link  *link; /**< replies belonging to the same xprt. */
+
         /**
-         * Active replies belong to this export
+          Information to encode the reply message
          */
-        struct c2_list_link  *link;
-        struct svcrdma_msg    msg;
-        XDRS                  xdrs;
-
-        struct rdma_write_list  *writes;
-        struct rdma_write_chunk *reply;
-        int                writes_index;
-
-        int                curpos; /* current position of XDR stream */
+        struct svcrdma_msg       msg;    /**< reply encode message */
+        struct rdma_write_list  *writes; /**< write chunks, copied from rqst */
+        struct rdma_write_chunk *reply;  /**< reply chunks */
+        int                      writes_index;
+        XDRS                     xdrs;   /**< used for encoding message */
 
+        /**
+          buffers for reply message. They are preallocated buffers.
+         */
         void              *buffer;
         void              *rdma_buf;
         void              *rpc_buf;
@@ -106,31 +280,68 @@ struct rdma_reply {
         struct ibv_send_wr wr;
 };
 
+/**
+  rdma_request: describe a request from client side.
+ */
 struct rdma_request {
         struct svcrdma_xprt  *xprt;
         struct rdma_reply    *reply;
 
         /**
-         * Active repquests belong to this export.
-         * Sunrpc works sync right now, but we maintain requests assuming that
-         * multiple requests are handled simultaneously.
+          Active repquests belong to this export.
+          Sunrpc works sync right now, but we maintain requests assuming that
+          multiple requests are handled simultaneously.
          */
         struct c2_list_link  *link;
+
+        /**
+          RDMA decoding information
+         */
         struct svcrdma_msg    msg;
+        /**
+          rpcbuf_base points to the start address of rpc message.
+          It may point to the buffer in svcrdma_msg, or an external buffer, if
+          the rpcmsg was transferred via read chunk.
+         */
         void                 *rpcbuf_base;
         int                   rpcbuf_len;
-        int                   curpos;  /* current pos in XDR stream */
+
+        /**
+          Read chunks start offset and total chunks' length in the XDR stream
+         */
         int                   chunk_start;
         int                   chunk_len;
+
         XDR                   xdrs;
 
+        /**
+          Helper information. After the XDR message has been decoded, these
+          pointers will be assigned to the lists in svcrdma_msg.
+         */
         struct rdma_read_list   *reads;
         struct rdma_write_list  *writes;
-        struct rdma_write_chunk *reply;
 
+        /**
+          For each request, the rpcbuf is preregistered and the length of the
+          buffer has already been negootiated.
+         */
         void                 *buffer;
         struct ibv_mr        *mr;
         struct ibv_recv_wr    wr;
         struct ibv_sge        sge;
 };
 
+/** @} end group svcrdma */
+
+#endif /* ifndef _SVC_RDMA_INTERNAL_H_ */
+
+/*
+ *  Local variables:
+ *  c-indentation-style: "K&R"
+ *  c-basic-offset: 8
+ *  tab-width: 8
+ *  fill-column: 80
+ *  scroll-step: 1
+ *  End:
+ */
+
diff --git a/net/usunrpc/rfc5666/xdr_rdma.c b/net/usunrpc/rfc5666/xdr_rdma.c
index e0fcbb2..2216c5c 100644
--- a/net/usunrpc/rfc5666/xdr_rdma.c
+++ b/net/usunrpc/rfc5666/xdr_rdma.c
@@ -1,6 +1,7 @@
 /*
  */
-bool_t xdr_dbuf(XDR *xdrs, struct c2_dbuf *dbuf)
+bool_t xdr_dbuf(XDR *xdrs, struct c2_dbuf *dbuf,
+                bool_t (*xdr)(XDRS *, char *buffer, int buflen))
 {
         if (!xdr_u_int(xdrs, &dbuf->alignment))
                 return FALSE;
@@ -13,8 +14,8 @@ bool_t xdr_dbuf(XDR *xdrs, struct c2_dbuf *dbuf)
                                    dbuf->buflen))
                         return FALSE;
         case XDR_ENCODE:
-                if (dbuf->xdr)
-                        return dbuf->xdr(xdrs, dbuf->buffer, dbuf->buflen);
+                if (xdr)
+                        return xdr(xdrs, dbuf->buffer, dbuf->buflen);
                 return xdr_opaque(xdrs, dbuf->buffer, dbuf->buflen);
         case XDR_FREE:
                 mem_free(dbuf->buffer);
-- 
1.8.3.2

