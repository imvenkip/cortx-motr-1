From c7c5850a9e5dbded0794eb455411c6c6cc2bc671 Mon Sep 17 00:00:00 2001
From: "jinshan.xiong" <jinshan.xiong@clusterstor.com>
Date: Wed, 14 Jul 2010 15:16:53 -0600
Subject: [PATCH 05/34] - revised DLD

---
 net/usunrpc/rfc5666/rfc5666.h           |  5 ++
 net/usunrpc/rfc5666/rfc5666_xdr.c       |  5 --
 net/usunrpc/rfc5666/svc_rdma_internal.h | 98 +++++++++++++++------------------
 3 files changed, 48 insertions(+), 60 deletions(-)

diff --git a/net/usunrpc/rfc5666/rfc5666.h b/net/usunrpc/rfc5666/rfc5666.h
index b1de59a..fcd4960 100644
--- a/net/usunrpc/rfc5666/rfc5666.h
+++ b/net/usunrpc/rfc5666/rfc5666.h
@@ -82,6 +82,11 @@ struct rdma_body {
         } u;
 };
 
+/**
+  rdma message defined by rfc5666 - genreated by rpcgen.
+
+  TODO: we need to revise XDR function, there're two much memory allocation.
+ */
 struct svcrdma_msg {
         uint32_t         rdma_xid;
         uint32_t         rdma_vers;
diff --git a/net/usunrpc/rfc5666/rfc5666_xdr.c b/net/usunrpc/rfc5666/rfc5666_xdr.c
index a1a50ca..7066ab73 100644
--- a/net/usunrpc/rfc5666/rfc5666_xdr.c
+++ b/net/usunrpc/rfc5666/rfc5666_xdr.c
@@ -1,8 +1,3 @@
-/*
- * Please do not edit this file.
- * It was generated using rpcgen.
- */
-
 #include "rfc5666.h"
 
 static bool_t xdr_rdma_segment (XDR *xdrs, struct rdma_segment *objp)
diff --git a/net/usunrpc/rfc5666/svc_rdma_internal.h b/net/usunrpc/rfc5666/svc_rdma_internal.h
index 75a2450..93e8a7e 100644
--- a/net/usunrpc/rfc5666/svc_rdma_internal.h
+++ b/net/usunrpc/rfc5666/svc_rdma_internal.h
@@ -17,75 +17,74 @@
 /**
   @page svcrdma sunrpc rdma userspace implementation
 
-
   @section Introduction
 
-  We're going to implement an RDMA server endpoint conforming to rfc5666, in
-  this way, we can talk to clients of linux kernel. This eliminates a lot work
-  since we do not need to design our own protocol and have a proven-to-work
-  infrastructure.
+  In order to talk to linux sunrpc client over infiniband, We're going to
+  implement an sunrpc/rdma server endpoint in userspace. This eliminates a
+  lot work since we do not need to design our own protocol and have a
+  proven-to-work infrastructure.
+
+  @section logspec Logic Specification
+
+  <b>Connection Management</b>
 
-  Similar to svctcp implementation, we're also going to implement a connecting
-  orient with rdma connection management, which provides a series of interfaces
-  such as rdma_create_id, rdma_listen and rdma_accept, etc. These interfaces
-  make it easy to create an RDMA service like socket.
+  Similar to svctcp implementation, we're also going to implement a connection
+  orient endpoint by rdma cm library, which provides a series of interfaces
+  such as rdma_create_id, rdma_listen and rdma_accept, etc, which make easier
+  to create an RDMA service like TCP/IP socket.
 
   There're two type of svcxprt in our implementation:
   @li listener xprt, it represents a listening service like TCP listening
       socket, which takes the responsibility to monitor new coming connection
-      requests, and new connections will be built after receiving them;
+      requests, and new connections will be set up after receiving them;
   @li connection xprt, this is similar to accepted socket in TCP. A connection
-      xprt will be set up after receiving new request from a client. A
+      xprt will be set up after receiving connecting request from a client. A
       connection will have enough information to describe the link status,
       credits and requests and replies pending for this specific client.
 
-  @section logspec Logic Specification
-
-  <b>Connection Management</b>
-
-  Thanks to rdma connection manager library, I can work out a TCP similar
-  connection management scheme.
-
-  When the service is being created by calling svcrdma_create(), a rendevzous
-  export, svcrndzv_xprt, is initialized, A rdma cmid is created and a completion
+  When a service is being created by calling svcrdma_create(), a rendevzous
+  export, svcrndzv_xprt, is initialized, an rdma cmid is created and a completion
   channel fd is used to create SVCXPRT. In this way, the SVCXPRT will be notified
-  when a new connecting request is coming. For convenience, we also put global
-  pararmeters, resource into svcrndzv_xprt because it's system wide only.
+  when a new connecting request comes. For convenience, we also put global
+  pararmeters, global resources into svcrndzv_xprt because it's system wide.
 
   Whenever a client connecting request comes, a new connection export,
   svcrdma_xprt will be created. After RDMA related things have been initialized,
-  a new SVCXPRT will be created, and export fd will be assigned to the completion
-  channel's fd, in this way, it'll be notified if there is new requests coming
-  to this export. Connection export will be used to store connection related
-  information, such as credits assigned, requests being processed, reply pending,
-  etc.
+  a new SVCXPRT will be created, and export fd will be assigned to the fd of
+  queue pair's completion channel, so that it'll be notified if there is new
+  requests coming to this export. Connection exports are a container of
+  connection related information, such as credits assigned, requests being
+  processed, reply being sent, etc.
 
   <b>Multi-threaded Support</b>
-  At present, svcrdma is implemented based on sunrpc, so that the request handling
-  process is serialized. However, we have to consider multi-threaded support
-  because multi-threaded server is a must to get high performance.
 
-  Fortunately, we don't need to do much job to be multithread safe. Basically:
-  @li request handling cannot rely on SVCXPRT such as sunrpc callbacks do
-  @li rdma buffer operations have to be done under protection
+  At present, svcrdma is implemented based on sunrpc, so that the request
+  handling process is globally serialized. However, we have to do it with 
+  multi-threaded support because it is a must to get high performance.
+
+  Fortunately, we don't need to do much job to become multithread safe.
+  Basically:
+  @li request handling cannot rely on SVCXPRT like sunrpc callbacks do
+  @li allocating and free rdma buffer have to be done under protection
   @li credits management must be done under protection
 
   We may need to think about scalability problem in case the # of clients would
-  huge.
+  be huge.
 
   <b>Credits Management</b>
 
-  Credit is a number indicating how many requests from a specific client
+  Credit is an integer indicating how many requests from a specific client
   can be pending on the server. RDMA needs this limitation because it uses
-  preregistered mechanism to receive buffers - and hardware has limitation of
+  preregistered mechanism to receive requests - and hardware has limitation of
   how many preregistered buffers is allowed. In case there's no free buffers
   at the server side, sending request by client will fail and application will
   be disturbed.
 
   In RFC5666, whenever a client sends an rdma request to server, it brings a
-  expected credit, and server checks the current available buffers, then
-  returns client a grant credit. Client cannot send requests more than the
-  credit value, otherwise client may be disconnected.
+  expected credit, and server checks the current available credit, then
+  grants client a new credit. Client cannot send requests more than the
+  credit value, otherwise the request will be discarded and connection may be
+  disconnected.
 
   It's obvious that server has to reserve at least 1 credit for each client.
   Otherwise, deadlock occurs because client has no way to send request to client
@@ -95,32 +94,21 @@
   connected and reserves 1 credit for each client. We also track credits for
   each client in connection export. Thus if there're no any outstanding for a
   specific client, we'll set the credit to 1 - this is safe and reasonable.
-  Right now, I'm not going to implement a real credit control alogrithm before
-  porting to multi-threaded server.
  */
 
 /**
-  @defgroup svcrdma SUNRPC userspace RDMA(RFC5666)
+  @defgroup svcrdma sunrpc rdma userspace implementation
 
   Since inifiniband is a popular network architecture to get good performance
-  in HPCS world, also we're building our protocol stack based on sunrpc. So
-  here is a strong intention for sunrpc to use RDMA in user space.
+  in HPCS world, also we're building our protocol stack based on sunrpc, there
+  is a strong intention for sunrpc to use RDMA in user space.
 
   In linux kernel, there is also a kernel implementation of SUNRPC/RDMA in
   latest kernel. So our job is just to implement a user space svc sunrpc RDMA
   which will conform to client implementation of linux kernel.
 
-  However, the problem of current implementation of sunrpc userspace server is
-  that it is a single thread program. Even executing rpc requests from
-  different is also serialized. This is totaly not acceptable for a high
-  performance server. Definitely we're going to revise glibc sunrpc
-  implementation to make it multithreaded. There're two ways:
-  @li ganesha has already had an implementation which solves the problem
-  @li we can implement a multithreaded sunrpc ourselves
-
-  The solution is not determined. Anyway, svcrdma implementation MUST conisder
-  the possibility to be used under multithreaded env, also it will be under
-  minimum possible overhead to port to whatever solution is adopted finally.
+  Though sunrpc is not multi-threaded server, we still need to write the code
+  carefully to make it multi-thread safe or easy to port a multi-thread server.
 
   @{
  */
-- 
1.8.3.2

