From 9a4714b29eab63df47e8c766cb4ea31f6ec0f354 Mon Sep 17 00:00:00 2001
From: Sachin Patil <sachin_patil@xyratex.com>
Date: Tue, 19 Mar 2013 01:36:19 -0700
Subject: [PATCH 66/94] Added RVM under core directory, it builds along with
 mero source tree.

---
 Makefile.am               |    5 +-
 be/be_impl.h              |    2 +-
 be/be_rvm.c               |    6 +-
 be/reg.c                  |    2 +-
 be/seg.c                  |    8 +-
 configure.ac              |   47 +-
 rvm/Makefile.sub          |   32 +
 rvm/coda_mmap_anon.h      |   37 +
 rvm/cthreads.h            |  116 ++
 rvm/rds.h                 |  157 +++
 rvm/rds_coalesce.c        |  207 ++++
 rvm/rds_free.c            |  268 +++++
 rvm/rds_init.c            |  119 ++
 rvm/rds_malloc.c          |  136 +++
 rvm/rds_maxblock.c        |   45 +
 rvm/rds_prealloc.c        |  135 +++
 rvm/rds_private.h         |  192 +++
 rvm/rds_split.c           |  170 +++
 rvm/rds_start.c           |  137 +++
 rvm/rds_stats.c           |  223 ++++
 rvm/rds_util.c            |  266 +++++
 rvm/rds_zap.c             |  113 ++
 rvm/rvm.h                 |  476 ++++++++
 rvm/rvm_createseg.c       |  138 +++
 rvm/rvm_debug.c           | 1242 +++++++++++++++++++
 rvm/rvm_init.c            |  144 +++
 rvm/rvm_io.c              |  552 +++++++++
 rvm/rvm_loadseg.c         |  139 +++
 rvm/rvm_logflush.c        |  668 +++++++++++
 rvm/rvm_logrecovr.c       | 2901 +++++++++++++++++++++++++++++++++++++++++++++
 rvm/rvm_logstatus.c       | 1139 ++++++++++++++++++
 rvm/rvm_lwp.h             |  122 ++
 rvm/rvm_map.c             | 1091 +++++++++++++++++
 rvm/rvm_printers.c        |  771 ++++++++++++
 rvm/rvm_private.h         | 1716 +++++++++++++++++++++++++++
 rvm/rvm_pthread.c         |   32 +
 rvm/rvm_pthread.h         |  157 +++
 rvm/rvm_releaseseg.c      |   51 +
 rvm/rvm_segment.h         |   74 ++
 rvm/rvm_segment_private.h |   58 +
 rvm/rvm_segutil.c         |  170 +++
 rvm/rvm_statistics.h      |  201 ++++
 rvm/rvm_status.c          |  403 +++++++
 rvm/rvm_trans.c           | 1010 ++++++++++++++++
 rvm/rvm_unmap.c           |  100 ++
 rvm/rvm_utils.c           | 2325 ++++++++++++++++++++++++++++++++++++
 46 files changed, 18045 insertions(+), 58 deletions(-)
 create mode 100644 rvm/Makefile.sub
 create mode 100644 rvm/coda_mmap_anon.h
 create mode 100644 rvm/cthreads.h
 create mode 100644 rvm/rds.h
 create mode 100644 rvm/rds_coalesce.c
 create mode 100644 rvm/rds_free.c
 create mode 100644 rvm/rds_init.c
 create mode 100644 rvm/rds_malloc.c
 create mode 100644 rvm/rds_maxblock.c
 create mode 100644 rvm/rds_prealloc.c
 create mode 100644 rvm/rds_private.h
 create mode 100644 rvm/rds_split.c
 create mode 100644 rvm/rds_start.c
 create mode 100644 rvm/rds_stats.c
 create mode 100644 rvm/rds_util.c
 create mode 100644 rvm/rds_zap.c
 create mode 100644 rvm/rvm.h
 create mode 100644 rvm/rvm_createseg.c
 create mode 100644 rvm/rvm_debug.c
 create mode 100644 rvm/rvm_init.c
 create mode 100644 rvm/rvm_io.c
 create mode 100644 rvm/rvm_loadseg.c
 create mode 100644 rvm/rvm_logflush.c
 create mode 100644 rvm/rvm_logrecovr.c
 create mode 100644 rvm/rvm_logstatus.c
 create mode 100644 rvm/rvm_lwp.h
 create mode 100644 rvm/rvm_map.c
 create mode 100644 rvm/rvm_printers.c
 create mode 100644 rvm/rvm_private.h
 create mode 100644 rvm/rvm_pthread.c
 create mode 100644 rvm/rvm_pthread.h
 create mode 100644 rvm/rvm_releaseseg.c
 create mode 100644 rvm/rvm_segment.h
 create mode 100644 rvm/rvm_segment_private.h
 create mode 100644 rvm/rvm_segutil.c
 create mode 100644 rvm/rvm_statistics.h
 create mode 100644 rvm/rvm_status.c
 create mode 100644 rvm/rvm_trans.c
 create mode 100644 rvm/rvm_unmap.c
 create mode 100644 rvm/rvm_utils.c

diff --git a/Makefile.am b/Makefile.am
index d594458..a93f5b7 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -132,9 +132,7 @@ include $(top_srcdir)/xcode/ff2c/Makefile.sub
 lib_LTLIBRARIES                += mero/libmero.la
 mero_libmero_la_LDFLAGS   = -release @LT_RELEASE@ -pthread $(AM_LDFLAGS)
 mero_libmero_la_LIBADD    = @MATH_LIBS@ @PTHREAD_LIBS@ @AIO_LIBS@ @RT_LIBS@ 	\
-                            @DB_LIBS@ @GALOIS_LIBS@ @YAML_LIBS@ @CUNIT_LIBS@ 	\
-			    @RVM_LIBS@ @RDS_LIBS@ @SEG_LIBS@
-				
+                            @DB_LIBS@ @GALOIS_LIBS@ @YAML_LIBS@ @CUNIT_LIBS@
 
 # TODO: remove CUnit dependency from libmero, move lib/ut.c to ut/ and
 # libmero-ut
@@ -178,6 +176,7 @@ include $(top_srcdir)/net/lnet/Makefile.sub
 include $(top_srcdir)/pool/Makefile.sub
 include $(top_srcdir)/reqh/Makefile.sub
 include $(top_srcdir)/rpc/Makefile.sub
+include $(top_srcdir)/rvm/Makefile.sub
 include $(top_srcdir)/sm/Makefile.sub
 include $(top_srcdir)/sns/Makefile.sub
 include $(top_srcdir)/sns/repair/Makefile.sub
diff --git a/be/be_impl.h b/be/be_impl.h
index 50bffbb..9ab952d 100644
--- a/be/be_impl.h
+++ b/be/be_impl.h
@@ -28,7 +28,7 @@
 */
 #include "sm/sm.h"
 #include "stob/stob.h"
-#include "be/rvm.h"
+#include "rvm/rvm.h"
 
 #define MAXNAMELEN      1024
 
diff --git a/be/be_rvm.c b/be/be_rvm.c
index 6448e76..5080c7a 100644
--- a/be/be_rvm.c
+++ b/be/be_rvm.c
@@ -18,7 +18,7 @@
  */
 
 #include "be/be.h"
-
+#include "rvm/rvm.h"
 
 /**
   @addtogroup be_rvm
@@ -26,7 +26,7 @@
   */
 const char *log_file="/tmp/log_file";
 
-extern void *RecoverableHeapStartAddress;
+extern heap_header_t *RecoverableHeapStartAddress;
 
 /**
  * Initialize RVM
@@ -56,7 +56,7 @@ void m0_be_rvm_init()
 
 void *m0_be_rvm_get_heapaddr()
 {
-        return RecoverableHeapStartAddress;
+        return (void *)RecoverableHeapStartAddress;
 }
 /** @} end group be_rvm */
 
diff --git a/be/reg.c b/be/reg.c
index f7db827..f08c98a 100644
--- a/be/reg.c
+++ b/be/reg.c
@@ -28,7 +28,7 @@
 #include "be/reg.h"
 #include "be/tx.h"
 
-#include "be/rds.h"
+#include "rvm/rds.h"
 
 /**
         Back-end
diff --git a/be/seg.c b/be/seg.c
index a12701c..a323dd9 100644
--- a/be/seg.c
+++ b/be/seg.c
@@ -28,7 +28,7 @@
 #include "be/seg.h"
 #include "be/domain.h"
 #include "be/be_rvm.h"
-#include "be/rds.h"
+#include "rvm/rds.h"
 
 #include "stob/linux.h"
 #include "stob/linux_internal.h"
@@ -272,7 +272,7 @@ M0_INTERNAL void m0_be_seg_fail(struct m0_be_seg *seg)
         M0_LEAVE();
 }
 
-static int be_rvm_rds_zap_heap(char *file_path)
+static int be_rvm_rds_zap_heap(struct m0_be_seg *seg)
 {
         char                     *start_Addr        = (char *)0xbebd000;
         long                      rds_static_len    = 0;
@@ -287,7 +287,7 @@ static int be_rvm_rds_zap_heap(char *file_path)
         /* Create heap structure. */
         rds_static_len    = RVM_ROUND_LENGTH_DOWN_TO_PAGE_SIZE(4096);
 
-        result = rds_zap_heap(file_path,
+        result = rds_zap_heap(seg->bs_impl.path_name,
                               RVM_LENGTH_TO_OFFSET(dev_length),
                               start_Addr,
                               rds_static_len,
@@ -367,7 +367,7 @@ M0_INTERNAL void m0_be_seg_create_cb(void *cb_data)
                 goto exit;
         }
 
-        result = be_rvm_rds_zap_heap(seg->bs_impl.path_name);
+        result = be_rvm_rds_zap_heap(seg);
         if (result < 0) {
                 M0_LOG(M0_ERROR, "rds_zap_heap failed in m0_be_seg_create_cb");
                 seg_state = M0_BESEG_FAILED;
diff --git a/configure.ac b/configure.ac
index 168804b..87b335a 100644
--- a/configure.ac
+++ b/configure.ac
@@ -362,28 +362,6 @@ MERO_SEARCH_LIBS([yaml_parser_initialize], [yaml], [YAML_LIBS],
 $SRCDIR/../yaml or from rpm]
 )
 
-#Check for RVM
-
-MERO_SEARCH_LIBS([rvm_initialize], [rvmpt], [RVM_LIBS],
-        [rvm_initialize cannot be found! Try install rvm package from \
-$SRCDIR/../rvm or from rpm]
-)
-
-MERO_SEARCH_LIBS([rds_init_heap], [rdspt], [RDS_LIBS],
-        [rds_init_heap cannot be found! Try install rvm package from \
-$SRCDIR/../rvm or from rpm]
-)
-
-MERO_SEARCH_LIBS([rvm_create_segment], [segpt], [SEG_LIBS],
-        [rvm_create_segment cannot be found! Try install rvm package from \
-$SRCDIR/../rvm or from rpm]
-)
-
-AC_CHECK_HEADERS([rvm/rvm/rvm.h], [],
-                 AC_MSG_ERROR([rvm/rvm/rvm.h is not found. Try install rvm \
-from $SRCDIR/../rvm or rvm-devel rpm package])
-)
-
 # Check for CUnit
 
 MERO_SEARCH_LIBS([CU_add_suite], [cunit], [CUNIT_LIBS],
@@ -408,21 +386,6 @@ AC_CHECK_HEADERS([db.h], [],
 
 else # non-rpm setup
 
-#Check for rvm libs
-RVM_SRC=$ABS_SRCDIR/../rvm
-RVM_LIBS=$ABS_SRCDIR/../rvm/rvm/librvmpt.la
-RDS_LIBS=$ABS_SRCDIR/../rvm/rds/librdspt.la
-SEG_LIBS=$ABS_SRCDIR/../rvm/seg/libsegpt.la
-RVM_INCLUDES="-I$RVM_SRC/include"
-
-AC_MSG_CHECKING([for rvm libs])
-MERO_BUILD_LIB([rvmpt], [$RVM_SRC], [$RVM_LIBS], [.],
-               ["--enable-librvmpt --disable-librvm --disable-librvmlwp"])
-AC_MSG_CHECKING([for rds libs])
-MERO_BUILD_LIB([rdspt], [$RVM_SRC], [$RDS_LIBS])
-AC_MSG_CHECKING([for seg libs])
-MERO_BUILD_LIB([segpt], [$RVM_SRC], [$SEG_LIBS])
-
 # Check for galois
 
 GALOIS_SRC=$ABS_SRCDIR/../galois
@@ -477,11 +440,6 @@ AC_SUBST([DB_SRC])
 AC_SUBST([DB_INCLUDES])
 AC_SUBST([DB_LIBS])
 
-AC_SUBST([RVM_LIBS])
-AC_SUBST([RDS_LIBS])
-AC_SUBST([SEG_LIBS])
-AC_SUBST([RVM_INCLUDES])
-
 #
 # Handle configuration options
 #
@@ -516,7 +474,7 @@ AS_IF([test x$enable_mcheck = xyes],
 
 AS_IF([test x$enable_rpm != xyes],
       [M0_CPPFLAGS="$GALOIS_INCLUDES $YAML_INCLUDES $CUNIT_INCLUDES \
-                    $DB_INCLUDES $RVM_INCLUDES $M0_CPPFLAGS"]
+                    $DB_INCLUDES $M0_CPPFLAGS"]
 )
 
 # _GNU_SOURCE for asprintf(3), open(2) O_DIRECT flag
@@ -613,9 +571,6 @@ echo ""
 echo "Linux Obj    :  \"$LINUX_OBJ\""
 echo "Lustre       :  \"$LUSTRE\""
 echo ""
-echo "RVM_LIBS     :  \"$RVM_LIBS\""
-echo "RDS_LIBS     :  \"$RDS_LIBS\""
-echo "SEG_LIBS     :  \"$SEG_LIBS\""
 echo ""
 
 echo 'Run `make` to build Mero or `make help` for other options'
diff --git a/rvm/Makefile.sub b/rvm/Makefile.sub
new file mode 100644
index 0000000..24f65d6
--- /dev/null
+++ b/rvm/Makefile.sub
@@ -0,0 +1,32 @@
+nobase_mero_include_HEADERS +=  rvm/rvm_private.h               \
+                                rvm/rds_private.h               \
+                                rvm/rvm_segment_private.h
+
+mero_libmero_la_SOURCES     +=  rvm/rvm_debug.c                 \
+                                rvm/rvm_init.c                  \
+                                rvm/rvm_io.c                    \
+                                rvm/rvm_logflush.c              \
+                                rvm/rvm_logrecovr.c             \
+                                rvm/rvm_logstatus.c             \
+                                rvm/rvm_map.c                   \
+                                rvm/rvm_printers.c              \
+                                rvm/rvm_pthread.c               \
+                                rvm/rvm_status.c                \
+                                rvm/rvm_trans.c                 \
+                                rvm/rvm_unmap.c                 \
+                                rvm/rvm_utils.c                 \
+                                rvm/rds_coalesce.c              \
+                                rvm/rds_free.c                  \
+                                rvm/rds_init.c                  \
+                                rvm/rds_malloc.c                \
+                                rvm/rds_maxblock.c              \
+                                rvm/rds_prealloc.c              \
+                                rvm/rds_split.c                 \
+                                rvm/rds_start.c                 \
+                                rvm/rds_stats.c                 \
+                                rvm/rds_util.c                  \
+                                rvm/rds_zap.c                   \
+                                rvm/rvm_createseg.c             \
+                                rvm/rvm_loadseg.c               \
+                                rvm/rvm_releaseseg.c            \
+                                rvm/rvm_segutil.c
diff --git a/rvm/coda_mmap_anon.h b/rvm/coda_mmap_anon.h
new file mode 100644
index 0000000..5f0d128
--- /dev/null
+++ b/rvm/coda_mmap_anon.h
@@ -0,0 +1,37 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+
+#*/
+#pragma once
+
+#ifndef __MERO_RVM_CODA_MMAP_ANON_H__
+#define __MERO_RVM_CODA_MMAP_ANON_H__
+
+#include <unistd.h>
+#include <sys/mman.h>
+
+#ifndef MAP_ANON
+#define MAP_ANON 0
+#endif
+
+#define mmap_anon(raddrptr, addrptr, len, prot) do { \
+    int fd = -1, flags = MAP_ANON | MAP_PRIVATE; \
+    if (addrptr) flags |= MAP_FIXED; \
+    if (!MAP_ANON) fd = open("/dev/zero", O_RDWR); \
+    raddrptr = mmap((char *)addrptr, len, prot, flags, fd, 0); \
+    if (fd != -1) close(fd); \
+} while(0);
+
+#endif /* __MERO_RVM_CODA_MMAP_ANON_H__ */
diff --git a/rvm/cthreads.h b/rvm/cthreads.h
new file mode 100644
index 0000000..b8cc609
--- /dev/null
+++ b/rvm/cthreads.h
@@ -0,0 +1,116 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/* dummy version of
+ *
+ * Definitions for the C Threads package.
+ * (originally contributed by James W. O'Toole Jr., MIT)
+ */
+#pragma once
+
+#ifndef	__MERO_RVM_CTHREADS_H__
+#define	__MERO_RVM_CTHREADS_H__
+
+/*
+ * C Threads package initialization.
+ */
+
+#define cthread_init()          {}      /* nop */
+
+/*
+ * Mutex objects.
+ */
+#ifdef sun
+#define mutex ct_mutex
+#endif
+typedef struct mutex { int x; } *mutex_t;
+
+#define	MUTEX_INITIALIZER	{0}
+
+#define	mutex_init(m)		((m)->x = 0)
+#define	mutex_clear(m)		/* nop */
+
+#define	mutex_lock(m)		((m)->x = 1)
+#define mutex_try_lock(m)	((m)->x ? 0 : ((m)->x = 1))
+#define mutex_wait_lock(m)	((m)->x = 1)
+#define mutex_unlock(m)		((m)->x = 0)
+
+/*
+ * Condition variables.
+ */
+typedef struct condition { int x; } *condition_t;
+
+#define	CONDITION_INITIALIZER		{0}
+
+#define	condition_init(c)		{}
+#define	condition_clear(c)		{}
+
+#define	condition_signal(c) 		{}
+
+#define	condition_broadcast(c)		{}
+
+#define condition_wait(c,m)		{}
+
+/*
+ * Threads.
+ */
+typedef int cthread;
+
+/* What should be type of cthread_t ?
+ * In rvm_lwp.h, it is type (PROCESS)   (eq. to (struct lwp_pcb *)),
+ * In rvm_pthread.h, it is type (pthread_t *),
+ * Here, I leave it untouch as type (int) but we may need to modify this
+ * in future.  -- 3/18/97 Clement
+ */
+typedef long cthread_t;
+
+#define cthread_fork(func, arg)		(cthread_t)NULL
+
+#define cthread_join(t)			0
+
+#define cthread_yield()			{}
+
+#define cthread_exit(result)		exit(result)
+
+#define cthread_self()	    	    	(cthread_t)NULL
+/* Unsupported cthread calls */
+
+#define	mutex_alloc()			BOGUSCODE
+#define	mutex_set_name(m, x)		BOGUSCODE
+#define	mutex_name(m)			BOGUSCODE
+#define	mutex_free(m)			BOGUSCODE
+
+#define	condition_alloc()		BOGUSCODE
+#define	condition_set_name(c, x)	BOGUSCODE
+#define	condition_name(c)		BOGUSCODE
+#define	condition_free(c)		BOGUSCODE
+
+#define cthread_detach()		BOGUSCODE
+#define cthread_sp()			BOGUSCODE
+#define	cthread_assoc(id, t)		BOGUSCODE
+#define cthread_set_name		BOGUSCODE
+#define cthread_name			BOGUSCODE
+#define cthread_count()			BOGUSCODE
+#define cthread_set_limit		BOGUSCODE
+#define cthread_limit()			BOGUSCODE
+#define	cthread_set_data(t, x)		BOGUSCODE
+#define	cthread_data(t)			BOGUSCODE
+
+
+#endif /* __MERO_RVM_CTHREADS_H__ */
+
diff --git a/rvm/rds.h b/rvm/rds.h
new file mode 100644
index 0000000..2f4c3b0
--- /dev/null
+++ b/rvm/rds.h
@@ -0,0 +1,157 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+ * Public definitions for the Recoverable Dynamic Storage package.
+ */
+#pragma once
+
+#ifndef	__MERO_RVM_RDS_H__
+#define	__MERO_RVM_RDS_H__
+
+#include <stdio.h>
+#include "rvm.h"
+
+/* Error codes */
+
+#define SUCCESS             0
+#define ERVM_FAILED         -1
+#define EBAD_LIST           -2
+#define EBAD_SEGMENT_HDR    -3
+#define EHEAP_VERSION_SKEW  -4
+#define EHEAP_INIT          -5
+#define EBAD_ARGS           -6
+#define ECORRUPT            -7
+#define EFREED_TWICE        -8
+#define ENO_ROOM            -9
+
+/* Function definitions */
+
+extern int rds_zap_heap(
+      char 	            *DevName,
+      rvm_offset_t          DevLength,
+      char                  *startAddr,
+      rvm_length_t          staticLength,
+      rvm_length_t          heapLength,
+      unsigned long         nlists,
+      unsigned long         chunkSize,
+      int                   *err
+     );
+
+extern int rds_init_heap(
+      char                  *base,
+      rvm_length_t          length,
+      unsigned long         chunkSize,
+      unsigned long         nlists,
+      rvm_tid_t             *tid,
+      int                   *err
+    );
+
+extern int rds_load_heap(
+      char                  *DevName,
+      rvm_offset_t          DevLength,
+      char                  **staticAddr,
+      int                   *err
+    );
+
+extern int rds_start_heap(
+      char                  *startAddr,
+      int                   *err
+    );
+
+extern int rds_prealloc(
+      unsigned long         size,
+      unsigned long         nblocks,
+      rvm_tid_t             *tid,
+      int                   *err
+    );
+
+extern char *rds_malloc(
+      unsigned long size,
+      rvm_tid_t             *tid,
+      int                   *err
+    );
+
+extern int rds_free(
+      char                  *addr,
+      rvm_tid_t             *tid,
+      int                   *err
+    );
+
+int rds_maxblock(unsigned long size);
+
+/*
+ * Because a transaction may abort we don't actually want to free
+ * objects until the end of the transaction. So fake_free records our intention
+ * to free an object. do_free actually frees the object. It's called as part
+ * of the commit.
+ */
+
+typedef struct intlist {
+    unsigned long           size;
+    unsigned long           count;
+    char                    **table;
+} intentionList_t;
+
+#define STARTSIZE 128   /* Initial size of list, may grow over time */
+
+extern int rds_fake_free(
+      char                 *addr,
+      intentionList_t      *list
+    );
+
+extern int rds_do_free(
+      intentionList_t       *list,
+      rvm_mode_t            mode
+    );
+
+/* Heap statistics reporting */
+typedef struct {
+    unsigned            malloc;         /* Allocation requests */
+    unsigned            prealloc;       /* Preallocation requests */
+    unsigned            free;           /* Block free requests */
+    unsigned            coalesce;       /* Heap coalesce count */
+    unsigned            hits;           /* No need to split */
+    unsigned            misses;         /* Split required */
+    unsigned            large_list;     /* Largest list pointer changed */
+    unsigned            large_hits;     /* Large blocks present in list */
+    unsigned            large_misses;   /* Large block split required */
+    unsigned            merged;         /* Objects merged from coalesce */
+    unsigned            unmerged;       /* Objects not merged in coalesce */
+    unsigned            freebytes;      /* Number of free bytes in heap */
+    unsigned            mallocbytes;    /* Bytes allocated */
+} rds_stats_t;
+
+extern int rds_print_stats();
+extern int rds_clear_stats(int *err);
+extern int rds_get_stats(rds_stats_t *stats);
+
+extern int rds_tracing;
+extern FILE *rds_tracing_file;
+extern int rds_trace_on(FILE *);
+extern int rds_trace_off();
+extern int rds_trace_dump_heap();
+#define RDS_LOG(format, a...)\
+  do {                       \
+  if (rds_tracing && rds_tracing_file) { \
+    fprintf(rds_tracing_file, format, ## a);\
+    fflush(rds_tracing_file); }\
+} while (0) ;
+
+
+#endif /* __MERO_RVM_RDS_H__ */
diff --git a/rvm/rds_coalesce.c b/rvm/rds_coalesce.c
new file mode 100644
index 0000000..dbecc2b
--- /dev/null
+++ b/rvm/rds_coalesce.c
@@ -0,0 +1,207 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include <stdlib.h>
+#include "rvm/rds_private.h"
+
+/*
+ * Coalescing has proven necessary. This approach is to invoke it rarely,
+ * like in times of emergency or when the application starts up. This
+ * approach will merge all memory that can be merged, and runs linearly
+ * in the number of free blocks. After this phase has run, the MAXLIST
+ * is reset to the highest non-empty list.
+ */
+
+/* this function merges a freeblock with all of the following free blocks */
+int merge_with_next_free(free_block_t *fbp, rvm_tid_t *tid, int *err)
+{
+    free_block_t *nfbp;
+    guard_t *block_end;
+    rvm_return_t rvmret;
+    int list, merged;
+
+    assert(fbp->type == FREE_GUARD);	/* Ensure fbp is a free block */
+
+    nfbp = NEXT_CONSECUTIVE_BLOCK(fbp);
+    merged = 0;
+
+    /* Do the set_range outside of next loop if appropriate. */
+    if ((nfbp->type == FREE_GUARD) && (nfbp < RDS_HIGH_ADDR)) {
+	rvmret = rvm_set_range(tid, (char *)fbp, sizeof(free_block_t));
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int)rvmret;
+	    return 0;
+	}
+    }
+
+    /* See if the next consecutive object is free. */
+    while ((nfbp->type == FREE_GUARD) && (nfbp < RDS_HIGH_ADDR)) {
+	block_end = BLOCK_END(fbp);/* Save a ptr to the endguard */
+	merged = 1;
+	RDS_STATS.merged++;		/* Update merged object stat */
+	fbp->size += nfbp->size;	/* Update the object's size */
+
+	/* remove the second object from it's list */
+	list = (nfbp->size >= RDS_MAXLIST) ? RDS_MAXLIST : nfbp->size;
+	assert(RDS_FREE_LIST[list].head != NULL);
+
+	rm_from_list(&RDS_FREE_LIST[list], nfbp, tid, err);
+	if (*err != SUCCESS) return 0;
+
+	/* Take out the guards to avoid future confusion. I'm going
+	 * to assume that the next block follows immediately on
+	 * the endguard of the first block */
+	rvmret = rvm_set_range(tid, (char *)block_end,
+			       sizeof(guard_t) + sizeof(free_block_t));
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int)rvmret;
+	    return 0;
+	}
+	*block_end = 0;
+	BZERO(nfbp, sizeof(free_block_t));
+
+	nfbp = NEXT_CONSECUTIVE_BLOCK(fbp);
+    }
+    return merged;
+}
+
+
+void coalesce(rvm_tid_t *tid, int *err)
+{
+    free_block_t *fbp, *save;
+    rvm_return_t rvmret;
+    int i, old_maxlist, merged;
+
+    /* Make sure the heap has been initialized */
+    if (!HEAP_INIT) {
+	(*err) = EHEAP_INIT;
+	return;
+    }
+
+    /* Update stats - don't need setrange, assume caller already has done that. */
+    RDS_STATS.coalesce++;
+
+    *err = SUCCESS; 		/* Initialize the error value */
+
+    /* Go through the lists, examing objects. For each free object, merge it
+     * with the next consecutive object in memory if it is also free. Continue
+     * merging until the next consecutive object is not free. The resulting
+     * object is guaranteed to be larger than the original, so put it on a new
+     * list. Because of this last fact, and because objects are split off the
+     * tail in split(), it may be wiser to run this loop from high to low.
+     * Need to make sure objects aren't pulled off the list from under our feet.
+     */
+
+    for (i = RDS_NLISTS; i > 0; i--) {
+	/* Check to see if the list's guard is alright. */
+	if (RDS_FREE_LIST[i].guard != FREE_LIST_GUARD) {
+	    (*err) = ECORRUPT;
+	    return;
+	}
+
+	fbp = RDS_FREE_LIST[i].head;
+
+	while (fbp != NULL) {
+	    merged = merge_with_next_free(fbp, tid, err);
+	    if (*err)
+		return;
+
+	    if (!merged)
+		RDS_STATS.unmerged++;
+
+	    /* Move fbp, if merged, it must be larger. Don't move if already
+	     * on the highest list. -- Use NLISTS here if MAXLIST < NLISTS.
+	     */
+
+	    if (merged && i < RDS_NLISTS) {
+		/* remove fbp from it's list */
+		rm_from_list(&RDS_FREE_LIST[i], fbp, tid, err);
+		if (*err != SUCCESS) {
+		    return;
+		}
+
+		save = fbp->next; /* Save the old value of next */
+
+		/* place fbp in its new list. */
+		put_block((char *)fbp, tid, err);
+		if (*err != SUCCESS) {
+		    return;
+		}
+
+		fbp = save;
+	    }
+	    else {
+		fbp = fbp->next;
+	    }
+
+	    /* Yield to other threads, merging might take a while */
+	    cthread_yield();
+	}
+    }
+
+    /* Second part is to put any real large objects on RDS_MAXLIST back where they
+       belong. Reset maxlist to it's highest value, RDS_NLIST. Obviously don't
+       need to do the second or third phases if RDS_MAXLIST == RDS_NLISTS. */
+
+    if (RDS_MAXLIST < RDS_NLISTS) {
+	old_maxlist = RDS_MAXLIST;
+
+	rvmret = rvm_set_range(tid, (char *)&(RDS_MAXLIST), sizeof(RDS_MAXLIST));
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int)rvmret;
+	    return;
+	}
+	RDS_MAXLIST = RDS_NLISTS;
+
+	fbp = RDS_FREE_LIST[old_maxlist].head;
+	while (fbp != NULL) {
+	    if (fbp->size > old_maxlist) {
+
+		rm_from_list(&RDS_FREE_LIST[old_maxlist], fbp, tid, err);
+		if (*err != SUCCESS) {
+		    return;
+		}
+
+		save = fbp->next; /* Save the old value of next */
+
+		/* Place the object in it's appropriate list. */
+		put_block((char *)fbp, tid, err);
+		if (*err != SUCCESS) {
+		    return;
+		}
+		fbp = save;
+	    }
+	    else
+		fbp = fbp->next;
+
+	}
+
+	/* Third phase is reset RDS_MAXLIST to the highest non-empty value.*/
+
+	/* Used to be an assertion that maxlist != 1 in the next loop,
+	 * Is this really a problem? I don't see it 1/29 -- dcs
+	 */
+	while ((RDS_FREE_LIST[RDS_MAXLIST].head == NULL) && (RDS_MAXLIST > 1)) {
+	    RDS_MAXLIST--;
+	}
+    }
+
+    *err = SUCCESS;
+}
diff --git a/rvm/rds_free.c b/rvm/rds_free.c
new file mode 100644
index 0000000..2093501
--- /dev/null
+++ b/rvm/rds_free.c
@@ -0,0 +1,268 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <stdlib.h>
+#include "rds_private.h"
+    
+              /************** NOTE: ***************/
+/* we create our own transactions in the following routines, even
+ * though there is a tid in the interface. This might result in unreferenced
+ * objects if the thread which malloc'ed the object decides to abort. These
+ * routines have no way of knowing if such an event happens. Also, if the user
+ * free's then aborts, could have pointers into the free list objects.
+ *
+ * Interface has been changed so that if a user feels confident that his
+ * transactions are serialized, (the above problem won't arise), he can pass
+ * in a non-null tid ptr and it will be used. So if you aren't sure, make the
+ * tidptr is zero!
+ */
+
+/* There's another bug too: if tid1 frees an object, then tid2 mallocs it,
+ * then tid1 aborts, the object is referenced twice. To avoid this and the
+ * above two problems, rather than actually freeing an object during a
+ * transaction, keep a record of our intension to do so and only do it when
+ * the transaction commits. If it aborts, we don't want to free the object
+ * so we can just throw away the intension record. Thus we can keep that record
+ * in VM. The record only needs to indicate which transaction and which object.
+ * By keeping a list per transaction, each entry is simply an address.
+ * So rds_fake_free(addr, list) adds one's intension of freeing the object.
+ * rds_do_free(list, mode) actually frees the objects on the list, using
+ * mode as the end_transaction mode.
+ */
+     
+int
+rds_free(addr, tid, err)
+     char	  *addr;
+     rvm_tid_t    *tid;
+     int	  *err;
+{
+    free_block_t *bp = BLOCK_HDR(addr);  /* find pointer to block header */
+    rvm_tid_t *atid;
+    rvm_return_t rvmret;
+    
+    /* Make sure the heap has been initialized */
+    if (!HEAP_INIT) {
+	(*err) = EHEAP_INIT;
+	return -1;
+    }
+
+    RDS_LOG("rdstrace: Error!!! rds_free called\n");
+
+    /* Make sure that the pointer is word aligned */
+    if ((bp == NULL) || ((unsigned long)bp % sizeof(void *)) != 0) {
+	(*err) = EBAD_ARGS;
+	return -1;
+    }
+    
+    /* Verify that the guards are intact */
+    if (bp->type == FREE_GUARD) { 	/* object was already freed */
+	(*err) = EFREED_TWICE;
+	return -1;
+    }
+
+    /* improper guard --> memory corruption */
+    if ((bp->type != ALLOC_GUARD) || (*BLOCK_END(bp) != END_GUARD)) {
+	(*err) = ECORRUPT;
+	return -1;
+    }
+
+    if (tid == NULL) {			     /* Use input tid if non-null */
+	atid = rvm_malloc_tid();
+	rvmret = rvm_begin_transaction(atid, restore);
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int) rvmret;
+	    rvm_free_tid(atid);
+	    return -1;
+	}
+    } else
+	atid = tid;
+
+    *err = SUCCESS; 		/* Initialize the error value */
+    START_CRITICAL;
+    {
+	/* Update statistics */
+	rvmret = rvm_set_range(atid, &RDS_STATS, sizeof(rds_stats_t));
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int)rvmret;
+	} else {
+	    /* Set the lowguard to reflect that the block has been allocated. */
+	    rvmret = rvm_set_range(atid, &(bp->type), sizeof(guard_t));
+	    if (rvmret != RVM_SUCCESS) {
+		if (tid == NULL) {
+		    rvm_abort_transaction(atid);
+		    rvm_free_tid(atid);
+		}
+		(*err) = (int)rvmret;
+	    } else {
+		bp->type = FREE_GUARD;
+
+		RDS_STATS.free++;
+		RDS_STATS.freebytes   += bp->size * RDS_CHUNK_SIZE;
+		RDS_STATS.mallocbytes -= bp->size * RDS_CHUNK_SIZE;    
+
+		/* try to merge with trailing free blocks */
+		merge_with_next_free(bp, atid, err);
+
+		/* Add the block to the approprite free list. */
+		if (*err == SUCCESS)
+		    put_block(bp, atid, err); /* Error is picked up below... */
+	    }
+	}
+
+	if ((*err != SUCCESS) && (tid == NULL)) {
+	    rvm_abort_transaction(atid);
+	    rvm_free_tid(atid);
+	} else if (tid == NULL) {
+	    rvmret = rvm_end_transaction(atid, no_flush);
+	    rvm_free_tid(atid);
+	    if (rvmret != RVM_SUCCESS) {
+		(*err) = (int)rvmret;
+	    }
+	}
+    }
+    END_CRITICAL;
+	
+    return 0;
+}
+
+/* Assume only one thread can have an open tid at a time. Since we're only
+ * modifying per-tid structures, we don't have any concurency.
+ */
+int rds_fake_free(addr, list)
+     char *addr;
+     intentionList_t *list;
+{
+    char **temp;
+    free_block_t *bp = BLOCK_HDR(addr);  /* find pointer to block header */
+
+    
+    /* Make sure the heap has been initialized */
+    if (!HEAP_INIT) {
+	return EHEAP_INIT;
+    }
+
+    /* Freeing a NULL ptr? */
+    if (!addr)
+	return SUCCESS;
+
+    /* Make sure that the pointer is word aligned */
+    if (((unsigned long)bp % sizeof(void *)) != 0)
+	return EBAD_ARGS;
+    
+    /* Verify that the guards are intact */
+    if (bp->type == FREE_GUARD) 	/* object was already freed */
+	return EFREED_TWICE;
+
+    /* improper guard --> memory corruption */
+    if ((bp->type != ALLOC_GUARD) || (*BLOCK_END(bp) != END_GUARD))
+	return ECORRUPT;
+
+    /* If no intention list has been built for this tid yet, build one. */
+    if (list->table == NULL) {
+	list->size = STARTSIZE;
+	list->table = (char **)malloc(list->size);
+	list->count = 0;
+    } else if ((list->count * sizeof(void *)) == list->size) {
+	list->size *= 2;
+	temp = (char **)malloc(list->size);
+	BCOPY(list->table, temp, list->count * sizeof(char *));
+	free(list->table);
+	list->table = temp;
+    }
+
+    (list->table)[list->count++] = (char *)addr;
+    return(SUCCESS);
+}
+
+int rds_do_free(list, mode)
+     intentionList_t *list;
+     rvm_mode_t mode;
+{
+    int i, err;
+    rvm_tid_t *tid = rvm_malloc_tid();
+    rvm_return_t rvmret;
+    
+    rvmret = rvm_begin_transaction(tid, restore);
+    if (rvmret != RVM_SUCCESS) {
+        rvm_free_tid(tid);
+	return (int) rvmret;
+    }
+    
+    RDS_LOG("rdstrace: start do_free\n");
+
+    err = SUCCESS; 		/* Initialize the error value */
+    START_CRITICAL;
+    {
+	/* Only need to set the range once. To be safe, doing in critical...*/
+	rvmret = rvm_set_range(tid, &RDS_STATS, sizeof(rds_stats_t));
+	if (rvmret != RVM_SUCCESS) {
+	    err = (int)rvmret;
+	} else
+	  for (i = 0; i < list->count; i++) {
+	    /* find pointer to block header */
+	    free_block_t *bp = BLOCK_HDR((list->table)[i]); 
+
+	    /* Set the lowguard to reflect that the block has been allocated. */
+	    assert(bp->type == ALLOC_GUARD);
+	    rvmret = rvm_set_range(tid, &(bp->type), sizeof(guard_t));
+	    if (rvmret != RVM_SUCCESS) {
+		err = (int)rvmret;
+		break;
+	    }
+	    bp->type = FREE_GUARD;
+
+	    /* Update statistics */
+	    RDS_STATS.free++;
+	    RDS_STATS.freebytes   += bp->size * RDS_CHUNK_SIZE;
+	    RDS_STATS.mallocbytes -= bp->size * RDS_CHUNK_SIZE; 
+	    
+	    RDS_LOG("rdstrace: addr %p size %lx\n",
+				     USER_BLOCK(bp)  , bp->size * RDS_CHUNK_SIZE);
+
+	    /* try to merge with trailing free blocks */
+	    merge_with_next_free(bp, tid, &err);
+	    if (err != SUCCESS)
+		break;
+
+	    /* Add the block to the approprite free list. */
+	    put_block(bp, tid, &err); 
+	    if (err != SUCCESS)
+		break;
+	}
+	
+	RDS_LOG("rdstrace: end do_free\n");
+
+	if (err != SUCCESS) {
+	    rvm_abort_transaction(tid);
+	} else {
+	    rvmret = rvm_end_transaction(tid, mode);
+	}
+    }
+    END_CRITICAL;
+	
+    rvm_free_tid(tid);
+    free(list->table);
+    list->table = NULL;		/* Just to be safe */
+
+    if (err != SUCCESS) return err;
+    if (rvmret != RVM_SUCCESS) return (int) rvmret;
+    return (err != SUCCESS) ? err : ((rvmret != RVM_SUCCESS) ? (int)rvmret : 0);
+}
diff --git a/rvm/rds_init.c b/rvm/rds_init.c
new file mode 100644
index 0000000..a6feff1
--- /dev/null
+++ b/rvm/rds_init.c
@@ -0,0 +1,119 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include "rds_private.h"
+
+/* This routine initializes a region of memory to contain the heap. The
+ * calling routine is assumed to have started a transaction and so assumes
+ * the responsibility of aborting on error.
+ * This routine should run entirely seperately from the other heap operations.
+ * As such it is assumed to never be run concurently with any other rds
+ * routine.
+ */
+
+int
+rds_init_heap(base, length, chunk_size, nlists, tid, err)
+     char *base;
+     rvm_length_t length;
+     unsigned long chunk_size;
+     unsigned long nlists;
+     rvm_tid_t *tid;
+     int       *err;
+{
+    heap_header_t *hdrp = (heap_header_t *)base;
+    free_block_t *fbp;
+    int i, remaining_space;
+    unsigned long heap_hdr_len;
+    rvm_return_t rvmret;
+    guard_t *addr;
+    
+    /* heap consists of a heap_header_t followed by nlist list headers */
+    heap_hdr_len = sizeof(heap_header_t) + nlists * sizeof(free_list_t);
+    if (heap_hdr_len > length) {
+	printf("Heap not long enough to hold heap header\n");
+	(*err) = ENO_ROOM;
+	return -1;
+    }
+
+    rvmret = rvm_set_range(tid, base, heap_hdr_len);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+	return -1;
+    }
+
+    assert(chunk_size >= sizeof(free_block_t) + sizeof(guard_t));
+
+    strcpy(hdrp->version, RDS_HEAP_VERSION);
+    hdrp->heaplength = length;
+    hdrp->chunk_size = chunk_size;
+    hdrp->nlists = hdrp->maxlist = nlists;
+
+    /* Initialize the statistics to zero */
+    BZERO(&(hdrp->stats), sizeof(rds_stats_t));
+    
+    /* create nlists free list structures, making each list null. */
+    /* Since the lists are indexed by number of chunks,
+     * 1 should be the first entry, not zero. */
+
+    for (i = 1; i < nlists + 1; i++) {
+	hdrp->lists[i].head = (free_block_t *)NULL;
+	hdrp->lists[i].guard = FREE_LIST_GUARD;
+    }
+
+    /* For the last list, make it point to the first page after the list header
+     * array. At this point, write out a null pointer and the size of the
+     * remaining region. Make the last list header point to this address */
+
+    remaining_space = length - heap_hdr_len;
+
+    /* determine where the first block will start */
+    fbp = (free_block_t *)((char *)&(hdrp->lists[nlists]) + sizeof(free_list_t)); 
+    /* Round this up to the chunk size */
+    fbp = (free_block_t *)(((long)((char *)fbp + chunk_size - 1) / chunk_size) * chunk_size);
+
+    rvmret = rvm_set_range(tid, fbp, sizeof(free_block_t));
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+	return -1; 
+    }
+
+    /* put the block on the list, making it null */
+    fbp->size = remaining_space / chunk_size;
+    fbp->type = FREE_GUARD;
+    fbp->prev = fbp->next = (free_block_t *)NULL;
+    hdrp->lists[nlists].head = fbp;
+
+    hdrp->stats.freebytes = fbp->size * chunk_size;  /* Num of allocatable bytes*/
+
+    /* Add the guard to the end of the block */
+    addr = (guard_t *)((char *)fbp + fbp->size * chunk_size);
+    assert((char *)addr <= base + length);
+    
+    addr--;  /* point to last word in the block */
+    rvmret = rvm_set_range(tid, addr, sizeof(guard_t));
+    if (rvmret != RVM_SUCCESS) {  
+	(*err) = (int) rvmret;
+	return -1;
+    }
+    (*addr) = END_GUARD;
+
+    (*err) = SUCCESS;
+    return 0;
+}
diff --git a/rvm/rds_malloc.c b/rvm/rds_malloc.c
new file mode 100644
index 0000000..46d646f
--- /dev/null
+++ b/rvm/rds_malloc.c
@@ -0,0 +1,136 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include "rds_private.h"
+
+              /************** NOTE: ***************/
+/* we create our own transactions in the following routines, even
+ * though there is a tid in the interface. This might result in unreferenced
+ * objects if the thread which malloc'ed the object decides to abort. These
+ * routines have no way of knowing if such an event happens. Also, if the user
+ * free's then aborts, could have pointers into the free list objects.
+ *
+ * Interface has been changed so that if a user feels confident that his
+ * transactions are serialized, (the above problem won't arise), he can pass
+ * in a non-null tid ptr and it will be used. So if you aren't sure, make the
+ * tidptr is zero!
+ */
+  
+/* Allocate a free block which will hold size + some bytes. The some refers to
+ * a size field, and two guards to detect overwriting of memory, which will
+ * be added to the object. Treat the last list seperately since it holds objects
+ * of that size chunks OR LARGER. A pointer to size bytes will be returned. */ 
+char *
+rds_malloc(size, tid, err)
+     unsigned long size;
+     rvm_tid_t     *tid;
+     int	   *err;
+{
+    free_block_t *fbp=NULL;
+    rvm_tid_t *atid;
+    rvm_return_t rvmret;
+    int i;
+    unsigned long orig_size = size;
+
+    /* Make sure the heap has been initialized */
+    if (!HEAP_INIT) {
+	(*err) = EHEAP_INIT;
+	return NULL;
+    }
+
+    /* Reserve bytes to hold the block's size and 2 guards, hidden from user */
+    size += RDS_BLOCK_HDR_SIZE;
+
+    i = (size / RDS_CHUNK_SIZE) + 1;         /* determine which list to use */
+    
+    if (tid == NULL) {			     /* Use input tid if non-null */
+	atid = rvm_malloc_tid();
+	rvmret = rvm_begin_transaction(atid, restore);
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int)rvmret;
+	    rvm_free_tid(atid);
+	    return NULL;
+	}
+    } else
+	atid = tid;
+    
+
+    *err = SUCCESS; 		/* Initialize the error value */
+    START_CRITICAL;
+    {
+	/* Update stats */
+	rvmret = rvm_set_range(atid, &RDS_STATS, sizeof(rds_stats_t));
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int)rvmret;
+	    if (tid == NULL) {
+		rvm_abort_transaction(atid);
+		rvm_free_tid(atid);
+	    }
+	    LEAVE_CRITICAL_SECTION;
+	} 
+
+	RDS_STATS.malloc++;	
+	RDS_STATS.freebytes -= i * RDS_CHUNK_SIZE;
+	RDS_STATS.mallocbytes += i * RDS_CHUNK_SIZE;
+		
+	/* Get a block of the correct size. */
+	fbp = get_block(i, atid, err); 
+	if (*err != SUCCESS) {
+	    if (tid == NULL) {
+		rvm_abort_transaction(atid);
+		rvm_free_tid(atid);
+	    }
+	    LEAVE_CRITICAL_SECTION;
+	}
+
+	assert(fbp->size == i);	/* Sanity check */
+    
+	/* Check to see that the guards are valid and the type is free */
+	assert((fbp->type == FREE_GUARD) && ((*BLOCK_END(fbp)) == END_GUARD));
+		
+	/* Set the lowguard to reflect that the block has been allocated. */
+	rvmret = rvm_set_range(atid, fbp, sizeof(free_block_t));
+	if (rvmret != RVM_SUCCESS) {
+	    if (tid == NULL) {
+		rvm_abort_transaction(atid);
+		rvm_free_tid(atid);
+	    }
+	    (*err) = (int)rvmret;
+	    LEAVE_CRITICAL_SECTION;
+
+	}
+	fbp->type = ALLOC_GUARD;
+	fbp->prev = fbp->next = NULL;
+    
+	if (tid == NULL) {		/* Let code below pick up the error. */
+	    (*err) =(int) rvm_end_transaction(atid, no_flush);
+	    rvm_free_tid(atid);
+	}
+    }
+    END_CRITICAL;
+
+    if (*err != SUCCESS) return NULL;
+
+    RDS_LOG("rdstrace: malloc addr %p size %lx req %lx\n",
+			     USER_BLOCK(fbp), i * RDS_CHUNK_SIZE, orig_size);
+    
+    return(USER_BLOCK(fbp));
+}
+
diff --git a/rvm/rds_maxblock.c b/rvm/rds_maxblock.c
new file mode 100644
index 0000000..288f2b6
--- /dev/null
+++ b/rvm/rds_maxblock.c
@@ -0,0 +1,45 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/* rds_maxblock - check if we will be able to allocate enough RVM data
+ * written by J. Michael German */
+
+#include "rds_private.h"
+
+int rds_maxblock(unsigned long size) 
+{
+    unsigned long nblocks = size / RDS_CHUNK_SIZE;
+    free_block_t *tempbp;
+    int i;
+
+    if (RDS_FREE_LIST[RDS_MAXLIST].head != NULL) {
+	tempbp = RDS_FREE_LIST[RDS_MAXLIST].head;
+	while (tempbp) {
+	    if (tempbp->size >= nblocks) 
+		return 1;
+	    tempbp = tempbp->next;
+	}
+    } else {
+	for (i = RDS_MAXLIST - 1; i > nblocks; i--) {
+	    if (RDS_FREE_LIST[i].head) 
+		return 1;
+	}
+    }
+    return 0;
+}
+
diff --git a/rvm/rds_prealloc.c b/rvm/rds_prealloc.c
new file mode 100644
index 0000000..7a0df6e
--- /dev/null
+++ b/rvm/rds_prealloc.c
@@ -0,0 +1,135 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include "rds_private.h"
+
+	    /************** NOTE: ***************/
+/* we create our own transactions in the following routines, even
+ * though there is a tid in the interface. This might result in unreferenced
+ * objects if the thread which malloc'ed the object decides to abort. These
+ * routines have no way of knowing if such an event happens. Also, if the user
+ * free's then aborts, could have pointers into the free list objects.
+ *
+ * Interface has been changed so that if a user feels confident that his
+ * transactions are serialized, (the above problem won't arise), he can pass
+ * in a non-null tid ptr and it will be used. So if you aren't sure, make the
+ * tidptr is zero!
+ */
+
+/* Preallocate nblocks of size bytes. Used to fill free lists with blocks
+ * of appropriate size so splits won't happen during rds_malloc().
+ */
+
+int
+rds_prealloc(size, nblocks, tid, err)
+     unsigned long size, nblocks;
+     rvm_tid_t     *tid;
+     int	   *err;
+{
+    free_block_t *bp;
+    rvm_tid_t *atid;
+    int i;
+    rvm_return_t rvmerr;
+    
+    if (!HEAP_INIT) {  /* Make sure the heap is initialized */
+	(*err) = EHEAP_INIT;    
+	return -1;
+    }
+
+    /* Reserve bytes to hold the block's size and 2 guards, hidden from user */
+    /* Calculate the chunk size which holds that many bytes. */
+    size = ((size + RDS_BLOCK_HDR_SIZE) / RDS_CHUNK_SIZE) + 1;
+    
+    /*
+     * if size == maxlist, then preallocing is pointless. The new object
+     * is placed on the beginning of the list, then every split after that
+     * will return that same block, and put_block will put it back at the head.
+     */
+    if (size == RDS_MAXLIST) {
+	*err = SUCCESS;
+	return -1;
+    }
+
+    if (tid == NULL) {		     /* Use input tid if non-null */
+	atid = rvm_malloc_tid();
+	rvmerr = rvm_begin_transaction(atid, restore);
+	if (rvmerr != RVM_SUCCESS) {
+	    (*err) = (int) rvmerr;
+	    rvm_free_tid(atid);
+	    return -1;
+	}
+    } else
+	atid = tid;
+
+    /* Update statistics */
+    rvmerr = rvm_set_range(atid, &RDS_STATS, sizeof(rds_stats_t));
+    if ((rvmerr != RVM_SUCCESS) && (tid == NULL)) {
+	rvm_abort_transaction(atid);
+	(*err) = (int)rvmerr;
+	rvm_free_tid(atid);
+	return -1;
+    }
+    RDS_STATS.prealloc++;	/* Update statistics. */
+
+    *err = SUCCESS; 		/* Initialize the error value */
+
+    /*
+     * Here I put the critical section within the loop. I don't think prealloc
+     * needs to be streamlined and it allows slightly more parallelization.
+     */
+
+    for (i = 0; i < nblocks; i++) {
+	START_CRITICAL;
+	{
+	    /* Get a block */	
+	    bp = split(size, atid, err); 
+	    if (bp != NULL) { 
+		/* Add the block to the appropriate list. */
+		put_block(bp, atid, err);
+	    }
+	}
+	END_CRITICAL;
+
+	if (*err != SUCCESS) {
+	    if (tid == NULL) {
+		rvm_abort_transaction(atid);
+		rvm_free_tid(atid);
+	    }
+	    return -1;
+	}
+    }
+
+    if (tid == NULL) {
+	rvmerr = rvm_end_transaction(atid, no_flush);
+	if (rvmerr != RVM_SUCCESS) {
+	    (*err) = (int) rvmerr;
+	    rvm_free_tid(atid);
+	    return -1;
+	}
+	
+	rvm_free_tid(atid);
+    }
+    
+    *err = SUCCESS;
+    return 0;
+}
+
+
+
diff --git a/rvm/rds_private.h b/rvm/rds_private.h
new file mode 100644
index 0000000..be41c53
--- /dev/null
+++ b/rvm/rds_private.h
@@ -0,0 +1,192 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+/*
+ * Internal type definitions for the Recoverable Dynamic Storage package.
+ */
+#pragma once
+
+#ifndef __MERO_RVM_RDS_PRIVATE_H__
+#define __MERO_RVM_RDS_PRIVATE_H__
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#ifdef __STDC__
+#include <string.h>
+#include "assert.h"
+#endif
+
+#include "rvm/rvm.h"
+#include "rvm/rvm_segment.h"
+#include "rvm/rds.h"
+
+/********************
+ *Type definitions
+ */
+
+/*
+ * Rather than wasting cycles to manage locks which will only produce a
+ * small amount of concurrency, We decided to have one mutex on the entire
+ * heap. With the exception of coalescing, none of the routines should take
+ * substantial time since most are memory changes and simple computations.
+ */
+
+/* Synchronization and Threads support */
+
+/*
+ * We can have one of three thread models:
+ *          cthreads:         Mach threads (kernel or coroutine)
+ *          lwp:              Coda's lightweight process package
+ *          pthreads:         POSIX threads
+ *
+ * If RVM_USELWP is defined, then lwp support is compiled in.
+ * If RVM_USEPT  is defined, then pthreads support is compiled in.
+ * If niether of these is defined, then cthreads support is compiled in.
+ *
+ * It is assumed in the rds package that cthreads and pthreads use
+ * preemptive scheduling, and they are synchronized appropriately.
+ *
+ * You must define only one of the above targets, and it must be defined
+ * consistently across the following packages: RVM, RDS, and URT
+ */
+
+#ifdef RVM_USELWP	 /* special thread support for Coda */
+#include "rvm/rvm_lwp.h"
+#elif defined(RVM_USEPT) /* special support for pthreads */
+#include "rvm/rvm_pthread.h"
+#else			 /* normal: use Cthreads */
+#include "rvm/cthreads.h"
+
+/* define types symbolically to permit use of non-Cthread thread support */
+#define RVM_MUTEX       struct mutex
+#define RVM_CONDITION	struct condition
+#endif
+
+#define START_CRITICAL	mutex_lock(&heap_lock)
+#define LEAVE_CRITICAL_SECTION	goto end_critical
+#define END_CRITICAL	goto end_critical; \
+    end_critical:	mutex_unlock(&heap_lock)
+
+/* Guards detect if the block structure had been illegally overwritten.
+ * One is placed after the size, and before user's data. The other is placed
+ * at the end of the block. */
+
+#define FREE_GUARD  0x345298af
+#define ALLOC_GUARD 0x783bd92c
+#define END_GUARD   0xfd10a32e
+
+#define RDS_BLOCK_HDR_SIZE (sizeof(block_size_t) + 2 * sizeof(guard_t))
+#define BLOCK_END(bp) ((guard_t *)((char *)(bp) + ((bp)->size * RDS_CHUNK_SIZE)) - 1)
+
+#define USER_BLOCK(bp) ((char *)&((bp)->prev))
+#define BLOCK_HDR(bp)  ((free_block_t *)((char *)(bp) - \
+			     (sizeof(block_size_t) + sizeof(guard_t))))
+
+typedef unsigned long block_size_t;
+typedef unsigned long guard_t;
+
+typedef struct fbt {
+    guard_t	 type;
+    block_size_t size;
+    struct fbt   *prev, *next;
+} free_block_t;
+
+#define FREE_LIST_GUARD 0xad938945
+
+typedef struct {
+    guard_t	 guard;
+    free_block_t *head;
+} free_list_t;
+
+#define NEXT_CONSECUTIVE_BLOCK(bp) ((free_block_t *)((char *)(bp) + ((bp)->size * RDS_CHUNK_SIZE)))
+
+#define HEAP_LIST_GROWSIZE 20		/* Number of blocks to prealloc */
+
+#define RDS_HEAP_VERSION "Dynamic Allocator Using Rvm Release 0.1 1 Dec 1990"
+#define RDS_VERSION_MAX 80
+
+/*typedef struct {*/
+struct heap_header {
+    char          version[RDS_VERSION_MAX]; /* Version String */
+    unsigned long heaplength;
+    unsigned long chunk_size;
+    unsigned long nlists;
+    rds_stats_t	  stats;		/* statistics on heap usage. */
+    unsigned long maxlist;		/* Current non-empty largest list */
+    unsigned long dummy[10];		/* Space to allow header to grow */
+    free_list_t lists[1];              /* Number of lists is dynamically set */
+};
+
+/* Global data extern declarations. */
+extern heap_header_t *RecoverableHeapStartAddress;
+extern free_block_t  *RecoverableHeapHighAddress;
+extern RVM_MUTEX heap_lock;
+
+extern int rds_tracing;
+extern FILE *rds_tracing_file;
+
+
+#define HEAP_INIT   		(RecoverableHeapStartAddress != 0)
+#define RDS_VERSION_STAMP	(RecoverableHeapStartAddress->version)
+#define RDS_HEAPLENGTH 		(RecoverableHeapStartAddress->heaplength)
+#define RDS_CHUNK_SIZE 		(RecoverableHeapStartAddress->chunk_size)
+#define RDS_FREE_LIST  		(RecoverableHeapStartAddress->lists)
+#define RDS_NLISTS		(RecoverableHeapStartAddress->nlists)
+#define RDS_MAXLIST		(RecoverableHeapStartAddress->maxlist)
+#define RDS_STATS		(RecoverableHeapStartAddress->stats)
+#define RDS_HIGH_ADDR		(RecoverableHeapHighAddress)
+
+/*******************
+ * byte <-> string
+ */
+#ifndef BZERO
+#ifdef __STDC__
+#define BCOPY(S,D,L)   memcpy((D),(S),(L))
+#define BZERO(D,L)     memset((D),0,(L))
+#else
+#define BCOPY(S,D,L)   bcopy((S),(D),(L))
+#define BZERO(D,L)     bzero((D),(L))
+#endif
+#endif
+
+/********************
+ * Definitions of worker functions.
+ */
+extern int enqueue();
+extern free_block_t *dequeue();
+extern int print_heap();
+extern free_block_t *split();
+extern free_block_t *get_block();
+extern int put_block();
+
+/*********************
+ * Definitions of util functions
+ */
+free_block_t *dequeue();
+int           rm_from_list();
+
+/***********************
+ * Coalesce
+ */
+int merge_with_next_free(free_block_t *fbp, rvm_tid_t *tid, int *err);
+void coalesce(rvm_tid_t *tid, int *err);
+
+#endif /* __MERO_RVM_RDS_PRIVATE_H__ */
diff --git a/rvm/rds_split.c b/rvm/rds_split.c
new file mode 100644
index 0000000..547978d
--- /dev/null
+++ b/rvm/rds_split.c
@@ -0,0 +1,170 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include "rds_private.h"
+
+/* Split the first block larger than size chunks into two objects.
+ * The object with the appropriate size is returned to the caller
+ * AND IS NOT PLACED ON ANY LIST. The remaining object is
+ * placed onto the appropriate list, based on its new size. Assume caller
+ * aborts on error. The returned object is Object2 to make thing easier.
+ */
+
+free_block_t *
+split(size, tid, err)
+     int 	  size;
+     rvm_tid_t	  *tid;
+     int	  *err;
+{
+    free_block_t *newObject1, *newObject2;
+    free_block_t *bp, *tempbp;
+    rvm_return_t rvmerr;
+    int remaining_size, i;
+    free_list_t  *list;
+    int second_attempt = 0;
+    
+    /* Find the list with the largest blocks that is non-empty. */
+    /* Only do the setrange if necessary... */
+    if (RDS_FREE_LIST[RDS_MAXLIST].head == NULL) {
+	rvmerr = rvm_set_range(tid, &RDS_MAXLIST, sizeof(unsigned long));
+	if (rvmerr != RVM_SUCCESS) {
+	    (*err) = (int) rvmerr;
+	    return NULL;
+	}
+	
+	/* Don't need a set range, assume caller already did that. */
+	RDS_STATS.large_list++; /* Only bump once, not once per MAXLIST-- */
+	
+	/* find the first nonempty list larger than size */
+	while (RDS_MAXLIST > size && RDS_FREE_LIST[RDS_MAXLIST].head == NULL) {
+	    RDS_MAXLIST--;
+	}
+	
+	/* If no possible block big enough now, see if coalesce will save us.
+	 * Coalesce resets MAXLIST to the highest nonempty list.
+	 */
+	if (RDS_FREE_LIST[RDS_MAXLIST].head == NULL) { 
+	    coalesce(tid, err);        
+	    if (*err)
+		return NULL;
+	}
+    }
+
+retry:
+    /* Kind of a hack, try to avoid fragmenting the blocks on MAXLIST by
+     * picking from a list that is a multiple of the requested size. */
+    list = &RDS_FREE_LIST[RDS_MAXLIST];
+    for (i = size * 2; i < RDS_MAXLIST; i += size) {
+	if (RDS_FREE_LIST[i].head) {
+	    list = &RDS_FREE_LIST[i];
+	    break;
+	}
+    }
+    
+    bp = NULL;
+    tempbp = list->head;
+    while (tempbp) {
+	/* best fit strategy, only really useful for the largest list which
+	 * contains mixed size blocks but since we break out whenever a match
+	 * is found this shouldn't add much overhead to the simpler cases */
+	if (tempbp->size >= size && (!bp || tempbp->size < bp->size)) {
+	    bp = tempbp;
+	    if (bp->size == size)
+		break;
+	}
+	tempbp = tempbp->next;
+    }
+
+    if (!bp) {
+	if (!second_attempt) {
+	    /* No block was big enough on the first try,
+	     * coalesce and try again */
+	    coalesce(tid, err);  
+	    if (*err) return NULL;
+
+	    second_attempt = 1;
+	    goto retry;
+	}
+
+	*err = ENO_ROOM; 
+	return NULL;
+    }
+
+    assert(bp && bp->size >= size); /* Assume we found an appropriate block */
+    
+    if (size == bp->size) { /* We have an exact fit */
+	rm_from_list(list, bp, tid, err);
+	if (*err != SUCCESS)
+	    return NULL;
+	return bp;
+    }
+    
+    /* Calculate size of block remaining after desired block is split off. */
+    remaining_size = bp->size - size;
+    assert(remaining_size > 0);
+    
+    newObject1 = bp;
+    newObject2 = (free_block_t *)	/* Cast as char * to get byte addition */
+	((char *)bp + remaining_size * RDS_CHUNK_SIZE);
+
+    /* Init the headers for the new objects. */
+    
+    /* For newObject1, lowguard is set, size and highguard need updating. */
+    rvmerr = rvm_set_range(tid, newObject1, sizeof(free_block_t));
+    if (rvmerr != RVM_SUCCESS) {
+	(*err) = (int) rvmerr;
+	return NULL;
+    }
+    newObject1->size = remaining_size;
+
+    /* Add the highguard to the end of the block */
+    rvmerr = rvm_set_range(tid, BLOCK_END(newObject1), sizeof(guard_t));
+    if (rvmerr != RVM_SUCCESS)  {
+	(*err) = (int) rvmerr;
+	return NULL;
+    }
+    (*BLOCK_END(newObject1)) = END_GUARD;
+    
+    /* for newObject2, size and lowguard need setting, highguard doesn't */
+    rvmerr = rvm_set_range(tid, newObject2, sizeof(free_block_t));
+    if (rvmerr != RVM_SUCCESS)  {
+	(*err) = (int) rvmerr;
+	return NULL;
+    }
+    newObject2->size = size;
+    newObject2->type = FREE_GUARD;
+
+    /* Put Object1 on the appropriate free list(s). */
+    /* If Object1 doesn't need to be moved, nothing needs to happen. */
+
+    /* Otherwise Object1 is taken off the old list and moved to a new one. */
+    if (newObject1->size < RDS_MAXLIST) {
+	rm_from_list(list, newObject1, tid, err);
+	if (*err != SUCCESS)
+	    return NULL;
+
+	/* newObject1 has been removed, now add it to the appropriate list */
+	put_block(newObject1, tid, err);
+	if (*err != SUCCESS) return NULL;
+    }
+
+    *err = SUCCESS;
+    return newObject2;
+}
diff --git a/rvm/rds_start.c b/rvm/rds_start.c
new file mode 100644
index 0000000..05fd859
--- /dev/null
+++ b/rvm/rds_start.c
@@ -0,0 +1,137 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#include <stdlib.h>
+#include <stdio.h>
+#include "rds_private.h"
+    
+              /************** NOTE: ***************/
+/* we create our own transactions in the following routines, even
+ * though there is a tid in the interface. This might result in unreferenced
+ * objects if the thread which malloc'ed the object decides to abort. These
+ * routines have no way of knowing if such an event happens. Also, if the user
+ * free's then aborts, could have pointers into the free list objects.
+ *
+ * Interface has been changed so that if a user feels confident that his
+ * transactions are serialized, (the above problem won't arise), he can pass
+ * in a non-null tid ptr and it will be used. So if you aren't sure, make the
+ * tidptr is zero!
+ */
+
+
+/* Global pointer to the start of the heap, but only accessed via heap macros
+ * and routines. Should never be directly addressed by outside code.
+ */
+    
+heap_header_t *RecoverableHeapStartAddress;
+
+/*
+ * Global pointer to last address in heap region. Make it a block ptr to
+ * avoid casts in comparisons.
+ */
+free_block_t *RecoverableHeapHighAddress;
+rvm_region_def_t *RegionDefs;
+unsigned long     NRegionDefs;
+rvm_bool_t       rds_testsw = rvm_false;   /* switch to allow special
+                                              test modes */
+/*
+ * Global lock for the heap. See comment in rds_private.h.
+ */
+
+/* 
+ * Cannot statically initialize locks in pthreads.  So, we also need a flag
+ * for double initialization.
+ */
+#ifndef RVM_USEPT
+RVM_MUTEX	   heap_lock = MUTEX_INITIALIZER;
+#else
+RVM_MUTEX          heap_lock;
+static rvm_bool_t  inited = rvm_false;
+#endif
+
+/* Let the application know the starting address of the heap. This is
+ * EFFECTIVELY idempotent since it will fail in rvm_load_segment without
+ * modifying any structures if it has already been called.
+ */
+int
+rds_load_heap(DevName, DevLength, static_addr, err)
+     char	  *DevName;
+     rvm_offset_t DevLength;
+     char	  **static_addr; /* Start of region holding statics */
+     int	  *err;
+{
+    rvm_return_t     rvmret;
+
+    /* Map in the appropriate structures by calling Rvm_Load_Segment. */
+    rvmret = rvm_load_segment(DevName, DevLength, NULL,
+			      &NRegionDefs, &RegionDefs);
+    if (rvmret != RVM_SUCCESS) {
+	    printf("Error rvm_load_segment returns %d\n", rvmret);
+	    (*err) = (int) rvmret;
+	    return -1;
+    }
+
+    if (NRegionDefs != 2) {
+	free(RegionDefs);
+	(*err) = EBAD_SEGMENT_HDR;
+	return -1;
+    }
+    
+    (*static_addr) = (char *)RegionDefs[1].vmaddr;
+
+    rds_start_heap(RegionDefs[0].vmaddr, err);
+
+    return 0;
+}
+
+/*
+ * Provide an interface which doesn't know about the segment layout.
+ */
+
+int
+rds_start_heap(startAddr, err)
+     char *startAddr;
+     int *err;
+{
+    unsigned long    heap_hdr_len;
+
+    /* If we are pthreaded, we need to init the heap lock.  Must only
+       do this once! */
+#ifdef RVM_USEPT
+    if (inited == rvm_false) {
+	inited = rvm_true;
+	mutex_init(&heap_lock);
+    }
+#endif
+    RecoverableHeapStartAddress = (heap_header_t *)startAddr;
+
+    /* Match version stamps */
+    if (strcmp(RDS_HEAP_VERSION, RDS_VERSION_STAMP) != 0) {
+	*err = EHEAP_VERSION_SKEW;
+        return -1;
+    }
+
+    heap_hdr_len = sizeof(heap_header_t) + RDS_NLISTS * sizeof(free_list_t);
+    RecoverableHeapHighAddress = (free_block_t *)
+	((char *)RecoverableHeapStartAddress +
+	    ((RDS_HEAPLENGTH - heap_hdr_len)/ RDS_CHUNK_SIZE) * RDS_CHUNK_SIZE +
+		heap_hdr_len);
+
+    *err = SUCCESS;
+    return -1;
+}
diff --git a/rvm/rds_stats.c b/rvm/rds_stats.c
new file mode 100644
index 0000000..32de07d
--- /dev/null
+++ b/rvm/rds_stats.c
@@ -0,0 +1,223 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include "rds_private.h"
+
+int rds_tracing = FALSE;
+FILE *rds_tracing_file = NULL;
+    
+/*
+ * Print out the current statistics
+ */
+int
+rds_print_stats()
+{
+    if (!HEAP_INIT)	/* Make sure RecoverableHeapStartAddress is inited */
+	return -1;	
+
+    /* Not making this a critical section since a race condition only reports
+       slightly bogus statistics -- and off by one over several thousand isn't
+       significant. */
+       
+    printf("Number of\n");
+    printf(" Free bytes: \t %x\n", RDS_STATS.freebytes);
+    printf(" Alloced bytes:\t %x\n", RDS_STATS.mallocbytes);
+    printf(" Mallocs: \t %d\n", RDS_STATS.malloc);
+    printf(" Frees: \t %d\n",  RDS_STATS.free);
+    printf(" Preallocs: \t %d\n",  RDS_STATS.prealloc);
+    printf(" Hits: \t\t %d\n",  RDS_STATS.hits);
+    printf(" Misses: \t %d\n",  RDS_STATS.misses);
+    printf(" Large Hits: \t %d\n",  RDS_STATS.large_hits);
+    printf(" Large Misses: \t %d\n",  RDS_STATS.large_misses);
+    printf(" Coalesces: \t %d\n",  RDS_STATS.coalesce);
+    printf(" Merges \t %d\n", RDS_STATS.merged);
+    printf(" Not Merged: \t %d\n", RDS_STATS.unmerged);
+    printf(" Times the Large List pointer has changed: %d\n", RDS_STATS.large_list);
+
+    return 0;
+}
+
+/* Zero out the stats structure. */
+int
+rds_clear_stats(err)
+     int *err;
+{
+    rvm_return_t rvmret;
+    rvm_tid_t *atid = rvm_malloc_tid();
+
+    rvmret = rvm_begin_transaction(atid, restore);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int)rvmret;
+	rvm_free_tid(atid);
+	return -1;
+    }    
+
+    START_CRITICAL;
+    {
+	rvmret = rvm_set_range(atid, &RDS_STATS, sizeof(rds_stats_t));
+	if (rvmret == RVM_SUCCESS) 
+	    BZERO(&RDS_STATS, sizeof(rds_stats_t));
+    }
+    END_CRITICAL;
+
+    if (rvmret != RVM_SUCCESS) {
+	rvm_abort_transaction(atid);
+	(*err) = (int)rvmret;
+	rvm_free_tid(atid);
+	return -1;
+    }
+    
+    rvmret = rvm_end_transaction(atid, no_flush);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int)rvmret;
+	rvm_free_tid(atid);
+	return -1; /* should I abort here just in case? */
+    }
+	
+    *err = SUCCESS;
+    rvm_free_tid(atid);
+    return 0;
+}
+
+/*
+ * Return a structure initialized from the statistics in the heap header.
+ * Like print_stats, this really doesn't need to be critical -- dcs 1/29
+ */
+
+int rds_get_stats(stats)
+     rds_stats_t *stats;
+{
+    if (stats == NULL)    /* stats structure must be already allocated */
+	return EBAD_ARGS;
+
+    BCOPY(&RDS_STATS, stats, sizeof(rds_stats_t));
+    return 0;
+}
+
+int rds_trace_on(FILE *file)
+{
+  assert(HEAP_INIT);
+  assert(file);
+  rds_tracing = TRUE;
+  rds_tracing_file = file;
+  RDS_LOG("rdstrace: tracing on\n");
+
+  return 0;
+}
+
+
+int
+rds_trace_off ()
+{
+  assert(HEAP_INIT);
+  RDS_LOG("rdstrace: tracing off\n");
+  rds_tracing = FALSE;
+  return 0;
+}
+
+void rds_trace_dump_stats()
+{
+  RDS_LOG("rdstrace: start dump_stats\n");
+  RDS_LOG("rdstrace: Free_bytes \t %x\n", RDS_STATS.freebytes);
+  RDS_LOG("rdstrace: Alloced_bytes\t %x\n",
+		       RDS_STATS.mallocbytes);
+  RDS_LOG("rdstrace: Mallocs \t %d\n", RDS_STATS.malloc);
+  RDS_LOG("rdstrace: Frees \t %d\n",  RDS_STATS.free);
+  RDS_LOG("rdstrace: Preallocs \t %d\n",  RDS_STATS.prealloc);
+  RDS_LOG("rdstrace: Hits \t\t %d\n",  RDS_STATS.hits);
+  RDS_LOG("rdstrace: Misses \t %d\n",  RDS_STATS.misses);
+  RDS_LOG("rdstrace: Large_Hits \t %d\n",  RDS_STATS.large_hits);
+  RDS_LOG("rdstrace: Large_Misses \t %d\n",
+		       RDS_STATS.large_misses);
+  RDS_LOG("rdstrace: Coalesces \t %d\n",  RDS_STATS.coalesce);
+  RDS_LOG("rdstrace: Merges \t %d\n", RDS_STATS.merged);
+  RDS_LOG("rdstrace: Not_Merged \t %d\n", RDS_STATS.unmerged);
+  RDS_LOG("rdstrace: Large_List %d\n", RDS_STATS.large_list);
+  RDS_LOG("rdstrace: stop dump_stats\n");
+}
+
+void rds_trace_dump_free_lists()
+{
+  int i, j;
+  free_block_t *fbp, *ptr;
+  
+  RDS_LOG("rdstrace: start dump_free_lists\n");
+  
+  for (i = 1; i < RDS_NLISTS + 1; i++) {
+
+    fbp = RDS_FREE_LIST[i].head;
+    
+    if (RDS_FREE_LIST[i].guard != FREE_LIST_GUARD)
+      RDS_LOG("rdstrace: Error!!! Bad guard on list %d!!!\n", i);
+    
+    if (fbp && (fbp->prev != (free_block_t *)NULL))
+      RDS_LOG("rdstrace: Error!!! Non-null Initial prev pointer.\n");
+    
+    j = 0;
+    while (fbp != NULL) {
+      j++;
+
+      if (i == RDS_MAXLIST) {
+	RDS_LOG("rdstrace: size %ld count 1\n", fbp->size);
+      }
+      
+      if (fbp->type != FREE_GUARD)
+	RDS_LOG("rdstrace: Error!!! Bad lowguard on block\n");
+      
+      if ((*BLOCK_END(fbp)) != END_GUARD)
+	RDS_LOG("rdstrace: Error!!! Bad highguard, %p=%lx\n",
+			     BLOCK_END(fbp), *BLOCK_END(fbp));
+      
+      ptr = fbp->next;
+      
+      if (ptr && (ptr->prev != fbp))
+	RDS_LOG("rdstrace: Error!!! Bad chain link %p <-> %p\n",
+			     fbp, ptr);
+      
+      if (i != RDS_MAXLIST && fbp->size != i)
+	RDS_LOG("rdstrace: Error!!! OBJECT IS ON WRONG LIST!!!!\n");
+      
+      fbp = fbp->next;
+    }
+    
+    if (i != RDS_MAXLIST)
+      RDS_LOG("rdstrace: size %d count %d\n", i, j);
+  }
+  RDS_LOG("rdstrace: stop dump_free_lists\n");
+}
+
+int rds_trace_dump_heap(void)
+{
+    assert(HEAP_INIT);
+    START_CRITICAL;
+    {
+      RDS_LOG("rdstrace: start heap_dump\n");
+      RDS_LOG("rdstrace: version_string %s\n", RDS_VERSION_STAMP);
+      RDS_LOG("rdstrace: heaplength %ld\n", RDS_HEAPLENGTH);
+      RDS_LOG("rdstrace: chunk_size %ld\n", RDS_CHUNK_SIZE);
+      RDS_LOG("rdstrace: nlists %ld\n", RDS_NLISTS);
+      rds_trace_dump_stats();
+      RDS_LOG("rdstrace: maxlist %ld\n", RDS_MAXLIST);
+      rds_trace_dump_free_lists();
+      RDS_LOG("rdstrace: stop heap_dump\n");
+    }
+    END_CRITICAL;
+    return 0;
+}
diff --git a/rvm/rds_util.c b/rvm/rds_util.c
new file mode 100644
index 0000000..4fe6f4b
--- /dev/null
+++ b/rvm/rds_util.c
@@ -0,0 +1,266 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+
+#include <stdio.h>
+#include "rds_private.h"
+    
+#ifndef ZERO
+#define ZERO 0
+#endif
+
+int
+enqueue(list, block, tid, err)
+     free_list_t  *list;
+     free_block_t *block;
+     rvm_tid_t    *tid;
+     int	  *err;
+{
+    assert(0);
+    return -1;
+}
+
+/*
+ * Dequeue a memory object from the front of a list, return the memory object.
+ */
+
+free_block_t *
+dequeue(list, tid, err)
+     free_list_t  *list;
+     rvm_tid_t    *tid;
+     int	   *err;
+{
+    free_block_t *block = list->head;
+    free_block_t *ptr;
+    rvm_return_t rvmerr;
+    
+    /* Take the block off the head of the list. */
+    rvmerr = rvm_set_range(tid, list, sizeof(free_list_t));
+    if (rvmerr != RVM_SUCCESS) {
+	(*err) = (int) rvmerr;
+	return NULL;
+    }
+    ptr = list->head = block->next;
+
+    if (ptr) {
+	rvmerr = rvm_set_range(tid, &(ptr->prev), sizeof(free_list_t *));
+	if (rvmerr != RVM_SUCCESS) {
+	    (*err) = (int) rvmerr;
+	    return NULL;
+	}
+	ptr->prev = (free_block_t *)NULL;
+    }
+	
+    *err = SUCCESS;
+    return block;
+}
+
+/*
+ * Sometimes we want to remove an object from the middle of a list. This
+ * routine takes a list, a pointer to the previous object, a ptr to the object,
+ * and a transaction ID, the status is passed back in err. NOTE: If the object
+ * is the first one on the list, prev isn't used.
+ */
+int
+rm_from_list(list, bp, tid, err)
+     free_list_t   *list;
+     free_block_t  *bp;
+     rvm_tid_t     *tid;
+     int	    *err;
+{
+    rvm_return_t rvmret;
+    free_block_t *ptr;
+    
+    /* If block is at the head of the list, dequeue will cleanly remove it. */
+    if (bp == list->head) {
+	bp = dequeue(list, tid, err);
+	if (bp == NULL)
+	    return ZERO;
+    } else {
+	/* Because we're not at the end, we know bp->prev is a valid pointer. */
+	ptr = bp->prev;
+	rvmret = rvm_set_range(tid, &(ptr->next), sizeof(free_block_t *));
+	if (rvmret != RVM_SUCCESS) {
+	    (*err) = (int) rvmret;
+	    return ZERO;
+	}
+	ptr->next = bp->next;
+	
+	ptr = bp->next;
+	if (ptr) {
+	    rvmret = rvm_set_range(tid, &(ptr->prev), sizeof(free_block_t *));
+	    if (rvmret != RVM_SUCCESS) {
+		(*err) = (int) rvmret;
+		return ZERO;
+	    }
+	    ptr->prev = bp->prev; /* NOTE: this may be zero */
+	}
+    }
+
+    *err = SUCCESS;
+    return 1;
+}
+
+/* Print out the free list structure */
+int
+print_heap()
+{
+    int i, j;
+    int total_size = 0;
+    free_block_t *fbp, *ptr;
+    
+    if (!HEAP_INIT)	/* Make sure RecoverableHeapStartAddress is inited */
+	return -1;
+
+    START_CRITICAL;
+    {
+	printf("Heap starts at %lx, uses %ld sized chunks, and use %ld of %ld lists\n",
+	       (long)RecoverableHeapStartAddress, RDS_CHUNK_SIZE, 
+	       RDS_MAXLIST, RDS_NLISTS);
+
+	for (i = 1; i < RDS_NLISTS + 1; i++) {
+	    printf("list %d %c\n",i, ((i == RDS_MAXLIST)?'+':' '));
+	    fbp = RDS_FREE_LIST[i].head;
+
+	    if (RDS_FREE_LIST[i].guard != FREE_LIST_GUARD)
+		printf("Bad guard on list %d!!!\n", i);
+
+	    if (fbp && (fbp->prev != (free_block_t *)NULL))
+		printf("Non-null Initial prev pointer.\n");
+	    
+	    j = 1;
+	    while (fbp != NULL) {
+		printf("%d	block %lx, size %ld\n", j++, (long)fbp, 
+		       fbp->size);
+		total_size += fbp->size;
+	    
+		if (fbp->type != FREE_GUARD)
+		    printf("Bad lowguard on block\n");
+		if ((*BLOCK_END(fbp)) != END_GUARD)
+		    printf("Bad highguard, %p=%lx\n", BLOCK_END(fbp), 
+						      *BLOCK_END(fbp));
+		ptr = fbp->next;
+		if (ptr && (ptr->prev != fbp))
+		    printf("Bad chain link %lx <-> %lx\n", (long)fbp, 
+			   (long)ptr);
+		if (i != RDS_MAXLIST && fbp->size != i)
+		    printf("OBJECT IS ON WRONG LIST!!!!\n");
+		    
+		fbp = fbp->next;
+	    }
+	}
+    }
+    END_CRITICAL;
+    
+    printf("Sum of sizes of objects in free lists is %d.\n", total_size);
+    return 0;
+}
+
+/*
+ * Rather than having malloc access the lists directly, it seems wiser to
+ * have intermediary routines to get and put blocks on the list.
+ */
+
+free_block_t *
+get_block(size, tid, err)
+      int       size;
+      rvm_tid_t *tid;
+      int	*err;
+{
+    int list = ((size >= RDS_MAXLIST)?RDS_MAXLIST:size);
+    
+    /* Check the guard on the list. */
+    if (RDS_FREE_LIST[list].guard != FREE_LIST_GUARD) {
+	*err = ECORRUPT;
+	return NULL;
+    }
+    
+    /* Update stats. Don't need a setrange, caller should have done that. */
+    if ((RDS_FREE_LIST[list].head == NULL) ||	   /* For smaller blocks */
+	(RDS_FREE_LIST[list].head->size != size)) { /* In case of large block */
+	/* A block isn't available so we need to split one. */
+	if (list < RDS_MAXLIST)
+	    RDS_STATS.misses++;
+	else
+	    RDS_STATS.large_misses++;
+	
+	return split(size, tid, err);
+    }
+
+    assert(RDS_FREE_LIST[list].head->size == size); /* Sanity check */
+
+    if (list < RDS_MAXLIST)
+	RDS_STATS.hits++;	
+    else
+	RDS_STATS.large_hits++;	
+	
+
+    /* Fbp could be null indicating an error occured in dequeue. Let
+       the calling routine handle this error. */
+    return dequeue(&RDS_FREE_LIST[list], tid, err);
+}
+
+int
+put_block(bp, tid, err)
+     free_block_t *bp;
+     rvm_tid_t    *tid;
+     int	  *err;
+{
+    rvm_return_t rvmerr;
+    int size = (((bp->size) >= RDS_MAXLIST)?RDS_MAXLIST:(bp->size));
+    free_list_t *list = &RDS_FREE_LIST[size];
+    free_block_t *ptr;
+
+    /* Check the guard on the list. */
+    if (list->guard != FREE_LIST_GUARD) {
+	*err = ECORRUPT;
+	return -1;
+    }
+    
+    /* Add the block to the head of the list */
+    rvmerr = rvm_set_range(tid, bp, sizeof(free_block_t));
+    if (rvmerr != RVM_SUCCESS) {
+	(*err) = (int) rvmerr;
+	return -1;
+    }
+    bp->next = list->head;
+    bp->prev = (free_block_t *)NULL;
+
+    /* Make the old head of the list point to the new block. */
+    ptr = bp->next;
+    if (ptr) {
+	rvmerr = rvm_set_range(tid, &(ptr->prev), sizeof(free_block_t *));
+	if (rvmerr != RVM_SUCCESS) {
+	    (*err) = (int) rvmerr;
+	    return -1;
+	}
+	ptr->prev = bp;
+    }
+	
+    /* Make the head point to the freed block */
+    rvmerr = rvm_set_range(tid, list, sizeof(free_list_t));
+    if (rvmerr != RVM_SUCCESS) {
+	(*err) = (int) rvmerr;
+	return -1;
+    }
+    list->head = bp;
+    
+    (*err) = SUCCESS;
+    return 0;
+}
+
diff --git a/rvm/rds_zap.c b/rvm/rds_zap.c
new file mode 100644
index 0000000..55546fa
--- /dev/null
+++ b/rvm/rds_zap.c
@@ -0,0 +1,113 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#include <stdlib.h>
+#include <stdio.h>
+#include "rds_private.h"
+    
+/*
+ * Put the heap in the first, or lower address range, and the statics in the
+ * upper address range.
+ */
+int
+rds_zap_heap(DevName, DevLength, startAddr, staticLength, heapLength, nlists, chunkSize, err)
+     char 		*DevName;
+     rvm_offset_t 	DevLength;
+     char  		*startAddr;
+     rvm_length_t 	staticLength;
+     rvm_length_t	heapLength;
+     unsigned long 	nlists;
+     unsigned long 	chunkSize;
+     int		*err;
+{
+    rvm_region_def_t regions[2], *loadregions = NULL;
+    rvm_tid_t *tid = NULL;
+    unsigned long n_loadregions;
+    rvm_return_t rvmret;
+    
+    memset(regions, 0, 2 * sizeof(rvm_region_def_t));
+    regions[0].length = heapLength;
+    regions[0].vmaddr = startAddr;
+    regions[1].length = staticLength;
+    regions[1].vmaddr = startAddr + heapLength;
+    /* Create an air bubble at the end? */
+    /* Determine the length of the segment, and create a region which makes the
+     * rest of it air. */
+
+    /* Create the segments */
+    rvmret = rvm_create_segment(DevName, DevLength, NULL, 2, regions);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+	return -1;
+    }
+
+    /* Force the writes from create to appear in the data segment. */
+    if ((rvmret = rvm_truncate()) != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+	return -1;
+    }
+    
+    /* Map in the appropriate structures by calling Rvm_Load_Segment. */
+    rvmret = rvm_load_segment(DevName, DevLength, NULL, &n_loadregions, &loadregions);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+        return -1;
+    }
+
+    /* Total sanity checks -- since we just created the segment */
+    if (n_loadregions != 2) {
+	*err = EBAD_SEGMENT_HDR;
+        rvm_release_segment(n_loadregions, &loadregions);
+        return -1;
+    }
+    
+    /* Start a transaction to initialize the heap */
+    tid = rvm_malloc_tid();
+    rvmret = rvm_begin_transaction(tid, restore);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+        rvm_free_tid(tid);
+        rvm_release_segment(n_loadregions, &loadregions);
+        return -1;
+    }
+
+    *err = SUCCESS; 		/* Initialize the error value */
+    rds_init_heap(startAddr, heapLength, chunkSize, nlists, tid, err);
+    if (*err != SUCCESS) {
+	rvm_abort_transaction(tid);
+        rvm_free_tid(tid);
+        rvm_release_segment(n_loadregions, &loadregions);
+        return -1;
+    }
+
+    rvmret = rvm_end_transaction(tid, no_flush);
+    if (rvmret != RVM_SUCCESS) {
+	(*err) = (int) rvmret;
+    }
+
+    rvm_free_tid(tid);
+
+    /* Make sure the initialization has been committed to rvm data */
+    rvm_flush();
+    rvm_truncate();
+
+    rvm_release_segment(n_loadregions, &loadregions);
+
+    return (*err == SUCCESS ? 0 : -1);
+}
+
diff --git a/rvm/rvm.h b/rvm/rvm.h
new file mode 100644
index 0000000..abbd4d2
--- /dev/null
+++ b/rvm/rvm.h
@@ -0,0 +1,476 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*                       Definitions for RVM
+*
+*
+*/
+
+/*LINTLIBRARY*/
+#pragma once
+
+#ifndef __MERO_RVM_RVM_H__
+#define __MERO_RVM_RVM_H__
+
+/* Version string for initialization */
+#define RVM_VERSION         "RVM Interface Version 1.3  7 Mar 1994"
+#define RVM_VERSION_MAX     128         /* 128 char maximum version str length */
+
+/* be sure parallel libraries are used */
+#ifndef _PARALLEL_LIBRARIES
+#define _PARALLEL_LIBRARIES 1
+#endif
+
+/* get timestamp structures and system constants */
+#include <sys/time.h>
+#include <sys/param.h>
+
+/* define bool, TRUE, and FALSE */
+
+#ifndef TRUE
+#define TRUE 1
+#endif
+
+#ifndef FALSE
+#define FALSE 0
+#endif
+
+/* RVM's use of false, true and bool causes trouble with versions of gcc
+   above 2.6 or so; because of this, the names of RVM's definitions
+   have been changed to rvm_{false,true,bool_t}. (Originally changed
+   in rvm.h by Satya (7/31/96); propogated to the rest of the RVM code
+   8/23/96 by tilt */
+
+typedef enum { rvm_false = 0, rvm_true = 1 } rvm_bool_t;
+
+/*  structure identifiers: rvm_struct_id_t
+    codes placed in the first field of each
+    structure instance to identify the object.
+*/
+typedef enum
+    {
+    rvm_first_struct_id = 39,           /* internal use only */
+
+    rvm_region_id,                      /* identifier for rvm_region's */
+    rvm_options_id,                     /* identifier for rvm_options */
+    rvm_tid_id,                         /* identifier for rvm_tid's */
+    rvm_statistics_id,                  /* identifier for rvm_statistics rec's */
+    rvm_last_struct_id                  /* internal use only */
+    }
+rvm_struct_id_t;
+
+/*  Transaction mode codes: rvm_mode_t */
+typedef enum
+    {
+    rvm_first_mode = 139,               /* internal use only */
+
+    restore,                            /* restore memory on abort */
+    no_restore,                         /* do not restore memory on abort */
+    flush,                              /* flush records to logdev on commit */
+    no_flush,                           /* do not flush records on commit */
+
+    rvm_last_mode                       /* internal use only */
+    }
+rvm_mode_t;
+
+/*  Function return codes:  rvm_return_t */
+typedef int  rvm_return_t;
+#define 	RVM_SUCCESS     0  /* success return code */
+#define rvm_first_code  199        /* internal use only */
+#define RVM_EINIT       200        /* RVM not initialized */
+#define RVM_EINTERNAL   201        /* internal error, see rvm_errmsg */
+#define RVM_EIO         202        /* I/O error, see errno */
+#define RVM_ELOG        204        /* invalid log device */
+#define RVM_ELOG_VERSION_SKEW 205  /* RVM log format version skew */
+#define RVM_EMODE 206              /* invalid transaction begin/end mode */
+#define RVM_ENAME_TOO_LONG 207     /* device name longer than 1023 chars */
+#define RVM_ENO_MEMORY 208         /* heap exhausted */
+#define RVM_ENOT_MAPPED  209       /* designated region not mapped */
+#define RVM_EOFFSET 210            /* invalid segment offset */
+#define RVM_EOPTIONS 211           /* invalid options record or pointer */
+#define RVM_EOVERLAP 212           /* region overlaps existing seg mapping */
+#define RVM_EPAGER 213             /* invalid external pager */
+#define RVM_ERANGE 214             /* invalid virtual memory address */
+#define RVM_EREGION 215            /* invalid region descriptor or pointer */
+#define RVM_EREGION_DEF 216        /* invalid region definition descriptor */
+#define RVM_ESRC 217               /* invalid address range for new values */
+#define RVM_ESTATISTICS 218        /* invalid statistics record */
+#define RVM_ESTAT_VERSION_SKEW 219 /* RVM statistics format version skew */
+#define RVM_ETERMINATED 220        /* terminated by error already reported */
+#define RVM_ETHREADS 221           /* illegal C Thread library */
+#define RVM_ETID 222               /* invalid transaction identifier or ptr */
+#define RVM_ETOO_BIG 223           /* internal resouces exceeded */
+#define RVM_EUNCOMMIT 224          /* uncommitted transaction(s) pending */
+#define RVM_EVERSION_SKEW 225      /* RVM library version skew */
+#define RVM_EVM_OVERLAP 226        /* region overlaps existing vm mapping */
+#define rvm_last_code  227         /* internal use only */
+
+/* Enumeration type print name functions */
+extern char *rvm_return(rvm_return_t code);
+extern char *rvm_mode(rvm_mode_t  mode);
+extern char *rvm_type(rvm_struct_id_t id);
+
+
+/*  RVM basic length and offset types:
+    these types are used throughout RVM to hide machine-dependent
+    representations of maximum virtual memory region lengths and
+    64 bit offsets.  Do not use internal types or fields or
+    portability can be compromised
+*/
+/*  region length: rvm_length_t
+    size must be >= sizeof(char *),
+    type must be unsigned arithmetic */
+
+typedef unsigned long rvm_length_t;
+
+/*  region offset descriptor: rvm_offset_t supports 64 bit unsigned integers
+    struct unecessary if machine has 64-bit ops */
+
+typedef struct
+    {                                   /* INTERNAL FIELDS static */
+    rvm_length_t    high;               /* private */
+    rvm_length_t    low;                /* private */
+    }
+rvm_offset_t;
+
+/* construct offset from two rvm_length_t sized quantities x,y
+   -- this will construct an offset from two lengths even if
+   8*sizeof(rvm_length_t) is > sizeof(rvm_offset_t); the "extra"
+   bits, the highest order bits of parameter y, will be discarded */
+#define RVM_MK_OFFSET(x,y)      rvm_mk_offset((rvm_length_t)(x), \
+                                              (rvm_length_t)(y))
+
+/* offset initializer -- same as RVM_MK_OFFSET, but compile time */
+#define RVM_OFFSET_INITIALIZER(x,y)     {(x),(y)}
+
+/* Zero an offset: create a zero offset and assign it to the parameter. */
+#define RVM_ZERO_OFFSET(x)      (x) = RVM_MK_OFFSET(0,0)
+
+/* offset and length conversion macros */
+
+/* return low-order bits of offset x as length
+   -- "low-order bits" are the lowest numerically valued bits
+   of the same size as rvm_length_t */
+#define RVM_OFFSET_TO_LENGTH(x) ((x).low)
+
+/* return high order bits of offset x as length
+   -- "high-order bits" are defined as the highest ordered
+   bits remaining after the low-order bits are extracted
+   they are returned as rvm_length_t */
+#define RVM_OFFSET_HIGH_BITS_TO_LENGTH(x) ((x).high)
+
+/* return length x as rvm_offset_t */
+#define RVM_LENGTH_TO_OFFSET(x) RVM_MK_OFFSET(0,(x))
+
+/* rvm_offset_t and rvm_length_t arithmetic support */
+
+/* add rvm_offset to rvm_offset; returns result (x+y)
+   implemented as function call -- or simple add if
+   machine has 64-bit integer operations */
+#define RVM_ADD_OFFSETS(x,y) \
+    rvm_add_offsets(&(x),&(y))
+
+/* add rvm_length to rvm_offset; return result (length+offset)
+   as rvm_offset_t
+   implemented as function call -- or simple add if
+   machine has 64-bit integer operations */
+#define RVM_ADD_LENGTH_TO_OFFSET(x,y) \
+    rvm_add_length_to_offset(&(x),(y))
+
+/* add rvm_length_t to vm address; returns address (char *)
+   always implemented as simple add */
+#define RVM_ADD_LENGTH_TO_ADDR(length,vmaddr) \
+    ((char *)((rvm_length_t)(length)+(rvm_length_t)(vmaddr)))
+
+/* subtract rvm_offset from rvm_offset; return result (x-y) as rvm_offset_t
+   implemented as function call -- or simple subtract if
+   machine has 64-bit integer operations */
+#define RVM_SUB_OFFSETS(x,y) \
+    rvm_sub_offsets(&(x),&(y))
+
+/* subtract rvm_length from rvm_offset; return result (offset-length)
+   as rvm_offset_t
+   implemented as function call or simple subtract if
+   machine has 64-bit integer operations  */
+#define RVM_SUB_LENGTH_FROM_OFFSET(x,y) \
+    rvm_sub_length_from_offset(&(x),(y))
+
+/* subtract rvm_length_t from vm address; returns address (char *)
+   always implemented as simple subtract */
+#define RVM_SUB_LENGTH_FROM_ADDR(vmaddr,length) \
+    ((char *)((rvm_length_t)(vmaddr)-(rvm_length_t)(length)))
+
+/*  rvm_offset_t comparison macros */
+#define RVM_OFFSET_LSS(x,y)     (((x).high < (y).high) || \
+                                 ((((x).high == (y).high) && \
+                                 ((x).low < (y).low))))
+#define RVM_OFFSET_GTR(x,y)     (((x).high > (y).high) || \
+                                 ((((x).high == (y).high) && \
+                                 ((x).low > (y).low))))
+#define RVM_OFFSET_LEQ(x,y)     (!RVM_OFFSET_GTR((x),(y)))
+#define RVM_OFFSET_GEQ(x,y)     (!RVM_OFFSET_LSS((x),(y)))
+#define RVM_OFFSET_EQL(x,y)     (((x).high == (y).high) && \
+                                 ((x).low == (y).low))
+#define RVM_OFFSET_EQL_ZERO(x)  (((x).high == 0) && ((x).low == 0))
+
+/* page-size rounding macros */
+
+/* return page size as rvm_length_t */
+#define RVM_PAGE_SIZE           rvm_page_size()
+
+/* return rvm_length x rounded up to next integral page-size length */
+#define RVM_ROUND_LENGTH_UP_TO_PAGE_SIZE(x)  ((rvm_length_t)( \
+    ((rvm_length_t)(x)+rvm_page_size()-1) & rvm_page_mask()))
+
+/* return rvm_length x rounded down to integral page-size length */
+#define RVM_ROUND_LENGTH_DOWN_TO_PAGE_SIZE(x)  ((rvm_length_t)( \
+     (rvm_length_t)(x) & rvm_page_mask()))
+
+/* return address x rounded up to next page boundary */
+#define RVM_ROUND_ADDR_UP_TO_PAGE_SIZE(x)  ((char *)( \
+    ((rvm_length_t)(x)+rvm_page_size()-1) & rvm_page_mask()))
+
+/* return address x rounded down to page boundary */
+#define RVM_ROUND_ADDR_DOWN_TO_PAGE_SIZE(x)  ((char *)( \
+    (rvm_length_t)(x) & rvm_page_mask()))
+
+/* return rvm_offset x rounded up to next integral page-size offset */
+#define RVM_ROUND_OFFSET_UP_TO_PAGE_SIZE(x)  \
+    rvm_rnd_offset_up_to_page(&(x))
+
+/* return rvm_offset x rounded down to integral page-size offset */
+#define RVM_ROUND_OFFSET_DOWN_TO_PAGE_SIZE(x)  \
+    rvm_rnd_offset_dn_to_page(&(x))
+
+/* transaction identifier descriptor */
+typedef struct
+    {
+    rvm_struct_id_t struct_id;          /* self-identifier, do not change */
+    rvm_bool_t      from_heap;          /* true if heap allocated;
+                                           do not change */
+    struct timeval  uname;              /* unique name (timestamp) */
+
+    void            *tid;               /* internal use only */
+    rvm_length_t    reserved;           /* internal use only */
+    }
+rvm_tid_t;
+
+/* rvm_tid_t initializer, copier & finalizer */
+
+extern rvm_tid_t    *rvm_malloc_tid ();
+
+extern void rvm_init_tid(rvm_tid_t       *tid); /* pointer to record to initialize */
+extern rvm_tid_t *rvm_copy_tid(rvm_tid_t *tid); /* pointer to record to be copied */
+extern void rvm_free_tid(rvm_tid_t  *tid);      /* pointer to record to be copied */
+
+/*  options descriptor:  rvm_options_t */
+typedef struct
+    {
+    rvm_struct_id_t struct_id;          /* self-identifier, do not change */
+    rvm_bool_t      from_heap;          /* true if heap allocated;
+                                           do not change */
+
+    char            *log_dev;           /* device name */
+    long            truncate;           /* truncation threshold, % of log */
+    rvm_length_t    recovery_buf_len;   /* length of recovery buffer */
+    rvm_length_t    flush_buf_len;      /* length of flush buffer (partitions only) */
+    rvm_length_t    max_read_len;       /* maximum single read length */
+    rvm_bool_t      log_empty;          /* TRUE  ==> log empty */
+    char            *pager;             /* char array for external pager name */
+    long            n_uncommit;         /* length of uncommitted tid array */
+    rvm_tid_t       *tid_array;         /* ptr to array of uncommitted tid's */
+
+    rvm_length_t    flags;              /* bit vector for optimization and
+                                           other flags */
+    rvm_bool_t      create_log_file;    /* TRUE  ==> create the log file */
+    rvm_offset_t    create_log_size;    /* when creating a new log file, this
+                                           is the wanted size */
+    long            create_log_mode;    /* when creating a new log file, this
+                                           is the wanted mode */
+    }
+rvm_options_t;
+
+/* rvm_options default values and other constants */
+
+#define TRUNCATE            50          /* 50% default truncation threshold */
+#define RECOVERY_BUF_LEN    (256*1024)  /* default recovery buffer length */
+#define MIN_RECOVERY_BUF_LEN (64*1024)  /* minimum recovery buffer length */
+#define FLUSH_BUF_LEN       (256*1024)  /* default flush buffer length */
+#define MIN_FLUSH_BUF_LEN    (64*1024)  /* minimum flush buffer length */
+#define MAX_READ_LEN        (512*1024)  /* default maximum single read length */
+
+#define RVM_COALESCE_RANGES 1           /* coalesce adjacent or shadowed
+                                           ranges within a transaction */
+#define RVM_COALESCE_TRANS  2           /* coalesce adjacent or shadowed ranges
+                                           within no_flush transactions */
+
+#define RVM_ALL_OPTIMIZATIONS   (RVM_COALESCE_RANGES | RVM_COALESCE_TRANS)
+
+/* Other flags */
+
+#define RVM_MAP_PRIVATE     8		/* Use private mapping, if available */
+
+/* rvm_options_t initializer, copier & finalizer */
+
+extern rvm_options_t *rvm_malloc_options();
+
+extern void rvm_init_options(rvm_options_t *options);
+extern rvm_options_t *rvm_copy_options(rvm_options_t *options);
+extern void rvm_free_options(rvm_options_t   *options);
+
+/*  region descriptor: rvm_region_t */
+typedef struct
+    {
+    rvm_struct_id_t struct_id;          /* self-identifier, do not change */
+    rvm_bool_t      from_heap;          /* true if heap allocated;
+                                           do not change */
+
+    char            *data_dev;          /* device name */
+    rvm_offset_t    dev_length;         /* maximum device length */
+    rvm_offset_t    offset;             /* offset of region in segment */
+    char            *vmaddr;            /* vm address of region/range */
+    rvm_length_t    length;             /* length of region/range */
+    rvm_bool_t      no_copy;            /* do not copy mapped data if true */
+    }
+rvm_region_t;
+
+/* rvm_region_t allocator, initializer, copier & finalizer */
+extern rvm_region_t *rvm_malloc_region ();
+extern void rvm_init_region(rvm_region_t *region);
+/* note: copier copies pointers to the char arrays */
+extern rvm_region_t *rvm_copy_region(rvm_region_t *region);
+extern void rvm_free_region(rvm_region_t *region);
+
+/*
+        Main Function Declarations
+*/
+
+/* RVM initialization: pass version and optional options
+   descriptor */
+extern rvm_return_t rvm_initialize(const char *version, rvm_options_t *opts);
+/* init macro */
+#define RVM_INIT(options) rvm_initialize(RVM_VERSION,(options))
+
+/* shutdown RVM */
+extern rvm_return_t rvm_terminate (void);   /* no parameters */
+
+/* map recoverable storage */
+extern rvm_return_t rvm_map(
+    rvm_region_t         *region,       /* pointer to region descriptor */
+    rvm_options_t        *options       /* optional ptr to option descriptor */
+    );
+
+/* unmap recoverable storage */
+extern rvm_return_t rvm_unmap(rvm_region_t *region);
+
+/* set RVM options */
+extern rvm_return_t rvm_set_options(rvm_options_t *options);
+
+/* query RVM options */
+extern rvm_return_t rvm_query(
+    rvm_options_t       *options,       /* address of pointer to option
+                                           descriptor [out] */
+    rvm_region_t        *region         /* optional pointer to region descriptor */
+    );
+
+/* begin a transaction */
+extern rvm_return_t rvm_begin_transaction(
+    rvm_tid_t           *tid,           /* pointer to transaction identifier */
+    rvm_mode_t          mode            /* transaction begin mode */
+    );
+
+/* declare a modification region for a transaction */
+extern rvm_return_t rvm_set_range(
+    rvm_tid_t           *tid,           /* pointer to transaction identifier */
+    void                *dest,          /* base address of modification range */
+    rvm_length_t        length          /* length of modification range */
+    );
+
+/* modification of a region for a transaction */
+extern rvm_return_t rvm_modify_bytes(
+    rvm_tid_t           *tid,           /* pointer to transaction identifier */
+    void                *dest,          /* base address of modification range */
+    const void          *src,           /* base address of source range */
+    rvm_length_t        length          /* length of modification range */
+    );
+
+/* abort a transaction */
+extern rvm_return_t rvm_abort_transaction(
+    rvm_tid_t           *tid            /* pointer to transaction identifier */
+    );
+
+/* commit a transaction */
+extern rvm_return_t rvm_end_transaction(
+    rvm_tid_t           *tid,           /* pointer to transaction identifier */
+    rvm_mode_t          mode            /* transaction commit mode */
+    );
+
+/* flush log cache buffer to log device */
+extern rvm_return_t rvm_flush(); /* no parameters */
+
+/* apply logged changes to segments and garbage collect the log device */
+extern rvm_return_t rvm_truncate(); /* no parameters */
+
+/* initialize log */
+extern rvm_return_t rvm_create_log(
+    rvm_options_t   *rvm_options,       /* ptr to options record */
+    rvm_offset_t    *log_len,           /* length of log data area */
+    long            mode                /* file creation protection mode */
+    );
+
+/* underlying support functions for length, offset, and rounding macros
+
+   *** use outside of the macros can compromise portability ***
+
+   these functions will not be implemented on machines with 64-bit
+   integer formats since their operations will be available in the
+   native instruction set
+*/
+extern rvm_offset_t rvm_mk_offset(
+    rvm_length_t        x,
+    rvm_length_t        y
+    );
+extern rvm_offset_t rvm_add_offsets(
+    rvm_offset_t        *x,
+    rvm_offset_t        *y
+);
+extern rvm_offset_t rvm_add_length_to_offset(
+    rvm_offset_t        *offset,
+    rvm_length_t        length
+    );
+extern rvm_offset_t rvm_sub_offsets(
+    rvm_offset_t        *x,
+    rvm_offset_t        *y
+    );
+extern rvm_offset_t rvm_sub_length_from_offset(
+    rvm_offset_t        *offset,
+    rvm_length_t        length
+    );
+
+// TODO
+typedef struct heap_header heap_header_t;
+
+/* private functions to support page rounding */
+
+extern rvm_length_t rvm_page_size ();
+extern rvm_length_t rvm_page_mask ();
+extern rvm_offset_t rvm_rnd_offset_up_to_page(rvm_offset_t *x);
+extern rvm_offset_t rvm_rnd_offset_dn_to_page(rvm_offset_t *x);
+
+#endif /* __MERO_RVM_RVM_H__ */
diff --git a/rvm/rvm_createseg.c b/rvm/rvm_createseg.c
new file mode 100644
index 0000000..1263d64
--- /dev/null
+++ b/rvm/rvm_createseg.c
@@ -0,0 +1,138 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#include <stdio.h>
+#include <string.h>
+#include <assert.h>
+
+#include "rvm/rvm.h"
+#include "rvm/rvm_segment.h"
+#include "rvm/rvm_segment_private.h"
+
+/* rvm_create_segment erases the old contents of the recoverable
+ * segment, and write a new structure to it. The arguments specify the
+ * number of regions, and the form of each region in the new segment
+ * structure. It is important to realize that all information that
+ * used to exist in the segment will no longer be accessible.  */
+
+rvm_return_t
+rvm_create_segment(DevName, DevLength, options, nregions, regionDefs)
+     char 	    	*DevName;
+     rvm_offset_t	DevLength;
+     rvm_options_t  	*options;
+     rvm_length_t	nregions;
+     rvm_region_def_t   regionDefs[];
+{
+    rvm_region_t *region = rvm_malloc_region();
+    rvm_segment_hdr_t *hptr;
+    rvm_offset_t offset;
+    rvm_tid_t *tid;
+    rvm_return_t err;
+    int i;
+
+    /* Make sure the region definitions do not overlap. */
+    if (overlap(nregions, regionDefs))
+	return RVM_ERANGE;
+
+    assert( nregions <= RVM_MAX_REGIONS );
+
+    /* Erase the old contents of the segment, including entries in the log */
+
+    /* Map in the first RVM_SEGMENT_HDR_SIZE bytes of the segment */
+
+    region->data_dev = DevName;
+    region->dev_length = DevLength;
+    RVM_ZERO_OFFSET(region->offset);
+    region->length = RVM_SEGMENT_HDR_SIZE;
+    region->vmaddr = 0;
+
+    /* allocate the address range for this region */
+    err = allocate_vm(&(region->vmaddr), region->length);
+    if (err != RVM_SUCCESS) {
+	rvm_free_region(region);
+	return err;
+    }
+
+    err = rvm_map(region, options);
+    if (err != RVM_SUCCESS) {
+	rvm_free_region(region);
+	return err; 	/* Some error condition exists, return the error code */
+    }
+
+    tid = rvm_malloc_tid();
+    err = rvm_begin_transaction(tid, restore);
+    if (err != RVM_SUCCESS) {
+	rvm_free_tid(tid);
+	rvm_free_region(region);
+	return err;
+    }
+
+    /* Set up the header region. This is always a fixed size. */
+    hptr = (rvm_segment_hdr_t *) region->vmaddr;
+
+    err = rvm_set_range(tid, (char *)hptr, RVM_SEGMENT_HDR_SIZE);
+    if (err != RVM_SUCCESS) {
+	rvm_abort_transaction(tid);
+	rvm_free_tid(tid);
+	rvm_free_region(region);
+	return err;
+    }
+
+    hptr->struct_id = rvm_segment_hdr_id;
+    strcpy(hptr->version, RVM_SEGMENT_VERSION);
+    hptr->nregions = nregions;
+
+    /* First region goes right after segment header */
+    RVM_ZERO_OFFSET(offset);
+    offset = RVM_ADD_LENGTH_TO_OFFSET(offset, RVM_SEGMENT_HDR_SIZE);
+
+    /* For each region definition, set it to start at the next available spot
+     * in the segment, fill in the length and vmaddr fields, and
+     * determine the next available spot in the segment.
+     */
+
+    /* XXXXXX this needs a check to bound the number of regions */
+
+    for (i = 0; i < nregions; i++) {
+	hptr->regions[i].offset = offset;
+	hptr->regions[i].length = regionDefs[i].length;
+	hptr->regions[i].vmaddr = regionDefs[i].vmaddr;
+	/* printf("Creating region at offset %x,%x , vmaddr %x, len %d\n",
+	       hptr->regions[i].offset.high, hptr->regions[i].offset.low,
+	       hptr->regions[i].vmaddr, hptr->regions[i].length); */
+	offset = RVM_ADD_LENGTH_TO_OFFSET(offset, regionDefs[i].length);
+    }
+
+    err = rvm_end_transaction(tid, flush);
+    rvm_free_tid(tid);
+    if (err != RVM_SUCCESS) {
+	rvm_free_region(region);
+	return err;
+    }
+
+    /* The segment should now be all set to go, clean up. */
+    err = rvm_unmap(region);
+    if (err != RVM_SUCCESS)
+	printf("create_segment unmap failed %s\n", rvm_return(err));
+
+    deallocate_vm(region->vmaddr, region->length);
+
+    rvm_free_region(region);
+    return err;
+}
+
diff --git a/rvm/rvm_debug.c b/rvm/rvm_debug.c
new file mode 100644
index 0000000..8823abe
--- /dev/null
+++ b/rvm/rvm_debug.c
@@ -0,0 +1,1242 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                   RVM internal structure debugging functions
+*
+*/
+
+#include <sys/types.h>
+#include <sys/uio.h>
+#include <stdio.h>
+#include "rvm/rvm_private.h"
+
+/* globals */
+
+extern rvm_length_t     page_size;
+extern rvm_length_t     page_mask;
+extern rvm_bool_t       rvm_no_log;
+extern char             *rvm_errmsg; /* internal error message buffer */
+
+/* roots of memory structures */
+
+/* structure cache */
+extern long             type_counts[NUM_CACHE_TYPES];
+extern list_entry_t     free_lists[NUM_CACHE_TYPES];
+extern long             pre_alloc[NUM_CACHE_TYPES];
+extern long             max_alloc[NUM_CACHE_TYPES];
+extern long             cache_type_sizes[NUM_CACHE_TYPES];
+
+/* main structures roots */
+extern list_entry_t     seg_root;       /* segment list */
+extern tree_root_t      region_tree;   /* mapped regions tree */
+extern list_entry_t     page_list;      /* free page list */
+extern list_entry_t     log_root;       /* log list */
+
+/* locals */
+
+/* structure names & sizes for debug support */
+static char             *type_names[NUM_TYPES]
+                            = {TYPE_NAMES};
+static rvm_length_t     type_sizes[NUM_TYPES]
+                            = {CACHE_TYPE_SIZES,OTHER_TYPE_SIZES};
+#define SIZE(id)        (type_sizes[ID_INDEX(id)])
+#define NAME(id)        (type_names[ID_INDEX(id)])
+/* address is in a structure */
+#define IN_STRUCT(x,s,id) \
+                        (((x) >= (rvm_length_t)(s)) && \
+                         ((x) < (((rvm_length_t)(s))+SIZE(id))))
+
+/* address validation: must be on rvm_length_t size boundary */
+#define ADDR_INVALID(x) ((rvm_length_t)(x) != CHOP_TO_LENGTH((x)))
+
+#define ADDR_INVALID_OR_NULL(x) \
+                        (ADDR_INVALID(x) || ((x) == NULL))
+
+/* empty routine to force loading of this module when referenced by a program
+   can also be used as a break point when a condition must be calculated */
+void rvm_debug(val)
+    rvm_length_t       val;
+    {
+    if (val != 0)
+        printf("\nAt rvm_debug: %ld (%lx)\n",val,val);
+    }
+/* power of 2 table -- must be extended for machines with address
+     spaces greater than 32 bits */
+#define NUM_TWOS        30
+static rvm_length_t     twos[NUM_TWOS] =
+                            {
+                            1<<3,1<<4,1<<5,1<<6,1<<7,1<<8,1<<9,1<<10,
+                            1<<11,1<<12,1<<13,1<<14,1<<15,1<<16,1<<17,
+                            1<<18,1<<19,1<<20,1<<21,1<<22,1<<23,1<<24,
+                            1<<25,1<<26,1<<27,1<<28,1<<29,1<<30,1<<31,
+                            -1
+                            };
+/* test ifaddress is in heap-allocated space */
+rvm_bool_t in_heap(addr,buf,len)
+    rvm_length_t        addr;           /* address to search for */
+    rvm_length_t        buf;            /* buffer to search */
+    rvm_length_t        len;            /* requested length of buffer */
+    {
+    long                i;
+
+    if (buf == 0) return rvm_false;     /* skip null buffers */
+
+    len += sizeof(rvm_length_t);        /* compensate for malloc back ptr */
+    buf -= sizeof(rvm_length_t);
+    for (i=0; i<NUM_TWOS; i++)
+        if ((len >= twos[i]) && (len < twos[i+1]))
+            break;
+    assert(i != NUM_TWOS);
+
+    if ((addr >= buf) && (addr < (buf+twos[i])))
+        return rvm_true;
+
+    return rvm_false;
+    }
+/* list checker -- makes sure elements of list are valid */
+rvm_bool_t chk_list(hdr,silent)
+    list_entry_t        *hdr;           /* header of list to check */
+    rvm_bool_t          silent;         /* print only errors if true */
+    {
+    list_entry_t        *entry;         /* current list entry */
+    list_entry_t        *prev;          /* previous list entry */
+    long                i = 0;
+    rvm_bool_t          retval = rvm_true;
+
+    if (hdr == NULL)
+        {
+        printf("  List header is null\n");
+        return rvm_false;
+        }
+    if (ADDR_INVALID(hdr))
+        {
+        printf("  List header address invalid, hdr = %lx\n",(long)hdr);
+        return rvm_false;
+        }
+    if (hdr->is_hdr != rvm_true)
+        {
+        printf("  List header is not valid, is_hdr = %ld\n",
+               (long)hdr->is_hdr);
+        return rvm_false;
+        }
+    if (!(((long)hdr->struct_id > (long)struct_first_id) &&
+           ((long)hdr->struct_id < (long)struct_last_id)))
+        {
+        printf("  List header type is not valid, struct_id = %ld\n",
+               (long)hdr->struct_id);
+        return rvm_false;
+        }
+    if (hdr->list.length < 0)
+        printf("  List length invalid, length = %ld\n",hdr->list.length);
+    if (ADDR_INVALID_OR_NULL(hdr->nextentry))
+        {
+        printf("  List header at %lx has invalid nextentry field, ",(long)hdr);
+	printf("hdr->nextentry = %lx\n",(long)hdr->nextentry);
+        return rvm_false;
+        }
+    if (ADDR_INVALID_OR_NULL(hdr->preventry))
+        {
+        printf("  List header at %lx has invalid preventry field, ",(long)hdr);
+	printf("hdr->preventry = %lx\n",(long)hdr->nextentry);
+        return rvm_false;
+        }
+    if ((hdr->nextentry == hdr->preventry) && (hdr->nextentry == hdr))
+        {
+        if (!silent)
+            printf("  List empty\n");
+        if (hdr->list.length != 0)
+            {
+            printf("  List length invalid, length = %ld\n",
+                   hdr->list.length);
+            return rvm_false;
+            }
+        return rvm_true;
+        }
+    if (!silent)
+        printf("  List length = %ld\n",hdr->list.length);
+    /* check ptrs */
+    if (ADDR_INVALID_OR_NULL(hdr->nextentry))
+        {
+        printf("  List header at %lx has invalid nextentry field, ",(long)hdr);
+	printf("hdr->nextentry = %lx\n",(long)hdr->nextentry);
+        return rvm_false;
+        }
+
+    /* check all elements of list */
+    prev = hdr;
+    entry = hdr->nextentry;
+    while (entry->is_hdr != rvm_true)
+        {
+        i++;
+        if (hdr->struct_id != entry->struct_id)
+            {
+            printf("  List entry %ld (%lx) has wrong type, struct_id = %ld, ",
+		   i, (long)entry, (long)entry->struct_id);
+	    printf("hdr->struct_iud = %ld\n",(long)hdr->struct_id);
+            retval = rvm_false;
+            }
+        if (entry->list.name != hdr)
+            {
+            printf("  List entry %ld (%lx) does not point to header, name = %lx\n",
+                   i,(long)entry,(long)entry->list.name);
+            retval = rvm_false;
+            }
+        if (entry->preventry != prev)
+            {
+            printf("  List entry %ld (%lx)does not have correct preventry,",
+		   i, (long)entry);
+	    printf(" preventry = %lx\n",(long)entry->preventry);
+            retval = rvm_false;
+            }
+        if (ADDR_INVALID_OR_NULL(entry->nextentry))
+            {
+            printf("  List entry %ld (%lx) has invalid nextentry field, ",
+		   i,(long)entry);
+	    printf("nextentry = %lx\n",(long)entry->nextentry);
+            return rvm_false;
+            }
+        prev = entry;
+        entry = entry->nextentry;
+        }
+    /* check results */
+    if (i != hdr->list.length)
+        {
+        printf("  List length wrong, length = %ld, actual length = %ld\n",
+               hdr->list.length,i);
+        retval = rvm_false;
+        }
+    if (ADDR_INVALID_OR_NULL(hdr->preventry))
+        {
+        printf("  List header at %lx has invalid preventry field, ",(long)hdr);
+	printf("hdr->preventry = %lx\n",(long)hdr->nextentry);
+        retval = rvm_false;
+        }
+
+    if ((retval) && (!silent))
+        printf("  List is OK\n");
+
+    return retval;
+    }
+/* structure cache free list checker */
+rvm_bool_t chk_free_list(struct_id)
+    struct_id_t     struct_id;          /* type of free list to check */
+    {
+    if (!(((long)struct_id > (long)struct_first_id) &&
+           ((long)struct_id < (long)struct_last_cache_id)))
+        {
+        printf("This structure is not cached\n");
+        return rvm_false;
+        }
+
+    return chk_list(&free_lists[ID_INDEX(struct_id)],rvm_true);
+    }
+
+/* check all free lists */
+void chk_all_free_lists()
+    {
+    long            i;
+
+    for (i=ID_INDEX(log_id);i < ID_INDEX(struct_last_cache_id); i++)
+        {
+        printf("Checking free list for %s\n",type_names[i]);
+        chk_free_list(INDEX_ID(i));
+        }
+    }
+/* locate an address in simple list */
+rvm_bool_t search_list(hdr,struct_id,addr)
+    list_entry_t    *hdr;               /* header of list to search */
+    struct_id_t     struct_id;          /* type of list to search */
+    rvm_length_t    addr;               /* address to search for */
+    {
+    list_entry_t    *entry;             /* current list entry */
+    long            i = 0;
+    rvm_bool_t      pr_hdr = rvm_true;
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if in header */
+    if (hdr == NULL)
+        return rvm_false;
+    if ((addr >= (rvm_length_t)hdr)
+        && (addr < ((rvm_length_t)hdr+addr)))
+        {
+        printf("  Address contained in %s list header at %lx\n",
+               NAME(struct_id),(long)hdr);
+        retval = rvm_true;
+        }
+
+    /* search the list */
+    entry = hdr->nextentry;
+    while (!entry->is_hdr)
+        {
+        i++;
+        if ((addr >= (rvm_length_t)entry)
+        && (addr < ((rvm_length_t)entry+SIZE(struct_id))))
+            {
+            if (pr_hdr)
+                {
+                printf("  Address contained in %s list at %lx\n",
+                       NAME(struct_id),(long)hdr);
+                pr_hdr = rvm_false;
+                }
+            printf("   in entry %ld at %lx\n",i,(long)entry);
+            retval = rvm_true;
+            }
+        entry = entry->nextentry;
+        }
+    return retval;
+    }
+/* locate an address in free page list */
+rvm_bool_t in_free_page_list(addr)
+    rvm_length_t    addr;               /* address to search for */
+    {
+    free_page_t     *pg;
+    rvm_bool_t      retval = rvm_false;
+
+    /* sanity check the list structure */
+    printf("Searching free page list\n");
+    if (!chk_list(&page_list,rvm_true))
+        return rvm_false;
+
+    /* search the pages */
+    FOR_ENTRIES_OF(page_list,free_page_t,pg)
+        {
+        if ((addr >= (rvm_length_t)pg)
+            && (addr < ((rvm_length_t)pg+pg->len)))
+            {
+            printf("  Address contained in free page entry at %lx\n",
+                   (long)pg);
+            retval = rvm_true;
+            }
+        }
+
+    return retval;
+    }
+/* locate an address in free list */
+rvm_bool_t in_free_list(struct_id,addr)
+    struct_id_t     struct_id;          /* type of free list to search */
+    rvm_length_t    addr;               /* address to search for */
+    {
+
+    /* check basic list structure */
+    if (!chk_list(&free_lists[ID_INDEX(struct_id)],rvm_true))
+        return rvm_false;
+
+    /* see if addr is in any element of list */
+    return search_list(&free_lists[ID_INDEX(struct_id)],struct_id,addr);
+    }
+
+/* locate an address in free lists (searches all) */
+rvm_bool_t in_free_lists(addr)
+    rvm_length_t    addr;               /* address to search for */
+    {
+    rvm_bool_t      retval = rvm_false;
+    long            i;
+
+    for (i=ID_INDEX(log_id);i < ID_INDEX(struct_last_cache_id); i++)
+        {
+        printf("Searching free list %s\n",type_names[i]);
+        if (in_free_list(INDEX_ID(i),addr))
+            retval = rvm_true;
+        }
+
+    return retval;
+    }
+/* mem_region_t tree node checks */
+rvm_bool_t chk_mem_node(node)
+    mem_region_t    *node;
+    {
+    region_t        *region;
+    seg_t           *seg;
+    rvm_bool_t      retval = rvm_true;
+
+    /* basic memory region node checks */
+    if (ADDR_INVALID_OR_NULL(node->region))
+        {
+        printf("  Region ptr is invalid, node->object = %lx\n",
+               (long)node->region);
+        return rvm_false;
+        }
+    region = node->region;
+    if (region->links.struct_id != region_id)
+        {
+        printf("  Mem_region node at %lx does not point to",(long)node);
+	printf(" region descriptor\n");
+        return rvm_false;
+        }
+    if (ADDR_INVALID_OR_NULL(region->mem_region)
+        || ((mem_region_t *)region->mem_region != node))
+        {
+        printf("  Region descriptor at %lx does not point back to",
+	       (long)region);
+	printf(" mem_region node at %lx\n",(long)node);
+        return rvm_false;
+        }
+    if (ADDR_INVALID_OR_NULL(region->seg))
+        {
+        printf("  Mem_region node at %lx region descriptor has invalid",
+	       (long)node);
+	printf(" segment ptr, ptr = %lx\n",(long)region->seg);
+        return rvm_false;
+        }
+    if (region->seg->links.struct_id != seg_id)
+        {
+        printf("  Mem_region node at %lx region descriptor has invalid",
+	       (long)node);
+	printf(" segment descriptor, seg = %lx\n",(long)region->seg);
+        return rvm_false;
+        }
+    /* related structure and list checks */
+    if (!chk_list(&seg_root,rvm_true))
+        return rvm_false;
+    FOR_ENTRIES_OF(seg_root,seg_t,seg)
+        if (seg == region->seg) break;
+    if ((list_entry_t *)seg == &seg_root)
+        {
+        printf("  Mem_region node at %lx region descriptor's segment",
+	       (long)region);
+	printf(" descriptor is not on seg_root list, seg = %lx\n",
+               (long)region->seg);
+        retval = rvm_false;
+        }
+
+    seg = region->seg;
+    if (!chk_list(&seg->map_list,rvm_true))
+        {
+        printf("  Mem_region's region's segment's map_list is damaged,");
+	printf(" seg = %lx\n",(long)seg);
+        return rvm_false;
+        }
+    FOR_ENTRIES_OF(seg->map_list,region_t,region)
+        if (region == node->region) break;
+    if (region != node->region)
+        {
+        printf("  Mem_region node at %lx region descriptor is",(long)node);
+	printf(" not on its segment's map_list, region = %lx\n",
+               (long)node->region);
+        return rvm_false;
+        }
+    region = node->region;
+    if (region->links.struct_id != region_id)
+        {
+        printf("  Mem_region node at %lx does not point to",(long)node);
+	printf(" region descriptor\n");
+        return rvm_false;
+        }
+    if (ADDR_INVALID_OR_NULL(region->mem_region)
+        || ((mem_region_t *)region->mem_region != node))
+        {
+        printf("  Region descriptor at %lx does not point back to",
+	       (long)region);
+	printf(" mem_region node at %lx\n",(long)node);
+        return rvm_false;
+        }
+    if (ADDR_INVALID_OR_NULL(region->seg))
+        {
+        printf("  Mem_region node at %lx region descriptor has invalid",
+	       (long)node);
+	printf(" segment ptr, ptr = %lx\n",(long)region->seg);
+        return rvm_false;
+        }
+    if (region->seg->links.struct_id != seg_id)
+        {
+        printf("  Mem_region node at %lx region descriptor has invalid",
+	       (long)node);
+	printf(" segment descriptor, seg = %lx\n",(long)region->seg);
+        return rvm_false;
+        }
+    /* related structure and list checks */
+    if (!chk_list(&seg_root,rvm_true))
+        return rvm_false;
+    FOR_ENTRIES_OF(seg_root,seg_t,seg)
+        if (seg == region->seg) break;
+    if ((list_entry_t *)seg == &seg_root)
+        {
+        printf("  Mem_region node at %lx region descriptor's segment",
+	       (long)region);
+	printf(" descriptor is not on seg_root list, seg = %lx\n",
+               (long)region->seg);
+        retval = rvm_false;
+        }
+
+    seg = region->seg;
+    if (!chk_list(&seg->map_list,rvm_true))
+        {
+        printf("  Mem_region's region's segment's map_list is damaged,");
+	printf(" seg = %lx\n",(long)seg);
+        return rvm_false;
+        }
+    FOR_ENTRIES_OF(seg->map_list,region_t,region)
+        if (region == node->region) break;
+    if (region != node->region)
+        {
+        printf("  Mem_region node at %lx region descriptor is",(long)node);
+	printf(" not on its segment's map_list, region = %lx\n",
+	       (long)node->region);
+        retval = rvm_false;
+        }
+
+    if (!chk_list(&seg->unmap_list,rvm_true))
+        {
+        printf("  Mem_region's region's segment's unmap_list is damaged,");
+	printf(" seg = %lx\n",(long)seg);
+        return rvm_false;
+        }
+    FOR_ENTRIES_OF(seg->unmap_list,region_t,region)
+        if (region == node->region)
+            {
+            printf("  Mem_region node at %lx region descriptor is",
+		   (long)node);
+	    printf(" on its segment's unmap_list, region = %lx\n",
+		   (long)region);
+            retval = rvm_false; break;
+            }
+
+    return retval;
+    }
+/* validate dev_region node */
+rvm_bool_t chk_dev_node(dev_region_t *node)
+{
+    rvm_bool_t      retval = rvm_true;
+    /* check validity of nv buffer ptrs */
+    if (!((node->nv_ptr == NULL) && (node->nv_buf == NULL)))
+        {
+        if (ADDR_INVALID_OR_NULL(node->nv_ptr))
+            {
+            printf("  Dev_region node at %lx has bad nv_ptr\n",(long)node);
+            retval = rvm_false;
+            }
+        if (ADDR_INVALID(node->nv_buf))
+            {
+            printf("  Dev_region node at %lx has bad nv_buf\n",(long)node);
+            retval = rvm_false;
+            }
+        }
+
+    /* check consistency of nv buffer vs. log offset */
+    if (!((node->nv_ptr != NULL)
+          && RVM_OFFSET_EQL_ZERO(node->log_offset))
+        || ((!RVM_OFFSET_EQL_ZERO(node->log_offset))
+            && (node->nv_ptr == NULL)))
+        {
+        printf("  Dev_region node at %lx has inconsistent nv_ptr",(long)node);
+	printf(" & log_offset\n");
+        retval = rvm_false;
+        }
+
+    return retval;
+}
+/* check validity of tree node */
+rvm_bool_t chk_node(tree_node_t *node, struct_id_t struct_id)
+{
+    rvm_bool_t      retval = rvm_true;
+
+    /* basic structure checks */
+    if (node->struct_id != struct_id)
+        {
+        printf("  Node at %lx has wrong struct_id, id = %d, should be %ld\'n",
+               (long)node,node->struct_id,(long)struct_id);
+        retval = rvm_false;
+        }
+    if (node->gtr != NULL)
+        if (ADDR_INVALID(node->gtr) || (node->gtr->struct_id != struct_id))
+            {
+            printf("  Node at %lx gtr ptr invalid\n",(long)node);
+            retval = rvm_false;
+            }
+    if (node->lss != NULL)
+        if (ADDR_INVALID(node->lss) || (node->lss->struct_id != struct_id))
+            {
+            printf("  Node at %lx lss ptr invalid\n",(long)node);
+            retval = rvm_false;
+            }
+
+    /* type-specific checks */
+    switch (struct_id)
+        {
+      case mem_region_id:
+        retval = chk_mem_node((mem_region_t *)node) && retval;
+        break;
+      case dev_region_id:
+        retval = chk_dev_node((dev_region_t *)node) && retval;
+        break;
+      default:      assert(rvm_false);
+        }
+
+    return retval;
+}
+/* search mem_region tree node */
+rvm_bool_t search_mem_region(addr,node)
+    rvm_length_t    addr;               /* address to search for */
+    mem_region_t    *node;              /* mem_region node to search */
+    {
+    rvm_bool_t      retval = rvm_false;
+
+    /* check tree node */
+    if (!chk_node((tree_node_t *)node,mem_region_id))
+        return rvm_false;
+
+    /* see if address is in node */
+    if ((addr >= (rvm_length_t)node) &&
+        (addr < ((rvm_length_t)node+SIZE(mem_region_id))))
+        {
+        printf("  ***  Address is in mem_region node at %lx\n",(long)node);
+        retval = rvm_true;
+        }
+
+    /* see if address is in node's vm */
+    if ((addr >= (rvm_length_t)node->vmaddr) &&
+        (addr < ((rvm_length_t)node->vmaddr+node->length)))
+        {
+        printf("  ***  Address is in vm represented by mem_region node at %lx\n"
+               ,(long)node);
+        retval = rvm_true;
+        }
+
+    /* check lower branches */
+    if (node->links.node.lss != NULL)
+        if (search_mem_region(addr,(mem_region_t *)node->links.node.lss))
+            retval = rvm_true;
+    if (node->links.node.gtr != NULL)
+        if (search_mem_region(addr,(mem_region_t *)node->links.node.gtr))
+            retval = rvm_true;
+
+    return retval;
+    }
+
+/* locate an address in region_tree */
+rvm_bool_t in_region_tree(addr)
+    rvm_length_t    addr;               /* address to search for */
+    {
+
+    printf("Searching mapped region tree\n");
+
+    return search_mem_region(addr,(mem_region_t *)&region_tree);
+    }
+/* */
+rvm_bool_t search_dev_region(addr,node)
+    rvm_length_t    addr;               /* address to search for */
+    dev_region_t    *node;              /* segment region node to search */
+    {
+    rvm_bool_t      retval = rvm_false;
+
+    /* basic node checks */
+    if (!chk_node((tree_node_t *)node,dev_region_id))
+        return rvm_false;
+
+    /* see if addr is in node */
+    if (IN_STRUCT(addr,node,dev_region_id))
+        {
+        printf("  ***  Address is in dev_region node at %lx\n",
+               (long)node);
+        retval = rvm_true;
+        }
+
+    /* see if address is in node's nv buffer */
+    if (node->nv_ptr != NULL)
+        if (in_heap(addr,(rvm_length_t)node->nv_buf,
+                    node->nv_buf->alloc_len))
+            {
+            printf("  ***  Address is in dev_region at %lx nv buffer\n",
+		   (long)node);
+            retval = rvm_true;
+            }
+
+    /* check lower nodes */
+    if (node->links.node.lss != NULL)
+        if (search_dev_region(addr,(dev_region_t *)node->links.node.lss))
+            retval = rvm_true;
+    if (node->links.node.gtr != NULL)
+        if (search_dev_region(addr,(dev_region_t *)node->links.node.gtr))
+            retval = rvm_true;
+
+    return retval;
+    }
+/* search region descriptor */
+rvm_bool_t in_region(addr,region,n)
+    rvm_length_t    addr;               /* address to search for */
+    region_t        *region;            /* region descriptor to search */
+    long            n;
+    {
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if in region descriptor */
+    printf("    Searching region %ld\n",n);
+    if (IN_STRUCT(addr,region,region_id))
+        {
+        printf("  ***  Address is in region descriptor at %lx\n",
+	       (long)region);
+        retval = rvm_true;
+        }
+
+    return retval;
+    }
+/* search segment descriptor */
+rvm_bool_t in_seg(addr,seg,n)
+    rvm_length_t    addr;               /* address to search for */
+    seg_t           *seg;               /* segment descriptor to search */
+    long            n;
+    {
+    region_t        *region,*region2;
+    long            i = 0;
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if address is in descriptor */
+    printf("  Searching segment %ld\n",n);
+    if (IN_STRUCT(addr,seg,seg_id))
+        {
+        printf("  ***  Address is in segment descriptor at %lx\n",
+	       (long)seg);
+        retval = rvm_true;
+        }
+
+    /* see if in device name */
+    if (ADDR_INVALID_OR_NULL(seg->dev.name))
+        printf("  Segment descriptor at %lx has bad dev.name\n",
+	       (long)seg);
+    else
+        if (in_heap(addr,(rvm_length_t)seg->dev.name,seg->dev.name_len))
+            {
+            printf("  ***  Address is in segment at %lx device name\n",
+		   (long)seg);
+            retval = rvm_true;
+            }
+
+    /* validate and scan mapping lists */
+    if (!chk_list(&seg->map_list,rvm_true))
+        {
+        printf("  Segment descriptor at %lx has bad map list\n",
+	       (long)seg);
+        return retval;
+        }
+    if (!chk_list(&seg->unmap_list,rvm_true))
+        {
+        printf("  Segment descriptor at %lx has bad unmap list\n",
+	       (long)seg);
+        return retval;
+        }
+    FOR_ENTRIES_OF(seg->map_list,region_t,region)
+        {
+        i++;
+        if (in_region(addr,region,i))
+            {
+            printf("  ***  Address is in region descriptor at %lx\n",
+		   (long)region);
+            retval = rvm_true;
+            }
+        FOR_ENTRIES_OF(seg->unmap_list,region_t,region2)
+            if (region == region2)
+                {
+                printf("  Region descriptor at %lx is on both map and unmap",
+		       (long)region);
+		printf(" lists of segment descriptor at %lx\n",
+		       (long)seg);
+                break;
+                }
+        }
+    i = 0;
+    FOR_ENTRIES_OF(seg->unmap_list,region_t,region)
+        {
+        i++;
+        if (in_region(addr,region,i))
+            {
+            printf("  ***  Address is in region descriptor at %lx\n",
+		   (long)region);
+            retval = rvm_true;
+            }
+        }
+
+    return retval;
+    }
+/* search segment list */
+rvm_bool_t in_seg_list(addr)
+    rvm_length_t    addr;               /* address to search for */
+    {
+    seg_t           *seg;
+    long            i = 0;
+    rvm_bool_t      retval = rvm_false;
+
+    /* basic list checks */
+    printf("Searching segment list\n");
+    if (!chk_list(&seg_root,rvm_true))
+        return retval;
+
+    /* check each segment descriptor */
+    FOR_ENTRIES_OF(seg_root,seg_t,seg)
+        {
+        i++;
+        if (in_seg(addr,seg,i))
+            retval = rvm_true;
+        }
+
+    return retval;
+    }
+/* locate an address in change tree */
+rvm_bool_t in_seg_dict(addr,seg_dict,n)
+    rvm_length_t    addr;               /* address to search for */
+    seg_dict_t      *seg_dict;          /* segment dictionary entry */
+    long            n;
+    {
+    char            *seg_name;
+    rvm_bool_t      retval = rvm_false;
+
+    printf("   Searching segment dictionary entry %ld\n",n);
+    if (seg_dict->seg != NULL)
+        seg_name = seg_dict->seg->dev.name;
+    else
+        seg_name = seg_dict->dev.name;
+    if (seg_name == NULL)
+        printf("Searching change tree for UNKNOWN segment at %lx\n",
+               (long)seg_dict);
+    else
+        printf("Searching change tree for %s\n",seg_name);
+
+    if (seg_dict->seg != NULL)
+        retval = in_seg(addr,seg_dict->seg,0);
+    if (IN_STRUCT(addr,seg_dict,seg_dict_id))
+        {
+        printf("  ***  Address is in seg_dict at %lx\n",(long)seg_dict);
+        retval = rvm_true;
+        }
+    if (seg_dict->dev.name != NULL)
+        if (in_heap(addr,(rvm_length_t)seg_dict->dev.name,seg_dict->dev.name_len))
+            {
+            printf("  ***  Address is in device name of seg_dict at %lx\n",
+                   (long)seg_dict);
+            retval = rvm_true;
+            }
+
+    if (search_dev_region(addr,(dev_region_t *)seg_dict->mod_tree.root))
+        retval = rvm_true;
+
+    return retval;
+    }
+/* search log special function descriptor */
+rvm_bool_t in_log_special(addr,special,n)
+    rvm_length_t    addr;               /* address to search for */
+    log_special_t   *special;           /* log special descriptor to search */
+    long            n;
+    {
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if in descriptor */
+    printf("   Searching special function descriptor %ld\n",n);
+    if (IN_STRUCT(addr,special,log_special_id))
+        {
+        printf("  ***  Address is in log special function decriptor at %lx\n",
+               (long)special);
+        retval = rvm_true;
+        }
+
+    /* structure specific tests */
+    switch (special->rec_hdr.struct_id)
+        {
+      case log_seg_id:
+        if (in_heap(addr,(rvm_length_t)special->special.log_seg.name,
+                    special->special.log_seg.name_len+1))
+            {
+            printf("  ***  Address is in segment name buffer\n");
+            retval = rvm_true;
+            }
+        break;
+      default:
+        printf("  Record has unknown struct_id\n");
+        }
+
+    return retval;
+    }
+/* search modification range descriptor */
+rvm_bool_t in_range(addr,range,n)
+    rvm_length_t    addr;               /* address to search for */
+    range_t         *range;
+    long            n;
+    {
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if in descriptor */
+    printf("     Searching range %ld\n",n);
+    if (IN_STRUCT(addr,range,range_id))
+        {
+        printf("  ***  Address is in modification range decriptor at %lx\n",
+               (long)range);
+        retval = rvm_true;
+        }
+
+    /* see if in old value save buffer */
+    if (in_heap(addr,(rvm_length_t)range->data,range->data_len))
+        {
+        printf("  ***  Address is in data buffer of range descriptor");
+	printf(" at %lx\n",(long)range);
+        retval = rvm_true;
+        }
+    if (range->nvaddr != NULL)
+        if ((addr >= (rvm_length_t)range->nvaddr) &&
+            (addr < ((rvm_length_t)range->nvaddr + range->nv.length)))
+            {
+            printf("  ***  Address is in data buffer of range descriptor");
+	    printf(" at %lx\n",(long)range);
+            retval = rvm_true;
+            }
+
+    /* check the region ptr */
+    if (ADDR_INVALID_OR_NULL(range->region))
+        printf("  Range at %lx has bad region ptr\n",(long)range);
+    else
+        if (range->region->links.struct_id != region_id) {
+            printf("  Region at %lx has invalid struct_id,",
+		   (long)range->region);
+	    printf(" struct_id = %d\n",range->region->links.struct_id);
+	}
+
+    return retval;
+    }
+/* search transaction descriptor */
+rvm_bool_t in_tid(addr,tid,n)
+    rvm_length_t    addr;               /* address to search for */
+    int_tid_t       *tid;               /* transaction descriptor to search */
+    long            n;
+    {
+    range_t         *range;
+    long            i = 0;
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if in descriptor */
+    printf("   Searching tid %ld\n",n);
+    if (IN_STRUCT(addr,tid,int_tid_id))
+        {
+        printf("    ***  Address is in transaction decriptor at %lx\n",
+               (long)tid);
+        retval = rvm_true;
+        }
+
+    /* see if in search vector */
+    if (in_heap(addr,(rvm_length_t)tid->x_ranges,
+                tid->x_ranges_alloc*sizeof(range_t *)))
+        {
+        printf("    ***  Address is in tid.x_ranges at %lx\n",(long)tid);
+        retval = rvm_true;
+        }
+
+    /* check range list and range descriptors */
+        printf("    Checking modification ranges\n");
+
+/* need chk_tree function
+    if (!chk_tree(&tid->range_tree,rvm_true))
+        printf("  Tid at %x has damaged range tree\n",tid);
+    else
+*/
+        FOR_NODES_OF(tid->range_tree,range_t,range)
+            {
+            i++;
+            if (in_range(addr,range,i))
+                retval = rvm_true;
+            }
+
+    return retval;
+    }
+/* search a log descriptor */
+rvm_bool_t in_log(addr,log,n)
+    rvm_length_t    addr;               /* address to search for */
+    log_t           *log;               /* log descriptor to search */
+    long            n;                  /* position in list */
+    {
+    long            i;
+    int_tid_t       *tid;
+    log_special_t   *special;
+    rvm_bool_t      retval = rvm_false;
+
+    /* see if in descriptor */
+    printf("  Searching log %ld\n",n);
+    if (IN_STRUCT(addr,log,log_id))
+        {
+        printf("  ***  Address is in log descriptor at %lx\n",(long)log);
+        retval = rvm_true;
+        }
+
+    /* check device name and raw i/o buffer */
+    if (ADDR_INVALID_OR_NULL(log->dev.name))
+        printf("  Log descriptor at %lx has bad dev.name\n",(long)log);
+    else
+        if (in_heap(addr,(rvm_length_t)log->dev.name,log->dev.name_len))
+            {
+            printf("  ***  Address is in log at %lx device name\n",(long)log);
+            retval = rvm_true;
+            }
+    if (log->dev.raw_io)
+        {
+        if (in_heap(addr,(rvm_length_t)log->dev.wrt_buf,log->dev.wrt_buf_len))
+            {
+            printf("  ***  Address is in log at %lx wrt_buf\n",(long)log);
+            retval = rvm_true;
+            }
+        }
+    /* check i/o vector and pad buffer */
+    if (log->dev.iov_length != 0)
+        {
+        if (ADDR_INVALID_OR_NULL(log->dev.iov))
+            printf("  Log descriptor at %lx has bad dev.iov ptr\n",(long)log);
+        else
+            {
+            if (in_heap(addr,(rvm_length_t)log->dev.iov,
+                        log->dev.iov_length * sizeof(struct iovec)))
+                {
+                printf("  ***  Address is in log at %lx i/o vector\n",
+		       (long)log);
+                retval = rvm_true;
+                }
+            }
+        }
+    if (log->dev.pad_buf_len != 0)
+        {
+        if (ADDR_INVALID_OR_NULL(log->dev.pad_buf))
+            printf("  Log descriptor at %lx has bad dev.pad_buf ptr\n",
+		   (long)log);
+        else
+            {
+            if (in_heap(addr,(rvm_length_t)log->dev.pad_buf,
+                        log->dev.pad_buf_len))
+                {
+                printf("  ***  Address is in log pad buffer at %lx\n",
+		       (long)log);
+                retval = rvm_true;
+                }
+            }
+        }
+    /* check recovery buffers */
+    if (ADDR_INVALID_OR_NULL(log->log_buf.buf))
+        printf("  Log descriptor at %lx has bad log_buf.malloc_buf ptr",
+               (long)log);
+    else
+        {
+        if (in_heap(addr,(rvm_length_t)log->log_buf.buf,
+                    log->log_buf.length))
+            {
+            printf("  ***  Address is in log recovery buffer at %lx\n",
+		   (long)log);
+            retval = rvm_true;
+            }
+        }
+    if (ADDR_INVALID_OR_NULL(log->log_buf.aux_buf))
+        printf("  Log descriptor at %lx has bad log_buf.aux_buf ptr",
+               (long)log);
+    else
+        {
+            if (in_heap(addr,(rvm_length_t)log->log_buf.aux_buf,
+                        log->log_buf.aux_length))
+                {
+                printf("  ***  Address is in auxillary buffer log at %lx",
+		       (long)log);
+		printf(" recovery buffer\n");
+                retval = rvm_true;
+                }
+        }
+    /* check tid and flush lists */
+    printf("  Checking uncommitted tids\n");
+    if (!chk_list(&log->tid_list,rvm_true))
+        printf("  Log at %lx has damaged uncommited tid list\n",(long)log);
+    else
+        {
+        i = 0;
+        FOR_ENTRIES_OF(log->tid_list,int_tid_t,tid)
+            {
+            i++;
+            if (in_tid(addr,tid,i))
+                retval = rvm_true;
+            }
+        }
+    printf("  Checking flush list\n");
+    if (!chk_list(&log->flush_list,rvm_true))
+        printf("  Log at %lx has damaged flush list\n",(long)log);
+    else
+        {
+        i = 0;
+        FOR_ENTRIES_OF(log->flush_list,int_tid_t,tid)
+            {
+            i++;
+            if (in_tid(addr,tid,i))
+                retval = rvm_true;
+            }
+        }
+
+    /* check immediate stream list */
+    printf("  Checking special list\n");
+    if (!chk_list(&log->special_list,rvm_true))
+        printf("  Log at %lx has damaged special list\n",(long)log);
+    else
+        {
+        i = 0;
+        FOR_ENTRIES_OF(log->special_list,log_special_t,special)
+            {
+            i++;
+            if (in_log_special(addr,special,i))
+                retval = rvm_true;
+            }
+        }
+
+    /* check segment dictionary */
+    if (log->seg_dict_vec != NULL)
+        {
+        if (ADDR_INVALID(log->seg_dict_vec))
+            printf("  Log descriptor at %lx has bad seg_dict_vec ptr\n",
+                   (long)log);
+        else
+            {
+            printf("  Searching segment dictionary\n");
+            if (in_heap(addr,(rvm_length_t)log->seg_dict_vec,
+                        log->seg_dict_len*sizeof(seg_dict_t)))
+                {
+                printf("  ***  Address is in log at %lx seg_dict_vec\n",
+		       (long)log);
+                retval = rvm_true;
+                }
+            for (i = 0; i < log->seg_dict_len; i++)
+                if (in_seg_dict(addr,&log->seg_dict_vec[i],i+1))
+                    retval = rvm_true;
+            }
+        }
+
+    return retval;
+    }
+/* search log list */
+rvm_bool_t in_log_list(addr)
+    rvm_length_t    addr;               /* address to search for */
+    {
+    log_t           *log;
+    long            i = 0;
+    rvm_bool_t      retval = rvm_false;
+
+    /* basic list checks */
+    printf("Searching log list\n");
+    if (!chk_list(&log_root,rvm_true))
+        return retval;
+
+    /* check each segment descriptor */
+    FOR_ENTRIES_OF(log_root,log_t,log)
+        {
+        i++;
+        if (in_log(addr,log,i)) retval = rvm_true;
+        }
+
+    return retval;
+    }
+/* locate an address in RVM internal structures */
+void find_addr(addr)
+    rvm_length_t    addr;               /* address to search for */
+    {
+    rvm_bool_t      retval = rvm_false;
+
+    if (in_free_page_list(addr))
+        retval = rvm_true;
+    if (in_free_lists(addr))
+        retval = rvm_true;
+    if (in_region_tree(addr))
+        retval = rvm_true;
+    if (in_seg_list(addr))
+        retval = rvm_true;
+    if (in_log_list(addr))
+        retval = rvm_true;
+
+    if (!retval)
+        printf("\nAddress not found\n");
+
+    }
+/* test if entry is on list -- more forgiving than chk_list */
+void on_list(hdr,addr)
+    list_entry_t        *hdr;           /* header of list to search */
+    list_entry_t        *addr;          /* entry to search for */
+    {
+    list_entry_t        *entry;         /* current list entry */
+    long                i = 0;
+
+    if (hdr == NULL)
+        {
+        printf("List header is null\n"); return;
+        }
+    if (ADDR_INVALID(hdr))
+        {
+        printf("List header address invalid\n"); return;
+        }
+    if (hdr->is_hdr != rvm_true)
+        {
+        printf("List header invalid\n"); return;
+        }
+    if (addr == hdr)
+        {
+        printf("Entry is list header\n"); return;
+        }
+
+    if (addr == NULL)
+        {
+        printf("Entry is null\n"); return;
+        }
+    if (ADDR_INVALID(addr))
+        {
+        printf("Entry address invalid\n"); return;
+        }
+    if (addr->is_hdr)
+        printf("Entry claims to be a list header\n");
+
+    if (!(((long)hdr->struct_id > (long)struct_first_id) &&
+           ((long)hdr->struct_id < (long)struct_last_id)))
+        printf("  List header type is not valid, struct_id = %ld\n",
+               (long)hdr->struct_id);
+    if (!(((long)addr->struct_id > (long)struct_first_id) &&
+           ((long)addr->struct_id < (long)struct_last_id)))
+        printf("  Entry type is not valid, struct_id = %ld\n",
+               (long)addr->struct_id);
+    if (hdr->struct_id != addr->struct_id) {
+        printf("Entry is not of same type as list -- \n");
+	printf("  Entry->struct_id  = %ld\n",(long)addr->struct_id);
+	printf("  Header->struct_id = %ld\n",(long)hdr->struct_id);
+    }
+    if (addr->list.name != hdr)
+        printf("Entry claims to be on list %lx\n",(long)addr->list.name);
+
+    if (ADDR_INVALID_OR_NULL(hdr->nextentry))
+        {
+        printf("  List header has invalid nextentry field, ");
+	printf("hdr->nextentry = %lx\n",(long)hdr->nextentry);
+        return;
+        }
+    if (ADDR_INVALID_OR_NULL(hdr->preventry)) {
+        printf("  List header has invalid preventry field, ");
+	printf("hdr->preventry = %lx\n",(long)hdr->nextentry);
+    }
+
+    /* check all elements of list */
+    entry = hdr->nextentry;
+    while (entry->is_hdr != rvm_true)
+        {
+        i++;
+        if (entry == addr)
+            {
+            printf("Entry is number %ld of list\n",i);
+            return;
+            }
+        if (ADDR_INVALID_OR_NULL(entry->nextentry))
+            {
+            printf("Entry %ld has invalid nextentry field, ",i);
+	    printf("nextentry = %lx\n",(long)entry->nextentry);
+            return;
+            }
+        entry = entry->nextentry;
+        }
+
+    printf("Entry not on list\n");
+    }
diff --git a/rvm/rvm_init.c b/rvm/rvm_init.c
new file mode 100644
index 0000000..320db17
--- /dev/null
+++ b/rvm/rvm_init.c
@@ -0,0 +1,144 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                   RVM Initialization and Termination
+*
+*/
+
+#include "rvm_private.h"
+
+/* global variables */
+extern log_t        *default_log;       /* default log descriptor ptr */
+extern rvm_bool_t   rvm_utlsw;          /* true if call by rvmutl */
+
+char                *rvm_errmsg;         /* internal error message ptr */
+
+/* initialization control */
+/* Cannot statically initialize locks with pthreads. */
+static RVM_MUTEX    init_lock = MUTEX_INITIALIZER;
+static rvm_bool_t   inited = rvm_false;     /* initialization complete flag */
+static rvm_bool_t   terminated = rvm_false; /* shutdown flag -- no
+                                               restart allowed */
+
+/* check that RVM properly initialized (for interface functions) */
+rvm_bool_t bad_init(void)
+{
+    if (inited == rvm_true)           /* return reverse sense */
+        return rvm_false;
+    else
+        return rvm_true;
+}
+
+/* rvm_initialize */
+rvm_return_t rvm_initialize(const char *rvm_version, rvm_options_t *rvm_options)
+{
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    rvm_debug(0);                       /* only causes module loading */
+    if (strcmp(rvm_version,RVM_VERSION) != 0)
+        return RVM_EVERSION_SKEW;       /* version skew */
+    assert(sizeof(rvm_length_t) == sizeof(char *));
+    if ((retval=bad_options(rvm_options,rvm_true)) != RVM_SUCCESS)
+        return retval;                  /* bad options ptr or record */
+
+    CRITICAL(init_lock,                 /* begin init_lock crit sec */
+        {
+        if (inited) goto err_exit;      /* did it all already ... */
+        if (terminated)
+            {
+            retval = RVM_EINIT;         /* restart not allowed */
+            goto err_exit;
+            }
+
+        cthread_init();                 /* init Cthreads */
+
+        /* init basic structures */
+        if ((init_utils()) != 0)
+            {
+            retval =  RVM_EIO;          /* can't get time stamp */
+	    printf("Error in init_utils\n");
+            goto err_exit;
+            }
+        init_map_roots();               /* mapping list and tree */
+        init_log_list();                /* log device list */
+
+        if (rvm_options && rvm_options->create_log_file)
+        {
+            retval = rvm_create_log(rvm_options, &rvm_options->create_log_size,
+                                    rvm_options->create_log_mode);
+
+            if (retval != RVM_SUCCESS) {
+		printf("rvm_create_log failed\n");
+		goto err_exit;
+            }
+        }
+
+        /* process options */
+        if ((retval=do_rvm_options(rvm_options)) != RVM_SUCCESS) {
+		printf("do_rvm_options failed\n");
+		goto err_exit;
+	}
+
+        /* take care of default log */
+        if (default_log == NULL) {
+		if ((retval=do_log_options(NULL,NULL)) != RVM_SUCCESS) {
+			printf("do_rvm_options failed\n");
+			goto err_exit;
+		}
+	}
+        inited = rvm_true;              /* all done */
+
+err_exit:;
+        });                             /* end init_lock crit sec */
+
+    return retval;
+    }
+
+/* rvm_terminate */
+rvm_return_t rvm_terminate(void)
+{
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    CRITICAL(init_lock,                 /* begin init_lock crit sec */
+        {
+        if (terminated) goto err_exit;  /* already terminated... */
+        if (!inited)
+            {
+            retval = RVM_EINIT;         /* RVM not initialized */
+            goto err_exit;
+            }
+
+        /* close log devices (will check for active transactions) */
+        if ((retval=close_all_logs()) != RVM_SUCCESS)
+            goto err_exit;
+
+        /* close segment devices */
+        if ((retval=close_all_segs()) != RVM_SUCCESS)
+            goto err_exit;
+
+        /* abort any further RVM function calls */
+        inited = rvm_false;
+        terminated = rvm_true;
+
+err_exit:;
+        });                             /* end init_lock crit sec */
+
+    return retval;
+}
diff --git a/rvm/rvm_io.c b/rvm/rvm_io.c
new file mode 100644
index 0000000..2b531a3
--- /dev/null
+++ b/rvm/rvm_io.c
@@ -0,0 +1,552 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                               RVM I/O
+*
+*/
+
+#include <fcntl.h>
+#include <sys/file.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/uio.h>
+#include <errno.h>
+#include <unistd.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include "rvm_private.h"
+
+#ifndef UIO_MAXIOV 
+#define UIO_MAXIOV 16
+#endif
+
+#ifdef HAVE_FDATASYNC
+#define FSYNC(fd) fdatasync(fd)
+#else
+#define FSYNC(fd) fsync(fd)
+#endif
+
+/* global variables */
+device_t            *rvm_errdev;        /* last device to have error */
+int                 rvm_ioerrno=0;      /* also save the errno for I/O error */
+rvm_length_t        rvm_max_read_len = MAX_READ_LEN; /* maximum single read in Mach */
+
+extern char         *rvm_errmsg;        /* internal error message buffer */
+extern log_t        *default_log;       /* log descriptor */
+extern rvm_bool_t   rvm_utlsw;          /* operating under rvmutl */
+extern rvm_bool_t   rvm_no_update;      /* no segment or log update if true */
+
+
+/* static prototypes */
+static rvm_bool_t in_wrt_buf(char *addr, rvm_length_t len);
+static long chk_seek(device_t *dev, rvm_offset_t *offset);
+
+#ifndef ZERO
+#define ZERO 0
+#endif
+
+/* buffer address checks: test if [addr, addr+len]
+   lies inside default_log->dev.wrt_buf */
+static rvm_bool_t in_wrt_buf(char *addr, rvm_length_t len)
+{
+    char            *end_addr;
+    char            *buf_end_addr;
+
+    if (default_log == NULL) 
+	return rvm_false;
+    if (default_log->dev.wrt_buf == NULL) 
+	return rvm_false;
+
+    end_addr = RVM_ADD_LENGTH_TO_ADDR(addr,len);
+    buf_end_addr = RVM_ADD_LENGTH_TO_ADDR(default_log->dev.wrt_buf,
+                                          default_log->dev.wrt_buf_len);
+    if (((addr >= default_log->dev.wrt_buf)
+	  && (addr < buf_end_addr))
+	&& ((end_addr > default_log->dev.wrt_buf)
+	     && (end_addr <= buf_end_addr)))
+        return rvm_true;
+
+    return rvm_false;
+}
+
+/* seek to position if required: raw devices must 
+   seek to a sector index. Sanity checks size of
+   device against offset. */
+static long chk_seek(device_t *dev, rvm_offset_t *offset)
+{
+    long            retval=0;
+
+    /* raw i/o offset must be specified and sector aligned */
+    assert((dev->raw_io) ? (offset != NULL) : 1);
+    assert((dev->raw_io) ? (OFFSET_TO_SECTOR_INDEX(*offset) == 0) : 1);
+    assert(RVM_OFFSET_LEQ(dev->last_position,dev->num_bytes));
+
+    /* seek if offset specified */
+    if (offset != NULL) {
+        assert(RVM_OFFSET_EQL_ZERO(*offset) ? 1
+               : RVM_OFFSET_LSS(*offset,dev->num_bytes));
+        if (!RVM_OFFSET_EQL(dev->last_position,*offset)) {
+            retval = lseek((int)dev->handle,
+                           (off_t)RVM_OFFSET_TO_LENGTH(*offset),
+                           SEEK_SET);
+            if (retval >= 0)
+                dev->last_position = *offset;
+	    else {
+		rvm_errdev = dev;
+		rvm_ioerrno = errno;
+	    }
+	}
+    }
+    return retval;
+}
+
+/* set device characteristics for device
+   device descriptor mandatory, lenght optional */
+long set_dev_char(device_t *dev, rvm_offset_t *dev_length)
+{
+    struct stat     statbuf;            /* status descriptor */
+    long            retval;             /* return value */
+    rvm_offset_t    temp;               /* offset calc. temp. */
+    unsigned long   mode;               /* protection mode */
+
+    errno = 0;
+
+    /* get file or device status */
+    retval = fstat(dev->handle,&statbuf);
+    if (retval != 0) {
+        rvm_errdev = dev;
+	rvm_ioerrno = errno;
+        return retval;
+    }
+
+    /* find type */
+    mode = statbuf.st_mode & S_IFMT;
+    dev->type = mode;
+    switch (mode)
+        {
+      case S_IFCHR:                     /* note raw io */
+        dev->raw_io = rvm_true;
+        break;
+	/* Linux doesn't have BSD style raw character devices.
+	   However, one can write to the block device directly.
+	   This takes care, since we must sync it as if we 
+	   do file IO.  We use dev->type == S_IFBLK 
+	   to achieve this. The result could be good, since the
+	   buffer cache will flush the blocks to the disk more 
+	   efficiently than individual synchronous writes would 
+	   take place.
+                     */
+      case S_IFBLK:  
+	dev->raw_io = rvm_true;
+	break;
+      case S_IFREG:
+        dev->num_bytes = RVM_MK_OFFSET(0,
+                             CHOP_TO_SECTOR_SIZE(statbuf.st_size));
+        break;
+      default:
+        rvm_errdev = dev;
+        return  -1;
+        }
+
+    /* set optional length of device or file */
+    if (dev_length != NULL)
+        {
+        temp = CHOP_OFFSET_TO_SECTOR_SIZE(*dev_length);
+
+        if (!RVM_OFFSET_EQL_ZERO(temp))
+            if (RVM_OFFSET_GTR(dev->num_bytes,temp)
+                || RVM_OFFSET_EQL_ZERO(dev->num_bytes))
+                dev->num_bytes = temp;
+        }
+
+    return 0;
+}
+/* open device or file */
+long open_dev(dev,flags,mode)
+    device_t        *dev;               /* device descriptor */
+    long            flags;              /* open option flags */
+    long            mode;               /* create protection modes */
+    {
+    long            handle;             /* device handle returned */
+
+    errno = 0;
+    dev->handle = 0;
+
+    /* attempt to open */
+    handle = (long)open(dev->name,flags | O_BINARY, mode);
+    if (handle < 0)
+        {
+        rvm_errdev = dev;
+	rvm_ioerrno = errno;
+        return handle;                  /* can't open, see errno... */
+        }
+
+    dev->handle = (long)handle;
+    RVM_ZERO_OFFSET(dev->last_position);
+    if (flags == O_RDONLY)
+        dev->read_only = rvm_true;
+
+    return 0;
+    }
+/* close device or file */
+long close_dev(dev)
+    device_t        *dev;               /* device descriptor */
+    {
+    long            retval;
+
+    assert(((dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    errno = 0;
+    if (dev->handle == 0) return 0;
+
+    /* close device */
+    if ((retval=close((int)dev->handle)) < 0)
+	{
+        rvm_errdev = dev;
+	rvm_ioerrno = errno;
+	}
+    else
+        dev->handle = 0;
+
+    return retval;
+    }
+/* read bytes from device or file */
+long read_dev(dev,offset,dest,length)
+    device_t        *dev;               /* device descriptor */
+    rvm_offset_t    *offset;            /* device offset */
+    char            *dest;              /* address of data destination */
+    rvm_length_t    length;             /* length of transfer */
+    {
+    rvm_offset_t    last_position;
+    long            nbytes;
+    long            read_len;
+    long            retval;
+
+    assert(dev->handle != ZERO);
+    assert(length != 0);
+    assert((dev->raw_io) ? (SECTOR_INDEX(length) == 0) : 1);
+    assert(((dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    /* seek if necessary */
+    errno = 0;
+    if ((retval = chk_seek(dev,offset)) < 0)
+        return retval;
+    last_position = RVM_ADD_LENGTH_TO_OFFSET(dev->last_position,
+                                             length);
+    assert(RVM_OFFSET_EQL_ZERO(*offset) ? 1
+           : RVM_OFFSET_LEQ(last_position,dev->num_bytes));
+
+    /* do read in larg-ish blocks to avoid kernel buffer availability problems
+       also zero region if /dev/null being read */
+    retval = 0;
+    while (length != 0)
+        {
+        if (length <= rvm_max_read_len) 
+		read_len = length;
+        else 
+		read_len = rvm_max_read_len;
+	nbytes=read((int)dev->handle,dest,(int)read_len);
+        if (nbytes < 0) {
+		rvm_errdev = dev;
+		rvm_ioerrno = errno;
+		return nbytes;
+	}
+        if (nbytes == 0)                /* force a cheap negative test */
+            if (rvm_utlsw && dev->raw_io) /* since rarely used */
+                if (!strcmp(dev->name,"/dev/null"))
+                    {
+                    retval = length;
+                    BZERO(dest,length); /* zero the read region */
+                    break;
+                    }
+        assert((dev->raw_io) ? (nbytes == read_len) : 1);
+        retval += nbytes;
+        dest += nbytes;
+        length -= nbytes;
+        }
+
+    /* update position */
+    dev->last_position = RVM_ADD_LENGTH_TO_OFFSET(dev->last_position,
+                                                  retval);
+    return retval;
+    }
+/* write bytes to device or file */
+long write_dev(dev,offset,src,length,sync)
+    device_t        *dev;               /* device descriptor */
+    rvm_offset_t    *offset;            /* device offset */
+    char            *src;               /* address of data source */
+    rvm_length_t    length;             /* length of transfer */
+    rvm_bool_t      sync;               /* fsync if true */
+{
+    rvm_offset_t    last_position;
+    long            retval;
+    long            wrt_len = length;   /* for no_update mode */
+
+    assert(dev->handle != ZERO);
+    assert(length != 0);
+    assert((dev->raw_io) ? (SECTOR_INDEX(length) == 0) : 1);
+    assert(((dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    /* seek if necessary */
+    errno = 0;
+    if ((retval = chk_seek(dev,offset)) < 0) 
+	return retval;
+    last_position = RVM_ADD_LENGTH_TO_OFFSET(dev->last_position,
+                                             length);
+    assert(RVM_OFFSET_LEQ(last_position,dev->num_bytes));
+
+    /* do write if not in no update mode */
+    if (!(rvm_utlsw && rvm_no_update)) {
+        if ((wrt_len=write((int)dev->handle,src,(int)length)) < 0) {
+            rvm_errdev = dev;
+	    rvm_ioerrno = errno;
+            return wrt_len;
+	}
+
+        /* fsync if doing file i/o */
+        if (((!dev->raw_io && sync==SYNCH) ||
+	     (dev->raw_io && dev->type == S_IFBLK))) {
+            if ((retval=FSYNC((int)dev->handle))  < 0) {
+                rvm_errdev = dev;
+		rvm_ioerrno = errno;
+                return retval;
+	    }
+	}
+    }
+
+    /* update position (raw i/o must be exact) */
+    assert((dev->raw_io) ? (wrt_len == length) : 1);
+    dev->last_position = RVM_ADD_LENGTH_TO_OFFSET(dev->last_position,
+                                                  wrt_len);
+    return wrt_len;
+}
+/* gather write for files */
+static long gather_write_file(dev,offset,wrt_len)
+    device_t        *dev;               /* device descriptor */
+    rvm_offset_t    *offset;            /* disk position */
+    rvm_length_t    *wrt_len;           /* num bytes written (out) */
+    {
+    long            retval;             /* kernel return value */
+    long            iov_index = 0;      /* index of current iov entry */
+    int             count;              /* iov count for Unix i/o */
+ 
+    assert(((dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    /* seek if necessary */
+    if ((retval = chk_seek(dev,offset)) < 0)
+        return retval;
+
+    /* do gather write in groups of 16 for Unix */
+    if (!(rvm_utlsw && rvm_no_update))
+        while (dev->iov_cnt > 0)
+            {
+            if (dev->iov_cnt > UIO_MAXIOV)
+		count = UIO_MAXIOV;
+            else
+		count = dev->iov_cnt;
+
+            retval = writev(dev->handle, &dev->iov[iov_index], count);
+            if (retval < 0)
+                {
+                rvm_errdev = dev;
+		rvm_ioerrno = errno;
+                return retval;
+                }
+
+            *wrt_len += (rvm_length_t)retval;
+            dev->iov_cnt -= count;
+            iov_index += count;
+            }
+    else
+        /* sum lengths for no_update mode */
+        for (count=0; count < dev->iov_cnt; count++)
+            *wrt_len += dev->iov[count].iov_len;
+
+
+    /* update position */
+    dev->last_position = RVM_ADD_LENGTH_TO_OFFSET(dev->last_position,
+                                                  *wrt_len);
+    assert(RVM_OFFSET_LEQ(dev->last_position,dev->num_bytes));
+    assert(*wrt_len == dev->io_length);
+
+    return 0;
+    }
+/* incremental write for gather-write to partitions */
+static long incr_write_partition(dev,offset,start_addr,end_addr)
+    device_t        *dev;               /* device descriptor */
+    rvm_offset_t    *offset;            /* disk position (in/out) */
+    char            *start_addr;        /* vm starting address of data */
+    char            *end_addr;          /* vm ending address of data */
+    {
+    long            retval;             /* kernel return value */
+    rvm_offset_t    tmp_offset;         /* position temp */
+    rvm_length_t    length;             /* num. bytes to write (request) */
+    rvm_length_t    len;                /* length corrected to sector size */
+    char            *wrt_addr;          /* write address corrected to sect size */
+
+    /* force seek to re-write partially filled sector */
+    len = OFFSET_TO_SECTOR_INDEX(*offset);
+    tmp_offset = CHOP_OFFSET_TO_SECTOR_SIZE(*offset);
+    wrt_addr = (char *)CHOP_TO_SECTOR_SIZE(start_addr);
+
+    /* calculate sector-sized write length */
+    length = (rvm_length_t)
+             RVM_SUB_LENGTH_FROM_ADDR(end_addr,start_addr);
+    if (length == 0) return 0;          /* ignore null writes */
+    len = ROUND_TO_SECTOR_SIZE(len+length);
+
+    /* write */
+    assert(in_wrt_buf(wrt_addr,len));
+    retval = write_dev(dev,&tmp_offset,wrt_addr,len,NO_SYNCH);
+    if (retval < 0) return retval;
+    assert(len == retval);
+
+    /* update position */
+    *offset = RVM_ADD_LENGTH_TO_OFFSET(*offset,length);
+
+    return length;
+    }
+/* gather write for disk partitions */
+static long gather_write_partition(dev,offset,wrt_len)
+    device_t        *dev;                 /* device descriptor */
+    rvm_offset_t    *offset;              /* disk position */
+    rvm_length_t    *wrt_len;             /* num bytes written (out) */
+    {
+    long            retval = 0;           /* kernel return value */
+    long            iov_index = 0;        /* index of current iov entry */
+    long            bytes_left;           /* num. bytes left in wrt_buf */
+    struct iovec   *iov = dev->iov;      /* i/o vector */
+
+    rvm_bool_t      did_wrap = rvm_false; /* debug use only */
+    rvm_offset_t    temp;
+    rvm_length_t    len;
+
+    assert((SECTOR_INDEX(dev->ptr-dev->wrt_buf)) ==
+           (OFFSET_TO_SECTOR_INDEX(*offset)));
+    len = (rvm_length_t)RVM_SUB_LENGTH_FROM_ADDR(dev->ptr,
+                                                 dev->buf_start);
+    temp = RVM_ADD_LENGTH_TO_OFFSET(dev->sync_offset,len);
+    assert(RVM_OFFSET_EQL(*offset,temp)); /* must match tail */
+
+    /* write io vector entries */
+    bytes_left = (long)RVM_SUB_LENGTH_FROM_ADDR(dev->buf_end,dev->ptr);
+    while (dev->iov_cnt > 0)
+        {
+        assert(bytes_left >= 0);
+        if (iov[iov_index].iov_len <= bytes_left)
+            {
+            /* copy whole range into wrt_buf */
+            BCOPY(iov[iov_index].iov_base, dev->ptr, iov[iov_index].iov_len);
+            bytes_left -= iov[iov_index].iov_len;
+            *wrt_len += iov[iov_index].iov_len;
+            dev->ptr = RVM_ADD_LENGTH_TO_ADDR(dev->ptr, iov[iov_index].iov_len);
+            iov_index++;                /* move to next entry */
+            dev->iov_cnt--;
+            }
+        else
+            {
+            /* copy what fits and leave remainder in iov entry */
+            if (bytes_left != 0)
+                {
+                BCOPY(iov[iov_index].iov_base, dev->ptr, bytes_left);
+                iov[iov_index].iov_len -= bytes_left;
+                *wrt_len += bytes_left;
+                iov[iov_index].iov_base =
+                    RVM_ADD_LENGTH_TO_ADDR(iov[iov_index].iov_base, bytes_left);
+                }
+            /* write what's in wrt_buf & re-init buffer */
+            if (dev->buf_start != dev->buf_end)
+                {
+                retval=incr_write_partition(dev,&dev->sync_offset,
+                                            dev->buf_start,dev->buf_end);
+                if (retval < 0) return retval;
+                }
+            did_wrap = rvm_true;
+            dev->ptr = dev->buf_start = dev->wrt_buf;
+            bytes_left = dev->wrt_buf_len;
+            }
+        }
+
+    assert((retval >= 0) ? (*wrt_len == dev->io_length) : 1);
+    return retval;
+    }
+/* gather write to device: accepts vector of any length
+   pointed to by device descriptor */
+long gather_write_dev(dev,offset)
+    device_t        *dev;               /* device descriptor */
+    rvm_offset_t    *offset;            /* device offset */
+    {
+    long            retval;             /* kernel return value */
+    rvm_length_t    wrt_len = 0;        /* #bytes actually written */
+
+    assert(RVM_OFFSET_GEQ(*offset,default_log->status.log_start));
+    assert(RVM_OFFSET_LSS(*offset,dev->num_bytes));
+    assert(RVM_OFFSET_LEQ(dev->last_position,dev->num_bytes));
+
+    errno = 0;
+
+    /* select gather-write mechanism for partitions or files */
+    if (dev->raw_io)
+        retval = gather_write_partition(dev,offset,&wrt_len);
+    else
+        retval = gather_write_file(dev,offset,&wrt_len);
+
+    if (retval < 0) return retval;      /* error */
+
+    return wrt_len;
+    }
+/* sync file */
+long sync_dev(dev)
+    device_t        *dev;               /* device descriptor */
+    {
+    long            retval;
+
+    assert(dev->handle != 0);
+    assert(((dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+    errno = 0;
+
+    /* use kernel call for file sync */
+    if (!dev->raw_io)
+	{
+	retval = FSYNC((int)dev->handle);
+	if (retval<0)
+	    {
+	    rvm_errdev = dev;
+	    rvm_ioerrno = errno;
+	    }
+        return retval;
+	}
+
+    /* raw i/o flushes buffer */
+    retval = incr_write_partition(dev,&dev->sync_offset,
+                                  dev->buf_start,dev->ptr);
+    if (retval >= 0)
+        dev->buf_start = dev->ptr;
+
+    return retval;
+    }
+
+
+
diff --git a/rvm/rvm_loadseg.c b/rvm/rvm_loadseg.c
new file mode 100644
index 0000000..98dc179
--- /dev/null
+++ b/rvm/rvm_loadseg.c
@@ -0,0 +1,139 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+#*/
+
+#include <stdlib.h>
+#include <string.h>
+#include "rvm/rvm.h"
+#include "rvm/rvm_segment.h"
+#include "rvm/rvm_segment_private.h"
+
+/* from rvm_private.h */
+rvm_bool_t rvm_register_page(char *vmaddr, rvm_length_t length);
+
+/* Global variables */
+
+extern rvm_bool_t rvm_map_private;  /* Do we map private or not. */
+
+/* Here's a hack to help debug the file server -- save the amount of space
+ * mapped in so the file server can dump it out again. -- Just a hack.
+ */
+long rds_rvmsize;
+char *rds_startaddr;
+
+/*
+ * rvm_load_segment
+ * - locates the place in the process's address where to load segments
+ * - then maps in the recoverable regions at that point.
+ * - it returns an array of the region descriptors.
+ */
+rvm_return_t
+rvm_load_segment(char *DevName, rvm_offset_t DevLength, rvm_options_t *options,
+		 unsigned long *nregions, rvm_region_def_t **regions)
+{
+    rvm_region_t *region = rvm_malloc_region();
+    rvm_region_t *hdr_region = rvm_malloc_region();
+    rvm_segment_hdr_t *hdrp;
+    rvm_return_t err;
+    int i;
+
+    /* HACK */ rds_rvmsize = 0; /* HACK */
+
+    /* Read in the header region of the segment. */
+    hdr_region->data_dev = DevName;
+    hdr_region->dev_length = DevLength;		/* Struct assignment */
+    RVM_ZERO_OFFSET(hdr_region->offset);
+    hdr_region->length = RVM_SEGMENT_HDR_SIZE;
+    hdr_region->vmaddr = 0;
+
+    hdr_region->vmaddr = NULL;
+    if (!rvm_map_private) {
+        err = allocate_vm(&(hdr_region->vmaddr), hdr_region->length);
+	if (err != RVM_SUCCESS)
+	    return err;
+    }
+    /* else, as vmaddr is NULL, the segment will be pre-allocated and
+     * registered by rvm_map->establish_range->round_region->page_alloc -JH */
+
+    err = rvm_map(hdr_region,options);
+    if (err != RVM_SUCCESS)
+	return err;    /* Some error condition exists, return the error code */
+
+    hdrp = (rvm_segment_hdr_t *)(hdr_region->vmaddr);
+
+    /* Make sure struct_id is correct */
+    if (hdrp->struct_id != rvm_segment_hdr_id)
+	return (rvm_return_t)RVM_ESEGMENT_HDR;
+
+    /* Match version stamps */
+    if (strcmp(hdrp->version, RVM_SEGMENT_VERSION) != 0)
+	return RVM_EVERSION_SKEW;
+
+    /* Make sure the regions do not overlap */
+    if (overlap(hdrp->nregions, hdrp->regions))
+	return RVM_EVM_OVERLAP;
+
+    /* Map in the regions */
+    region->data_dev = DevName;
+    region->dev_length = DevLength;		/* Struct assignment */
+
+    /* Setup return region definition array */
+    (*nregions) = hdrp->nregions;
+    (*regions) = (rvm_region_def_t *)malloc(sizeof(rvm_region_def_t)*(*nregions));
+
+    /* HACK */ rds_startaddr = hdrp->regions[0].vmaddr; /* HACK */
+
+    for (i = 0; i < hdrp->nregions; i++)
+	if ((unsigned long)(hdrp->regions[i].vmaddr) >= 0) {
+	    region->offset = (*regions)[i].offset = hdrp->regions[i].offset;
+	    region->length = (*regions)[i].length = hdrp->regions[i].length;
+	    region->vmaddr = (*regions)[i].vmaddr = hdrp->regions[i].vmaddr;
+
+	    /* HACK */ rds_rvmsize += region->length; /* HACK */
+
+	    if (!rvm_map_private) {
+	        err = allocate_vm(&(region->vmaddr), region->length);
+		if (err != RVM_SUCCESS)
+		    return err;
+	    } else
+	      if (!rvm_register_page(region->vmaddr, region->length))
+		return RVM_EINTERNAL;
+
+	    err = rvm_map(region, options);
+	    if (err != RVM_SUCCESS)
+		return err; 	/* Some error condition exists, abort */
+
+ 	}
+
+    /* Clean up, we no longer need the header region */
+    switch (err = rvm_unmap(hdr_region)) {
+      case RVM_EREGION:
+      case RVM_EUNCOMMIT:
+      case RVM_ENOT_MAPPED:
+      case RVM_ERANGE:
+	deallocate_vm(hdr_region->vmaddr, hdr_region->length);
+	return err;
+	break;
+      default:
+	/* do nothing */
+	break;
+    }
+
+    err = deallocate_vm(hdr_region->vmaddr, hdr_region->length);
+
+    rvm_free_region(hdr_region);
+    return err;
+}
diff --git a/rvm/rvm_logflush.c b/rvm/rvm_logflush.c
new file mode 100644
index 0000000..c36a5eb
--- /dev/null
+++ b/rvm/rvm_logflush.c
@@ -0,0 +1,668 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                            RVM log records flush support
+*
+*/
+
+#include <sys/time.h>
+#include <sys/uio.h>
+#include "rvm_private.h"
+
+/* global variables */
+
+extern  log_t       *default_log;       /* default log descriptor ptr */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+extern rvm_bool_t   rvm_utlsw;          /* running under rvmutl */
+extern rvm_length_t rvm_optimizations;  /* optimization switches */
+
+rvm_length_t        flush_times_vec[flush_times_len] = {flush_times_dist};
+rvm_length_t        range_lengths_vec[range_lengths_len] =
+                                        {range_lengths_dist};
+rvm_length_t        range_overlaps_vec[range_overlaps_len] =
+                                        {range_overlaps_dist};
+rvm_length_t        trans_overlaps_vec[trans_overlaps_len] =
+                                        {trans_overlaps_dist};
+rvm_length_t        range_elims_vec[range_elims_len] =
+                                        {range_elims_dist};
+rvm_length_t        trans_elims_vec[trans_elims_len] =
+                                        {trans_elims_dist};
+rvm_length_t        trans_coalesces_vec[trans_coalesces_len] =
+                                        {trans_coalesces_dist};
+/* allocate variable sized log i/o vector */
+static rvm_return_t make_iov(log,length)
+    log_t           *log;               /* log descriptor */
+    long            length;             /* entries needed */
+    {
+    device_t        *dev = &log->dev;   /* device descriptor */
+
+    /* test if enough space already available */
+    if (dev->iov_length < length)
+        {
+        /* no, free old array */
+        dev->iov_length = 0;
+        if (dev->iov != NULL)
+            free((char *)dev->iov);
+
+        /* reallocate io vector */
+        dev->iov = (struct iovec *)malloc(sizeof(struct iovec) * length);
+        if (dev->iov == NULL) return RVM_ENO_MEMORY;
+        dev->iov_length = length;
+        }
+
+    /* initialize */
+    dev->io_length = 0;
+    dev->iov_cnt = 0;
+
+    return RVM_SUCCESS;
+    }
+
+/* make record number */
+static long make_rec_num(log)
+    log_t           *log;               /* log descriptor */
+    {
+    if (log->status.first_rec_num == 0)
+        log->status.first_rec_num = log->status.next_rec_num;
+    return log->status.next_rec_num++;
+    }
+/* allocate variable sized pad buffer */
+static void make_pad_buf(device_t *dev, long length)
+{
+    assert((length >= 0) && (length < SECTOR_SIZE));
+
+    /* see if must reallocate */
+    if (length > dev->pad_buf_len)
+    {
+	dev->pad_buf = REALLOC(dev->pad_buf,length);
+	assert(dev->pad_buf != NULL);
+	(void)memset(&dev->pad_buf[dev->pad_buf_len],-1,
+		     length-dev->pad_buf_len);
+	dev->pad_buf_len = length;
+    }
+}
+/* setup wrap marker i/o */
+static rvm_return_t write_log_wrap(log_t *log)
+{
+    device_t        *dev = &log->dev;
+    log_wrap_t      *wrap = &log->log_wrap;
+    rvm_offset_t    pad_len;
+
+    /* set timestamp and record number for wrap */
+    make_uname(&wrap->rec_hdr.timestamp);
+    wrap->rec_hdr.rec_num = make_rec_num(log);
+    log->status.tot_wrap++;
+
+    /* make iov entry */
+    dev->iov[dev->iov_cnt].iov_base = wrap;
+    dev->iov[dev->iov_cnt++].iov_len = sizeof(log_wrap_t);
+    dev->io_length += sizeof(log_wrap_t);
+
+    /* pad last sector with all 1's to kill previous wrap mark */
+    pad_len = RVM_ADD_LENGTH_TO_OFFSET(log->status.log_tail,
+                                       dev->io_length);
+    pad_len = RVM_SUB_OFFSETS(dev->num_bytes,pad_len);
+    make_pad_buf(dev,RVM_OFFSET_TO_LENGTH(pad_len));
+    dev->iov[dev->iov_cnt].iov_base = dev->pad_buf;
+    dev->iov[dev->iov_cnt++].iov_len = RVM_OFFSET_TO_LENGTH(pad_len);
+    dev->io_length += RVM_OFFSET_TO_LENGTH(pad_len);
+
+    assert(dev->iov_cnt <= dev->iov_length);
+
+    if (gather_write_dev(&log->dev,&log->status.log_tail) < 0)
+        return RVM_EIO;
+#ifdef RVM_LOG_TAIL_SHADOW
+    /* 
+     * If we've gotten this far, we're going to update the log_tail pointer, 
+     * so there is a log wrap that we can allow when writing out the status
+     * block.
+     */
+    has_wrapped = rvm_true;
+#endif /* RVM_LOG_TAIL_SHADOW */
+    return update_log_tail(log, &wrap->rec_hdr);
+}
+/* setup header for nv log entry */
+static void build_trans_hdr(tid,is_first,is_last)
+    int_tid_t       *tid;
+    rvm_bool_t      is_first;           /* true if 1st header written */
+    rvm_bool_t      is_last;            /* true if last header written */
+    {
+    log_t           *log = tid->log;
+    trans_hdr_t     *trans_hdr = &tid->log->trans_hdr;
+    device_t        *dev = &log->dev;
+
+    /* setup entry header */
+    make_uname(&trans_hdr->rec_hdr.timestamp);
+    trans_hdr->rec_hdr.rec_num = make_rec_num(log);
+    trans_hdr->num_ranges = 0;
+    trans_hdr->rec_hdr.rec_length = TRANS_SIZE - sizeof(rec_end_t);
+    trans_hdr->uname = tid->uname;
+    trans_hdr->commit_stamp = tid->commit_stamp;
+    log->status.last_commit = tid->commit_stamp;
+
+    trans_hdr->flags = tid->flags;
+    if (is_first)
+        trans_hdr->flags |= FIRST_ENTRY_FLAG;
+    if (is_last)
+        trans_hdr->flags |= LAST_ENTRY_FLAG;
+    trans_hdr->n_coalesced = tid->n_coalesced;
+
+    tid->back_link = sizeof(trans_hdr_t);
+
+    /* enter in iovec */
+    dev->iov[0].iov_base = trans_hdr;
+    dev->iov[0].iov_len = sizeof(trans_hdr_t);
+    dev->io_length = TRANS_SIZE;
+    dev->iov_cnt = 1;
+
+    }
+/* setup end marker for log entry */
+static void build_rec_end(log,timestamp,rec_num,rec_type,back_link)
+    log_t           *log;               /* log descriptor */
+    struct timeval  *timestamp;         /* log record timestamp */
+    long            rec_num;            /* log record sequence number */
+    struct_id_t     rec_type;           /* struct_id of rec header */
+    rvm_length_t    back_link;          /* displacement to previous header */
+    {
+    rec_end_t       *rec_end = &log->rec_end;
+    trans_hdr_t     *trans_hdr = &log->trans_hdr;
+    device_t        *dev = &log->dev;
+
+    /* setup entry end marker */
+    rec_end->rec_hdr.rec_num = rec_num;
+    rec_end->rec_type = rec_type;
+    rec_end->rec_hdr.timestamp = *timestamp;
+    rec_end->rec_hdr.rec_length = dev->io_length - sizeof(rec_end_t);
+    trans_hdr->rec_hdr.rec_length = rec_end->rec_hdr.rec_length;
+    rec_end->sub_rec_len = back_link;
+
+    /* enter in iovec */
+    dev->iov[dev->iov_cnt].iov_base = rec_end;
+    dev->iov[dev->iov_cnt++].iov_len = sizeof(rec_end_t);
+
+    assert(dev->iov_cnt <= dev->iov_length);
+    }
+/* setup nv_range record */
+static void build_nv_range(log,tid,range)
+    log_t           *log;               /* log descriptor */
+    int_tid_t       *tid;               /* transaction descriptor */
+    range_t         *range;             /* range descriptor */
+    {
+    nv_range_t      *nv_range;          /* nv_range header */
+    device_t        *dev = &log->dev;
+
+    /* setup header fields */
+    nv_range = &range->nv;
+    log->trans_hdr.num_ranges += 1;
+    nv_range->rec_hdr.timestamp = log->trans_hdr.rec_hdr.timestamp;
+    nv_range->range_num = log->trans_hdr.num_ranges;
+    nv_range->rec_hdr.rec_num = log->trans_hdr.rec_hdr.rec_num;
+    nv_range->rec_hdr.rec_length = RANGE_SIZE(range);
+    nv_range->chk_sum =
+        chk_sum(range->nvaddr+BYTE_SKEW(nv_range->vmaddr),
+                range->nv.length);
+    dev->io_length += nv_range->rec_hdr.rec_length; /* accumulate lengths */
+    nv_range->sub_rec_len = tid->back_link;
+    tid->back_link = nv_range->rec_hdr.rec_length;
+
+    /* setup header i/o */
+    dev->iov[dev->iov_cnt].iov_base = nv_range;
+    dev->iov[dev->iov_cnt++].iov_len = sizeof(nv_range_t);
+    assert(dev->iov_cnt <= dev->iov_length);
+    
+    /* setup io for new values */
+    dev->iov[dev->iov_cnt].iov_base = range->nvaddr;
+    dev->iov[dev->iov_cnt++].iov_len = RANGE_LEN(range);
+
+    assert(dev->iov_cnt <= dev->iov_length);
+    enter_histogram(nv_range->length,log->status.range_lengths,
+                    range_lengths_vec,range_lengths_len);
+    }
+static void split_range(range,new_range,avail)
+    range_t         *range;             /* range to split */
+    range_t         *new_range;         /* temporary range descriptor */
+    rvm_length_t    avail;              /* space available */
+    {
+
+    /* copy basic data from parent range */
+    new_range->nv.rec_hdr.timestamp = range->nv.rec_hdr.timestamp;
+    new_range->nv.seg_code = range->nv.seg_code;
+    new_range->nv.vmaddr = range->nv.vmaddr;
+    new_range->nv.offset = range->nv.offset;
+    new_range->nv.is_split = range->nv.is_split;
+    new_range->nvaddr = range->nvaddr;
+    new_range->data = NULL;
+    new_range->data_len = 0;
+
+    /* set length of split */
+    assert(BYTE_SKEW(avail) == 0);
+    new_range->nv.length =
+        avail - BYTE_SKEW(RVM_OFFSET_TO_LENGTH(range->nv.offset));
+
+    /* adjust original range for split */
+    range->nvaddr = RVM_ADD_LENGTH_TO_ADDR(range->nvaddr,avail);
+    range->nv.vmaddr =
+        RVM_ADD_LENGTH_TO_ADDR(range->nv.vmaddr,new_range->nv.length);
+    range->nv.length -= new_range->nv.length;
+    range->nv.offset = RVM_ADD_LENGTH_TO_OFFSET(range->nv.offset,
+                                                new_range->nv.length);
+    range->nv.is_split = rvm_true;
+
+    assert(BYTE_SKEW(range->nv.vmaddr) == 0);
+    assert(BYTE_SKEW(range->nvaddr) == 0);
+    assert(BYTE_SKEW(RVM_OFFSET_TO_LENGTH(range->nv.offset)) == 0);
+    }
+static rvm_bool_t write_range(int_tid_t *tid, range_t *range,
+				rvm_offset_t *log_free)
+{
+    log_t           *log = tid->log;    /* log descriptor */
+    rvm_offset_t    avail;              /* log space available */
+
+    /* assign default nvaddr */
+    if (range->nvaddr == NULL)
+        range->nvaddr = (char *)CHOP_TO_LENGTH(range->nv.vmaddr);
+
+    /* see if range will fit in log */
+    avail = RVM_SUB_LENGTH_FROM_OFFSET(*log_free,
+                    ((long)log->dev.io_length+sizeof(log_wrap_t)));
+    assert(RVM_OFFSET_GTR(*log_free,avail)); /* underflow!! */
+
+    if (RANGE_SIZE(range) > RVM_OFFSET_TO_LENGTH(avail))
+    {
+        /* no, see if there's enough useful */
+        if (RVM_OFFSET_TO_LENGTH(avail) < MIN_NV_RANGE_SIZE)
+            return rvm_true;            /* no, wrap around first */
+
+        /* yes, build new descriptor for as much as fits */
+        split_range(range,&tid->split_range,
+                    RVM_OFFSET_TO_LENGTH(avail)-NV_RANGE_OVERHEAD);
+        build_nv_range(log,tid,&tid->split_range);
+        return rvm_true;                /* now wrap around */
+    }
+
+    /* enter nv_range header & new values */
+    build_nv_range(log,tid,range);
+
+    /* do region's uncommitted transaction accounting */
+    if (TID(FLUSH_FLAG))
+        CRITICAL(range->region->count_lock,
+            range->region->n_uncommit--);
+
+    return rvm_false;
+}
+static rvm_return_t write_tid(int_tid_t *tid)
+{
+    log_t           *log = tid->log;    /* log descriptor */
+    log_status_t    *status = &log->status; /* status block descriptor */
+    range_t         *range;             /* range ptr */
+    rvm_offset_t    log_free;           /* size of log tail area */
+    rvm_return_t    retval;
+
+    /* check that transactions are logged in commit order */
+    assert(TIME_GTR(tid->commit_stamp,log->status.last_commit));
+
+    /* initialize counters & allocate i/o vector for 2*(#ranges+1) plus
+       2 headers, 2 end marks, wrap marker, and padding (6) */
+    if ((retval=make_iov(log,2*(tid->range_tree.n_nodes+1)+6))
+        !=RVM_SUCCESS) return retval;
+
+    /* see if must wrap before logging tid */
+    log_tail_sngl_w(log,&log_free);
+    if (RVM_OFFSET_TO_LENGTH(log_free) < MIN_TRANS_SIZE)
+    {
+        if ((retval=write_log_wrap(log)) != RVM_SUCCESS)
+            return retval;
+        log_tail_sngl_w(log,&log_free);
+    }
+
+    /* output transaction header */
+    build_trans_hdr(tid,rvm_true,rvm_true);
+
+    /* build log records */
+    FOR_NODES_OF(tid->range_tree,range_t,range)
+    {
+        if (write_range(tid,range,&log_free))
+	{
+            /* insert end marker */
+            build_rec_end(log,&log->trans_hdr.rec_hdr.timestamp,
+                          log->trans_hdr.rec_hdr.rec_num,
+                          trans_hdr_id,tid->back_link);
+
+            /* write a wrap and restart */
+            log->status.n_split++;
+            log->trans_hdr.flags &= ~LAST_ENTRY_FLAG;
+            if ((retval=write_log_wrap(log)) != RVM_SUCCESS)
+                return retval;
+
+            /* make new transaction log entry header */
+            log_tail_sngl_w(log,&log_free);
+            build_trans_hdr(tid,rvm_false,rvm_true);
+
+            /* process remainder of range */
+            if (write_range(tid,range,&log_free))
+                assert(rvm_false);
+	}
+    }
+    /* insert end marker */
+    build_rec_end(log,&log->trans_hdr.rec_hdr.timestamp,
+                  log->trans_hdr.rec_hdr.rec_num,
+                  trans_hdr_id,tid->back_link);
+
+    /* accumulate range savings statistics and initiate i/o */
+    status->range_overlap = RVM_ADD_OFFSETS(status->range_overlap,
+                                            tid->range_overlap);
+    status->trans_overlap = RVM_ADD_OFFSETS(status->trans_overlap,
+                                            tid->trans_overlap);
+    status->n_range_elim += tid->range_elim;
+    status->n_trans_elim += tid->trans_elim;
+    status->n_trans_coalesced += tid->n_coalesced;
+    enter_histogram(tid->range_elim,status->range_elims,
+                    range_elims_vec,range_elims_len);
+    enter_histogram(tid->trans_elim,status->trans_elims,
+                    trans_elims_vec,trans_elims_len);
+    enter_histogram(RVM_OFFSET_TO_LENGTH(tid->range_overlap),
+                    status->range_overlaps,
+                    range_overlaps_vec,range_overlaps_len);
+    enter_histogram(RVM_OFFSET_TO_LENGTH(tid->trans_overlap),
+                    status->trans_overlaps,
+                    trans_overlaps_vec,trans_overlaps_len);
+    enter_histogram(tid->n_coalesced,status->tot_trans_coalesces,
+                    trans_coalesces_vec,trans_coalesces_len);
+    if (gather_write_dev(&log->dev,&log->status.log_tail) < 0)
+        return RVM_EIO;
+    return update_log_tail(log, &log->trans_hdr.rec_hdr);
+}
+/* wait for a truncation to free space for log record
+   -- assumes dev_lock is held */
+static rvm_return_t wait_for_space(log,space_needed,log_free,did_wait)
+    log_t           *log;               /* log descriptor */
+    rvm_offset_t    *space_needed;      /* amount of space required */
+    rvm_offset_t    *log_free;          /* size calculation temp */
+    rvm_bool_t      *did_wait;
+    {
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* see if enough space for current record */
+    *did_wait = rvm_false;
+    DO_FOREVER
+        {
+        log_tail_length(log,log_free);
+        if (RVM_OFFSET_GEQ(*log_free,*space_needed))
+            break;                      /* enough */
+
+        /* no, release log device & wait for truncation completion */
+        mutex_unlock(&log->dev_lock);
+        retval = wait_for_truncation(log,NULL);
+
+        /* regain log device & count waits */
+        mutex_lock(&log->dev_lock);
+        *did_wait = rvm_true;
+        log->status.n_truncation_wait++;
+        if (retval != RVM_SUCCESS) break;
+        }
+
+    return retval;
+    }
+/* compute log entry size; truncate if necessary */
+static rvm_return_t log_tid(log_t *log, int_tid_t *tid)
+{
+    rvm_offset_t    log_needed;
+    rvm_offset_t    log_free;           /* log size temp, debug only */
+    rvm_bool_t      did_wait;           /* debug only */
+    rvm_return_t    retval;             /* return value */
+
+    /* start daemon truncation if necessary */
+    (void)initiate_truncation(log,cur_log_percent(log,&tid->log_size));
+
+    CRITICAL(log->dev_lock,             /* begin dev_lock crit sec */
+    {
+        /* flush any immediate stream records */
+        if ((retval=flush_log_special(log)) != RVM_SUCCESS)
+            goto err_exit;
+
+        /* wait if truncation required to get space */
+	log_needed = RVM_ADD_LENGTH_TO_OFFSET(tid->log_size,
+				    sizeof(log_wrap_t) + sizeof(rec_end_t));
+	retval = wait_for_space(log, &log_needed, &log_free, &did_wait);
+	if (retval != RVM_SUCCESS) goto err_exit;
+
+        /* transfer tid to log device */
+        if ((retval=write_tid(tid)) != RVM_SUCCESS)
+            goto err_exit;
+
+        /* save uname of first & last transactions logged */
+        log->status.last_uname = tid->uname;
+        if (TIME_EQL_ZERO(log->status.first_uname))
+            log->status.first_uname = tid->uname;
+err_exit:;
+    });                             /* end dev_lock crit sec */
+    if (retval != RVM_SUCCESS) return retval;
+
+    /* scrap tid */
+    if (retval == RVM_SUCCESS)
+        CRITICAL(log->flush_list_lock,free_tid(tid));
+
+    return retval;
+}
+/* set up special log entry i/o */
+static void build_log_special(log_t *log, log_special_t *special)
+{
+    device_t *dev = &log->dev;	/* log device descriptor */
+    rvm_length_t length;
+
+    /* timestamp the entry */
+    make_uname(&special->rec_hdr.timestamp);
+
+    /* check that records are logged in strict FIFO order */
+    assert(TIME_GTR(special->rec_hdr.timestamp,log->status.last_write));
+
+    /* prepare i/o */
+    special->rec_hdr.rec_num = make_rec_num(log);
+    dev->io_length = special->rec_hdr.rec_length+sizeof(rec_end_t);
+    dev->iov[dev->iov_cnt].iov_base = &special->rec_hdr.struct_id;
+    dev->iov[dev->iov_cnt++].iov_len = LOG_SPECIAL_SIZE;
+
+    /* type-specific build operations */
+    switch (special->rec_hdr.struct_id)
+    {
+    case log_seg_id:                  /* copy segment device name */
+	length = special->rec_hdr.rec_length-LOG_SPECIAL_SIZE;
+	dev->iov[dev->iov_cnt].iov_base = special->special.log_seg.name;
+	dev->iov[dev->iov_cnt++].iov_len = length;
+	break;
+
+    default:
+	assert(rvm_false); /* unknown record type */
+    }
+    assert(dev->iov_cnt <= dev->iov_length);
+}
+/* insure space available in log; truncate if necessary, and initiate
+   i/o for special log entries; */
+static rvm_return_t log_special(log_t *log, log_special_t *special)
+{
+    rvm_offset_t    max_log_free;       /* log size temp, debug only */
+    rvm_bool_t      did_wait;           /* debug only */
+    rvm_offset_t    log_free;           /* size calculation temp */
+    rvm_offset_t    special_size;       /* maximum size needed in log */
+    rvm_return_t    retval;             /* return value */
+
+    /* see if truncation required to get space */
+    special_size = RVM_MK_OFFSET(0,special->rec_hdr.rec_length
+                  + sizeof(log_wrap_t) + sizeof(rec_end_t));
+    if ((retval=wait_for_space(log,&special_size,
+                               &max_log_free,&did_wait))
+        != RVM_SUCCESS) return retval;
+
+    /* be sure enough i/o vector slots available */
+    if ((retval=make_iov(log,LOG_SPECIAL_IOV_MAX))
+        != RVM_SUCCESS) return retval;
+    
+    /* find out how much log available & wrap if necessary */
+    log_tail_sngl_w(log,&log_free);
+    if (RVM_OFFSET_LSS(log_free,special_size))
+        if ((retval=write_log_wrap(log)) != RVM_SUCCESS)
+            return retval;
+
+    /* build special entry */
+    log->status.n_special++;
+    build_log_special(log,special);
+    build_rec_end(log,&special->rec_hdr.timestamp,special->rec_hdr.rec_num,
+                  special->rec_hdr.struct_id,special->rec_hdr.rec_length);
+
+    /* do the i/o & update log tail */
+    if (gather_write_dev(&log->dev, &log->status.log_tail) < 0)
+        return RVM_EIO;
+    retval = update_log_tail(log, &special->rec_hdr);
+    if (retval != RVM_SUCCESS) return retval;
+    
+    free_log_special(special);
+
+    return RVM_SUCCESS;
+}
+/* log immediate records flush -- log device locked by caller */
+rvm_return_t flush_log_special(log)
+    log_t           *log;
+    {
+    log_special_t   *special;           /* special record to log */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* process the special list */
+    DO_FOREVER
+        {
+        CRITICAL(log->special_list_lock, /* begin special_list_lock crit sec */
+            {
+            if (LIST_NOT_EMPTY(log->special_list))
+                special = (log_special_t *)
+                    move_list_entry(&log->special_list,NULL,NULL);
+            else special = NULL;
+            });                         /* end special_list_lock crit sec */
+        if (special == NULL) break;
+
+        /* flush this special request */
+        if ((retval=log_special(log,special)) != RVM_SUCCESS)
+            break;
+        }
+
+    return retval;
+    }
+/* internal log flush */
+rvm_return_t flush_log(log,count)
+    log_t           *log;
+    long            *count;              /* statistics counter */
+    {
+    int_tid_t       *tid;               /* tid to log */
+    rvm_bool_t      break_sw;           /* break switch for loop termination */
+    struct timeval  start_time;
+    struct timeval  end_time;
+    long            kretval;
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* allow only one flush at a time to avoid commit ordering problems */
+    RW_CRITICAL(log->flush_lock,w,      /* begin flush_lock crit sec */
+        {
+        /* process statistics */
+        if (count != NULL) (*count)++;
+        kretval= gettimeofday(&start_time,(struct timezone *)NULL);
+        if (kretval != 0)
+            {
+            retval = RVM_EIO;
+            goto err_exit;
+            }
+
+        /* establish flush mark so future commits won't be flushed
+           and cause extraordinarily long delay to this flush */
+        CRITICAL(log->flush_list_lock,  /* begin flush_list_lock crit sec */
+            {
+            if (LIST_NOT_EMPTY(log->flush_list))
+                ((int_tid_t *)(log->flush_list.preventry))->flags
+                    |= FLUSH_MARK;
+            });                         /* end flush_list_lock crit sec */
+        /* flush all queued tid's */
+        DO_FOREVER
+            {
+            /* do tid's one at a time to allow no_flush commits while flushing */
+            CRITICAL(log->flush_list_lock, /* begin flush_list_lock crit sec */
+                {
+                if (LIST_NOT_EMPTY(log->flush_list))
+                    tid = (int_tid_t *)log->flush_list.nextentry;
+                else tid = NULL;
+                });                     /* end flush_list_lock crit sec */
+            if (tid == NULL) break;
+
+            /* flush this tid */
+            break_sw = (rvm_bool_t)TID(FLUSH_MARK);
+            retval = log_tid(log,tid);
+            if ((retval != RVM_SUCCESS) || break_sw)
+                break;
+            }
+
+        /* force buffers to disk */
+        CRITICAL(log->dev_lock,
+            {
+            if (sync_dev(&log->dev) < 0)
+                retval = RVM_EIO;
+            });
+err_exit:;
+        });                             /* end flush_lock crit sec */
+
+    /* terminate timing */
+    if (retval == RVM_SUCCESS)
+    {
+	kretval= gettimeofday(&end_time,(struct timezone *)NULL);
+	if (kretval != 0) return RVM_EIO;
+	end_time = sub_times(&end_time,&start_time);
+	log->status.flush_time = add_times(&log->status.flush_time,
+					&end_time);
+	end_time.tv_usec = end_time.tv_usec/1000;
+	end_time.tv_usec += end_time.tv_sec*1000;
+	log->status.last_flush_time = end_time.tv_usec;
+	enter_histogram(end_time.tv_usec,log->status.flush_times,
+			flush_times_vec,flush_times_len);
+    }
+    return retval;
+}
+/* exported flush routine */
+rvm_return_t rvm_flush()
+    {
+    rvm_return_t    retval;
+
+    /* do application interface checks */
+    if (bad_init()) return RVM_EINIT;
+    if (default_log == NULL) return RVM_ELOG;
+
+    /* flush the queues */
+    if ((retval=flush_log(default_log,
+                          &default_log->status.n_rvm_flush))
+        != RVM_SUCCESS) return retval;
+
+    return RVM_SUCCESS;
+    }
+/* special log entries enqueuing routine */
+rvm_return_t queue_special(log,special)
+    log_t           *log;               /* log descriptor */
+    log_special_t   *special;           /* special entry descriptor */
+    {
+
+    /* queue  request for immediate flush */
+    CRITICAL(log->special_list_lock,
+             (void)move_list_entry(NULL,&log->special_list,
+                                   &special->links));
+
+    return RVM_SUCCESS;
+    }
diff --git a/rvm/rvm_logrecovr.c b/rvm/rvm_logrecovr.c
new file mode 100644
index 0000000..cfd8a95
--- /dev/null
+++ b/rvm/rvm_logrecovr.c
@@ -0,0 +1,2901 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                       RVM log recovery support
+*
+*/
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <sys/file.h>
+#include <sys/time.h>
+#include <errno.h>
+#include "rvm_private.h"
+
+#ifdef RVM_LOG_TAIL_BUG
+#include <rvmtesting.h>
+extern unsigned long *ClobberAddress;
+#endif /* RVM_LOG_TAIL_BUG */
+
+/* global variables */
+
+extern log_t        *default_log;       /* default log descriptor ptr */
+extern list_entry_t seg_root;           /* segment list */
+extern rw_lock_t    seg_root_lock;      /* segment list lock */
+extern rvm_bool_t   rvm_utlsw;          /* true if running in rvmutl */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+
+rvm_bool_t          rvm_no_yield = rvm_false; /* inhibit yields in recovery */
+rvm_length_t        rvm_num_nodes;      /* number of nodes in change tree */
+rvm_length_t        rvm_max_depth;      /* maximum depth of change tree */
+
+chk_vec_t           *rvm_chk_vec = NULL; /* monitor range vector */
+rvm_length_t        rvm_chk_len = 0;    /* length of monitor range vector */
+rvm_monitor_call_t  *rvm_monitor = NULL; /* call-back function ptr */
+rvm_signal_call_t   *rvm_chk_sigint;    /* SIGINT test call (rvmutl only) */
+rvm_length_t        truncation_times_vec[truncation_times_len]
+                                         = {truncation_times_dist};
+rvm_bool_t          rvm_no_update;      /* no segment or log update if true */
+rvm_bool_t          rvm_replay;         /* is replay if true */
+rvm_bool_t          rvm_chk_sum;        /* force checksumming of all records */
+rvm_bool_t          rvm_shadow_buf;     /* use shadow buffer */
+
+/* macros & locals */
+
+#ifndef ZERO
+#define ZERO 0
+#else
+#endif
+
+/*static rvm_length_t     nv_local_max = NV_LOCAL_MAX;*/
+static struct timeval   trunc_start_time;
+static rvm_length_t     last_tree_build_time;
+static rvm_length_t     last_tree_apply_time;
+
+#define NODES_PER_YIELD 1000000
+static rvm_length_t num_nodes = NODES_PER_YIELD;
+/* test if modification range will change monitored addresses */
+static void monitor_vmaddr(nv_addr,nv_len,nv_data,nv_offset,rec_hdr,msg)
+    char            *nv_addr;           /* vm address */
+    rvm_length_t    nv_len;             /* length of vm range */
+    char            *nv_data;           /* nv data in vm */
+    rvm_offset_t    *nv_offset;         /* offset of data in log */
+    rec_hdr_t       *rec_hdr;           /* ptr to record header if not null */
+    char            *msg;               /* invocation message */
+    {
+    rvm_length_t    last_chk_addr;
+    rvm_length_t    last_nv_addr;
+    rvm_length_t    i;
+
+    /* check monitored ranges for specified range */
+    for (i=0; i < rvm_chk_len; i++)
+        {
+        if (rvm_chk_sigint != NULL)
+            if ((*rvm_chk_sigint)(NULL)) return; /* test for interrupt */
+
+        last_chk_addr = (rvm_length_t)RVM_ADD_LENGTH_TO_ADDR(
+                         rvm_chk_vec[i].vmaddr,rvm_chk_vec[i].length);
+        last_nv_addr =
+            (rvm_length_t)RVM_ADD_LENGTH_TO_ADDR(nv_addr,nv_len);
+        
+        if ((((rvm_length_t)rvm_chk_vec[i].vmaddr
+              >= (rvm_length_t)nv_addr)
+             && ((rvm_length_t)rvm_chk_vec[i].vmaddr < last_nv_addr))
+            ||  ((last_chk_addr > (rvm_length_t)nv_addr)
+             && (last_chk_addr < last_nv_addr))
+            )
+
+            /* found modification, call print support */
+            if (nv_data != NULL)        /* check bytes offset */
+                nv_data = RVM_ADD_LENGTH_TO_ADDR(nv_data,
+                              BYTE_SKEW(nv_addr));
+            (*rvm_monitor)((rvm_length_t)nv_addr,nv_len,nv_data,
+                           nv_offset,rec_hdr,i,msg);
+        }
+
+    return;
+    }
+/* allocate log recovery buffers */
+char                *tst_buf;           /* debug temp */
+rvm_return_t alloc_log_buf(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+
+    if ((log_buf->buf=page_alloc(log_buf->length)) == NULL)
+        return RVM_ENO_MEMORY;
+#ifdef SPECIAL_DEBUG
+    if ((log_buf->shadow_buf=page_alloc(log_buf->length)) == NULL)
+        return RVM_ENO_MEMORY;
+    if ((tst_buf=page_alloc(log_buf->length)) == NULL)
+        return RVM_ENO_MEMORY;
+#endif /* SPECIAL_DEBUG */
+    log_buf->buf_len = RVM_MK_OFFSET(0,log_buf->length);
+
+    if ((log_buf->aux_buf=page_alloc(log_buf->aux_length)) == NULL)
+        return RVM_ENO_MEMORY;
+
+    /* write-protect the buffers */
+/* I've taken out the mach-specific code, but it might be interesting to
+ * implement this feature on other systems using mprotect. Therefore I've
+ * `retained' the essence of the original code in this comment -- JH
+ *
+ * MACH_RVM_PROTECT
+ *
+ * protect(log_buf->buf,        log_buf->length,     FALSE, VM_PROT_READ);
+ *
+ * #ifdef SPECIAL_DEBUG
+ * protect(log_buf->shadow_buf, log_buf->length,     FALSE, VM_PROT_READ);
+ * protect(tst_buf,             log_buf->length,     FALSE, VM_PROT_READ);
+ * #endif SPECIAL_DEBUG
+ *
+ * protect(log_buf->aux_buf,    log_buf->aux_length, FALSE, VM_PROT_READ);
+ */
+
+    return RVM_SUCCESS;
+    }
+
+/* free log recovery buffer */
+void free_log_buf(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+
+    if (log_buf->buf != NULL)
+        {
+        page_free(log_buf->buf,log_buf->length);
+        log_buf->buf = NULL;
+        log_buf->length = 0;
+        RVM_ZERO_OFFSET(log_buf->buf_len);
+        log_buf->ptr = -1;
+        }
+
+    if (log_buf->aux_buf != NULL)
+        {
+        page_free(log_buf->aux_buf,log_buf->aux_length);
+        log_buf->aux_buf = NULL;
+        log_buf->aux_length = 0;
+        }
+    }
+/* init log buffer with desired offset data from log */
+rvm_return_t init_buffer(log,offset,direction,synch)
+    log_t           *log;               /* log descriptor */
+    rvm_offset_t    *offset;            /* offset in log to load */
+    rvm_bool_t      direction;          /* true ==> forward */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_length_t    length;             /* length of buffer */
+    rvm_offset_t    read_len;           /* read length calculation temp */
+    rvm_return_t    retval = RVM_SUCCESS; /* return value */
+
+    assert(RVM_OFFSET_GEQ(*offset,log->status.log_start));
+    assert(RVM_OFFSET_LEQ(*offset,log->dev.num_bytes));
+    assert(log->trunc_thread == cthread_self());
+
+    /* calculate buffer read length and ptr */
+    log_buf->ptr = OFFSET_TO_SECTOR_INDEX(*offset);
+    if (direction == FORWARD)
+        {                               /* forward */
+        log_buf->offset = CHOP_OFFSET_TO_SECTOR_SIZE(*offset);
+        if (RVM_OFFSET_EQL(log_buf->offset,log->dev.num_bytes))
+            read_len = log->status.log_size;
+        else
+            read_len = RVM_SUB_OFFSETS(log->dev.num_bytes,
+                                       log_buf->offset);
+        }
+    else
+        {                               /* reverse */
+        log_buf->offset = ROUND_OFFSET_TO_SECTOR_SIZE(*offset);
+        if (RVM_OFFSET_EQL(log_buf->offset,log->status.log_start))
+            log_buf->offset = log->dev.num_bytes;
+        if (RVM_OFFSET_EQL(log_buf->offset,log->dev.num_bytes))
+            read_len = log->status.log_size;
+        else
+            read_len = RVM_SUB_OFFSETS(log_buf->offset,
+                                       log->status.log_start);
+        }
+
+    /* get actual length to read */
+    if (RVM_OFFSET_GTR(read_len,log_buf->buf_len))
+        length = log_buf->length;
+    else
+        length = RVM_OFFSET_TO_LENGTH(read_len);
+    /* set offset of read for reverse fill */
+    if (direction == REVERSE)
+        {
+        log_buf->offset = RVM_SUB_LENGTH_FROM_OFFSET(log_buf->offset,
+                                                     length);
+        if (log_buf->ptr == 0)
+            log_buf->ptr = length;
+        else
+            log_buf->ptr += (length-SECTOR_SIZE);
+        }
+
+    /* lock device & allow swap if necessary */
+    if (synch)
+        {
+        if (!rvm_no_yield) cthread_yield();
+        assert(log->trunc_thread == cthread_self());
+        mutex_lock(&log->dev_lock); /* begin dev_lock crit sec */
+        assert(log->trunc_thread == cthread_self());
+        }
+
+    /* allow write to buffer */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->buf, log_buf->length, FALSE,
+ *         VM_PROT_WRITE | VM_PROT_READ);
+ */
+
+    /* read data from log device */
+    if ((log_buf->r_length=read_dev(&log->dev,&log_buf->offset,
+                                   log_buf->buf,length)) < 0)
+        {
+        retval = RVM_EIO;               /* i/o error */
+        log_buf->r_length = 0;          /* buffer invalid */
+        }
+    assert(log->trunc_thread == cthread_self());
+
+    /* write protect buffer & unlock */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->buf, log_buf->length, FALSE, VM_PROT_READ);
+ *
+ * #ifdef SPECIAL_DEBUG
+ * / * re-read into shadow buffer & compare * /
+ * if (rvm_shadow_buf)
+ * {
+ *     ret = vm_protect(task_self_,(vm_address_t)(log_buf->shadow_buf),
+ *                      (vm_size_t)(log_buf->length),FALSE,
+ *                      VM_PROT_WRITE | VM_PROT_READ);
+ *     assert(ret == KERN_SUCCESS);
+ *     if ((r_length=read_dev(&log->dev,&log_buf->offset,
+ *                            log_buf->shadow_buf,length)) < 0)
+ *     {
+ *         retval = RVM_EIO;               / * i/o error * /
+ *         assert(rvm_false);
+ *     }
+ *     assert(r_length == length);
+ *     assert(r_length == log_buf->r_length);
+ *     ret = vm_protect(task_self_,(vm_address_t)(log_buf->shadow_buf),
+ *                      (vm_size_t)(log_buf->length),FALSE,VM_PROT_READ);
+ *     assert(ret == KERN_SUCCESS);
+ *     assert(memcmp(log_buf->buf,log_buf->shadow_buf,length) == 0);
+ * }
+ * #endif SPECIAL_DEBUG
+ */
+
+    if (synch)
+        mutex_unlock(&log->dev_lock);   /* end dev_lock crit sec */
+    assert(log->trunc_thread == cthread_self());
+
+    return retval;
+    }
+/* refill buffer in scan direction */
+static rvm_return_t refill_buffer(log,direction,synch)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      direction;          /* true ==> forward */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_offset_t    offset;             /* new buffer offset temp */
+
+    /* compute new offset for buffer fill */
+    offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,log_buf->ptr);
+
+    /* fill the buffer */
+    return init_buffer(log,&offset,direction,synch);
+    }
+/* compare buf & shadow buf from gdb */
+#ifdef SPECIAL_DEBUG
+int log_buf_cmp(disp)
+    int             disp;
+    {
+    log_buf_t       *log_buf = &default_log->log_buf;
+    int             i;
+
+    if (disp < 0) disp = 0;
+    for (i=disp;i<log_buf->r_length;i++)
+        if (log_buf->buf[i] != log_buf->shadow_buf[i])
+            return i;
+
+    return -1;
+    }
+
+/* compare with disk */
+int disk_buf_cmp(buf,disp)
+    char            *buf;
+    int             disp;
+    {
+    log_buf_t       *log_buf = &default_log->log_buf;
+    int             i;
+    int             r_length;
+
+    /* allow write to buffer */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->buf, log_buf->length, FALSE, VM_PROT_WRITE | VM_PROT_READ);
+ */
+
+    /* read buffer from log */
+    if ((r_length=read_dev(&default_log->dev,&log_buf->offset,
+                           tst_buf,log_buf->r_length)) < 0)
+        assert(rvm_false);          /* i/o error */
+    assert(r_length == log_buf->r_length);
+
+    /* re-protect buffer */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->buf, log_buf->length, FALSE, VM_PROT_READ);
+ */
+
+    /* compare results */
+    if (disp < 0) disp = 0;
+    for (i=disp;i<log_buf->r_length;i++)
+        if (buf[i] != tst_buf[i])
+            return i;
+
+    return -1;
+    }
+#endif /* SPECIAL_DEBUG */
+/* locate byte in buffer via gdb */
+int find_byte(chr,buf,disp,max_len)
+    char            chr;
+    char            *buf;
+    int             disp;
+    int             max_len;
+    {
+    int             i;
+
+    if (disp < 0) disp = 0;
+    for (i=disp;i<max_len;i++)
+        if (chr == buf[i])
+            return i;
+
+    return -1;
+    }
+
+/* locate word in buffer via gdb */
+int find_word(wrd,buf,disp,max_len)
+    rvm_length_t    wrd;
+    rvm_length_t    *buf;
+    int             disp;
+    int             max_len;
+    {
+    int             i;
+
+    if (disp < 0) disp = 0;
+    for (i=disp/sizeof(rvm_length_t);i<max_len/sizeof(rvm_length_t);i++)
+        if (wrd == buf[i])
+            return i;
+
+    return -1;
+    }
+
+/* find word in log buffer via gdb */
+int find_buf_word(wrd,disp)
+    rvm_length_t    wrd;
+    int             disp;
+    {
+    log_buf_t       *log_buf = &default_log->log_buf;
+
+    return find_word(wrd, (rvm_length_t *)log_buf->buf,disp,log_buf->r_length);
+    }
+/* load log auxillary buffer */
+rvm_return_t load_aux_buf(log,log_offset,length,aux_ptr,
+                                 data_len,synch,pre_load)
+    log_t           *log;               /* log descriptor */
+    rvm_offset_t    *log_offset;        /* buffer read offset */
+    rvm_length_t    length;             /* data length wanted */
+    rvm_length_t    *aux_ptr;           /* ptr to aux. buf offset */
+    rvm_length_t    *data_len;          /* ptr to actual data length read */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    rvm_bool_t      pre_load;           /* permit pre-loading of range */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_offset_t    high_offset;        /* end of read area */
+    rvm_length_t    read_len;           /* buffer read length */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    assert(log->trunc_thread == cthread_self());
+
+    /* check offset */
+    if (RVM_OFFSET_GTR(*log_offset,log->dev.num_bytes))
+        {
+        *aux_ptr = -1;                  /* out of bounds -- partial record */
+        return RVM_SUCCESS;
+        }
+
+    /* see if request is already in buffer */
+    high_offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->aux_offset,
+                                           log_buf->aux_rlength);
+    if ((RVM_OFFSET_GEQ(*log_offset,log_buf->aux_offset))
+        && (RVM_OFFSET_LSS(*log_offset,high_offset)))
+        {
+        /* yes, have at least some of the data so report how much */
+        *aux_ptr = RVM_OFFSET_TO_LENGTH(
+                     RVM_SUB_OFFSETS(*log_offset,log_buf->aux_offset));
+        read_len = RVM_OFFSET_TO_LENGTH(
+                       RVM_SUB_OFFSETS(high_offset,*log_offset));
+        if (read_len < length)
+            *data_len = read_len;
+        else
+            *data_len = length;
+        return RVM_SUCCESS;
+        }
+
+    /* if less than sector requested, see if pre-load permitted */
+    if (pre_load && (length < SECTOR_SIZE))
+        read_len = log_buf->aux_length; /* yes, fill buffer */
+    else
+        read_len = length;              /* no, just do what requested */
+    /* determine length and offset for log read */
+    log_buf->aux_offset = CHOP_OFFSET_TO_SECTOR_SIZE(*log_offset);
+    high_offset = RVM_ADD_LENGTH_TO_OFFSET(*log_offset,read_len);
+    high_offset = ROUND_OFFSET_TO_SECTOR_SIZE(high_offset);
+    if (RVM_OFFSET_GTR(high_offset,log->dev.num_bytes))
+        high_offset = log->dev.num_bytes; /* don't read past end of log */
+
+    /* report actual length read and ptr into buffer */
+    read_len = RVM_OFFSET_TO_LENGTH(
+                RVM_SUB_OFFSETS(high_offset,log_buf->aux_offset));
+    *aux_ptr = OFFSET_TO_SECTOR_INDEX(*log_offset);
+    if (read_len > log_buf->aux_length)
+        {
+        if ((read_len >= length)
+            && (length <= (log_buf->aux_length-SECTOR_SIZE)))
+            *data_len = length;
+        else
+            *data_len = log_buf->aux_length - *aux_ptr;
+        read_len = log_buf->aux_length;
+        }
+    else
+        *data_len = length;
+    
+    /* lock device and allow swap if necessary */
+    if (synch) 
+        {
+        if (!rvm_no_yield) cthread_yield(); /* allow swap now */
+        assert(log->trunc_thread == cthread_self());
+        mutex_lock(&log->dev_lock); /* begin dev_lock crit sec */
+        assert(log->trunc_thread == cthread_self());
+        }
+
+    /* allow write to buffer */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->aux_buf, log_buf->aux_length, FALSE,
+ *         VM_PROT_WRITE | VM_PROT_READ);
+ */
+
+    /* read new value data from log */
+    if ((log_buf->aux_rlength=read_dev(&log->dev,&log_buf->aux_offset,
+                 log_buf->aux_buf,read_len)) < 0)
+        {
+        retval = RVM_EIO;
+        log_buf->aux_rlength = 0;
+        }
+    assert(log->trunc_thread == cthread_self());
+
+    /* write protect buffer & unlock */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->aux_buf, log_buf->aux_length, FALSE, VM_PROT_READ);
+ */
+
+    if (synch)
+        mutex_unlock(&log->dev_lock);   /* end dev_lock crit sec */
+    assert(log->trunc_thread == cthread_self());
+
+    return retval;
+    }
+
+void clear_aux_buf(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+
+    RVM_ZERO_OFFSET(log_buf->aux_offset);
+    log_buf->aux_rlength = 0;
+    }
+/* record header type validation */
+rvm_bool_t chk_hdr_type(rec_hdr)
+    rec_hdr_t       *rec_hdr;           /* generic record header */
+    {
+    switch (rec_hdr->struct_id)
+        {
+      case trans_hdr_id:                /* transaction header */
+        return rvm_true;
+      case log_seg_id:                  /* log segment dictionary entry */
+        return rvm_true;
+      case log_wrap_id:                 /* log wrap-aound marker */
+        return rvm_true;
+      default:                          /* unknown header type */
+        return rvm_false;
+        }
+    }
+
+/* test if record belongs to currently valid part of log */
+rvm_bool_t chk_hdr_currency(log,rec_hdr)
+    log_t           *log;               /* log descriptor */
+    rec_hdr_t       *rec_hdr;           /* generic record header */
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+
+    /* be sure record number makes sense */
+    if ((status->first_rec_num != 0) &&
+        (rec_hdr->rec_num < status->first_rec_num))
+        return rvm_false;               /* obsolete record */
+    
+    /* be sure record written after previous truncation & before this one */
+    if (TIME_LSS(rec_hdr->timestamp,status->prev_trunc)
+        || TIME_GTR(rec_hdr->timestamp,status->last_trunc))
+        return rvm_false;                   /* obsolete record */
+
+    return rvm_true;
+    }
+
+void reset_hdr_chks(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+
+    log_buf->prev_rec_num = 0;
+    ZERO_TIME(log_buf->prev_timestamp);
+    }
+/* test if record is out of sequence in log */
+rvm_bool_t chk_hdr_sequence(log,rec_hdr,direction)
+    log_t           *log;               /* log descriptor */
+    rec_hdr_t       *rec_hdr;           /* generic record header */
+    rvm_bool_t      direction;          /* scan direction */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* recovery buffer descriptor */
+
+    /* check record number closely */
+    if ((log_buf->prev_rec_num != 0) &&
+        (((direction == FORWARD)
+          && (rec_hdr->rec_num != log_buf->prev_rec_num+1))
+         || ((direction == REVERSE)
+             && (rec_hdr->rec_num != log_buf->prev_rec_num-1))))
+        return rvm_false;                   /* sequence error */
+
+    /* check record write time closely */
+    if ((!TIME_EQL_ZERO(log_buf->prev_timestamp)) &&
+        (((direction == FORWARD)
+          && TIME_LSS(rec_hdr->timestamp,log_buf->prev_timestamp))
+         || ((direction == REVERSE)
+             && TIME_GTR(rec_hdr->timestamp,log_buf->prev_timestamp))))
+        return rvm_false;                   /* sequence error */
+
+    return rvm_true;
+    }
+/* record header validation */
+rvm_bool_t chk_hdr(log,rec_hdr,rec_end,direction)
+    log_t           *log;               /* log descriptor */
+    rec_hdr_t       *rec_hdr;           /* generic record header */
+    rec_end_t       *rec_end;           /* generic record end marker */
+    rvm_bool_t      direction;          /* scan direction */
+    {
+
+    /* be sure record type valid */
+    if (!chk_hdr_type(rec_hdr))
+        return rvm_false;
+
+    /* checks for normal operation only */
+    if (!rvm_utlsw)
+        {
+        /* make sure record current */
+        if (chk_hdr_currency(log,rec_hdr) != rvm_true)
+            return rvm_false;               /* record obsolete */
+
+        /* make sure record in proper sequence */
+        if (chk_hdr_sequence(log,rec_hdr,direction) != rvm_true)
+            return rvm_false;               /* sequence error */
+        }
+
+    /* generic record head/end validation */
+    if ((rec_end != NULL) &&
+        ((rec_end->rec_hdr.struct_id != rec_end_id)
+        || (rec_hdr->struct_id != rec_end->rec_type)
+        || (rec_hdr->rec_num != rec_end->rec_hdr.rec_num)
+        || (rec_hdr->rec_length != rec_end->rec_hdr.rec_length)
+        || (!TIME_EQL(rec_hdr->timestamp,rec_end->rec_hdr.timestamp))))
+        return rvm_false;
+
+    return rvm_true;
+    }
+/* log record header validation */
+rvm_bool_t validate_hdr(log,rec_hdr,rec_end,direction)
+    log_t           *log;               /* log descriptor */
+    rec_hdr_t       *rec_hdr;           /* generic record header */
+    rec_end_t       *rec_end;           /* generic record end marker */
+    rvm_bool_t      direction;          /* scan direction */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* recovery buffer descriptor */
+
+    /* clear sequence checking hide-a-ways if direction reversed */
+    if (direction != log_buf->prev_direction)
+        reset_hdr_chks(log);
+
+    /* do basic record header checks */
+    if (!chk_hdr(log,rec_hdr,rec_end,direction))
+        return rvm_false;               /* header invalid */
+
+    /* type-specific validation */
+    switch (rec_hdr->struct_id)
+        {
+      case trans_hdr_id:                /* transaction header */
+        break;
+      case log_seg_id:                  /* log segment dictionary entry */
+        break;
+      case log_wrap_id:                 /* log wrap-aound marker */
+        goto exit;
+      default:                          /* unknown/improper header type */
+        return rvm_false;
+        }
+
+    /* update buffer ptr and previous record state */
+    if (direction == FORWARD)           /* forward, return header position */
+        log_buf->ptr = (long)rec_hdr - (long)log_buf->buf;
+    else                                /* reverse, return end marker pos. */
+        log_buf->ptr = (long)rec_end - (long)log_buf->buf;
+
+  exit:
+    log_buf->prev_rec_num = rec_hdr->rec_num;
+    log_buf->prev_timestamp = rec_hdr->timestamp;
+    log_buf->prev_direction = direction;
+
+    return rvm_true;
+    }
+/* get next new value range by forward scan of transaction record
+   ptr points to next range header
+   exits with as much of range in buffer as will fit */
+rvm_return_t scan_nv_forward(log,synch)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_offset_t    offset;             /* offset calculation temp */
+    rec_hdr_t       *rec_hdr;           /* temporary cast for record header */
+    rvm_return_t    retval;             /* return value */
+ 
+    /* see if new header is entirely within buffer */
+    if ((log_buf->ptr+sizeof(rec_hdr_t)) >= log_buf->r_length)
+        {
+        /* no, refill buffer */
+        offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                                          log_buf->ptr);
+        if ((retval=init_buffer(log,&offset,FORWARD,synch))
+            != RVM_SUCCESS) return retval;
+        }
+
+    /* check header */
+    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+    switch (rec_hdr->struct_id)
+        {
+      case nv_range_id:     break;
+      case rec_end_id:      return RVM_SUCCESS;
+
+      default:              return RVM_SUCCESS; /* need better reporting */
+        }
+
+    /* get whole range in buffer */
+    if ((log_buf->ptr+rec_hdr->rec_length) > log_buf->r_length)
+        {
+        if ((retval=refill_buffer(log,FORWARD,synch))
+            != RVM_SUCCESS) return retval;
+        }
+ 
+    return RVM_SUCCESS;
+    }
+/* get previous new value range by reverse scan of transaction record
+   ptr points to previous range header; exits with range in buffer */
+rvm_return_t scan_nv_reverse(log,synch)
+    log_t          *log;                /* log descriptor */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rec_hdr_t       *rec_hdr;           /* temporary cast for record header */
+    long            len=0;                /* back displacement to prev. hdr */
+    rvm_offset_t    offset;             /* offset calculation temp */
+    rvm_return_t    retval;             /* return value */
+
+    /* get new header position */
+    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+    switch (rec_hdr->struct_id)
+    {
+      case rec_end_id:
+        len = ((rec_end_t *)rec_hdr)->sub_rec_len;
+        break;
+
+      case nv_range_id:
+        len = ((nv_range_t *)rec_hdr)->sub_rec_len;
+        break;
+
+      default:
+        assert(rvm_false);               /* trouble -- log damage? */
+    }
+
+    /* see if new header is entirely within buffer */
+    if ((log_buf->ptr-len) < 0)
+        {
+        /* no, refill buffer according to length of data */
+        if ((len-sizeof(nv_range_t)) <= NV_LOCAL_MAX)
+            {                           /* small, get data into buffer */
+            if ((retval=refill_buffer(log,REVERSE,synch))
+                != RVM_SUCCESS) return retval;
+            log_buf->ptr -= len;
+            }
+        else
+            {                           /* large, skip data for now */
+            offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                         (log_buf->ptr+sizeof(nv_range_t)));
+            offset = RVM_SUB_LENGTH_FROM_OFFSET(offset,len);
+            if ((retval=init_buffer(log,&offset,REVERSE,synch))
+                != RVM_SUCCESS) return retval;
+            log_buf->ptr -= sizeof(nv_range_t);           
+            }
+        }
+    else log_buf->ptr -= len;
+    /* exit pointing to new header */
+    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+    if (rec_hdr->struct_id == trans_hdr_id)
+        return RVM_SUCCESS;
+    assert(rec_hdr->struct_id == nv_range_id);
+ 
+    return RVM_SUCCESS;
+    }
+/* validate record in buffer in forward scan */
+rvm_return_t validate_rec_forward(log,synch)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rec_hdr_t       *rec_hdr;           /* temporary cast for next record hdr */
+    rec_end_t       *rec_end = NULL;    /* temporary cast for record end */
+    rvm_offset_t    end_offset;         /* temporary for caluculating end */
+    rvm_return_t    retval;
+    long            tmp_ptr;
+    rvm_length_t    tmp_len;
+
+    /* see if next header is entirely within buffer */
+    if ((log_buf->ptr + MAX_HDR_SIZE) > log_buf->r_length)
+        {
+        /* no, re-init buffer */
+        end_offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                                              log_buf->ptr); 
+       if ((retval=init_buffer(log,&end_offset,FORWARD,synch))
+           != RVM_SUCCESS) return retval;
+        }
+
+    /* check header type */
+    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+    if (rec_hdr->struct_id == log_wrap_id)
+        goto validate;                  /* skip rec_end stuff for wrap */
+    if (!chk_hdr(log,rec_hdr,NULL,FORWARD))
+        goto no_record;                 /* no next record */
+
+    /* see if record will fit in buffer */
+    if ((ROUND_TO_SECTOR_SIZE(rec_hdr->rec_length+sizeof(rec_end_t))
+         + SECTOR_SIZE)
+        <= log_buf->length)
+        {
+        /* yes, get whole record in buffer */
+        if ((log_buf->ptr+rec_hdr->rec_length+sizeof(rec_end_t))
+            > log_buf->length)
+            {
+            /* refill buffer */
+            if ((retval=refill_buffer(log,FORWARD,synch))
+                != RVM_SUCCESS) return retval;
+            rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+            }
+        tmp_ptr = log_buf->ptr + rec_hdr->rec_length;
+        rec_end = (rec_end_t *)&log_buf->buf[tmp_ptr];
+        }
+    else
+        {
+        /* no, won't fit -- read rec_end into aux buffer for validation */
+        end_offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                         log_buf->ptr+rec_hdr->rec_length);
+
+        /* check offset alignment to see if rec_hdr is trash */
+        tmp_ptr = RVM_OFFSET_TO_LENGTH(end_offset);
+        if (tmp_ptr != CHOP_TO_LENGTH(tmp_ptr))
+            goto no_record;             /* end marker alignment wrong */
+        retval = load_aux_buf(log, &end_offset, sizeof(rec_end_t),
+			      &tmp_ptr, &tmp_len, synch, rvm_false);
+        if (retval != RVM_SUCCESS) return retval;
+        if (tmp_ptr == -1)
+            goto no_record;             /* record end not available */
+        rec_end = (rec_end_t *)&log_buf->aux_buf[tmp_ptr];
+        }
+
+    /* validate whole record now that end is available */
+  validate:
+    if (validate_hdr(log,rec_hdr,rec_end,FORWARD))
+        return RVM_SUCCESS;
+
+  no_record:                            /* no next record */
+    log_buf->ptr = -1;
+    return RVM_SUCCESS;
+    }
+/* scan forward from present position at a record structure
+   returns updated offset indexed by ptr; -1 ==> no next rec. */
+rvm_return_t scan_forward(log,synch)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rec_hdr_t       *rec_hdr;           /* cast for next record hdr */
+    rvm_return_t    retval;
+
+    assert(log_buf->ptr != -1);         /* invalid position */
+    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+    switch (rec_hdr->struct_id)
+        {
+      case trans_hdr_id: case log_seg_id:
+        log_buf->ptr += (rec_hdr->rec_length+sizeof(rec_end_t));
+        break;
+      case rec_end_id:
+        log_buf->ptr += sizeof(rec_end_t);
+        break;
+      case nv_range_id:                 /* scan past remaining ranges */
+        DO_FOREVER
+            {
+            if ((retval=scan_nv_forward(log,synch)) != RVM_SUCCESS)
+                return retval;
+            rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+            switch (rec_hdr->struct_id)
+                {
+              case nv_range_id:
+                log_buf->ptr += rec_hdr->rec_length;
+                break;
+              case rec_end_id:
+                log_buf->ptr += sizeof(rec_end_t);
+                goto trans_done;
+              default:                  /* validate_rec_forward will handle */
+                goto trans_done;
+                }
+            }
+trans_done:
+        break;
+      case log_wrap_id:
+        if ((retval=init_buffer(log,&log->status.log_start,
+                                FORWARD,synch))
+                != RVM_SUCCESS) return retval;
+        break;
+      default:
+        if (rvm_utlsw)
+            {
+            log_buf->ptr = -1;          /* utility can handle unknown records */
+            return RVM_SUCCESS;
+            }
+        assert(rvm_false);                  /* unknown record type */
+        }
+
+    /* validate next record */
+    return validate_rec_forward(log,synch);
+    }
+/* scan for wrap marker */
+rvm_return_t scan_wrap_reverse(log,synch)
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    log_t           *log;               /* log descriptor */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rec_hdr_t       *rec_hdr;           /* temporary cast for record header */
+    log_wrap_t      *log_wrap;          /* temporary cast for wrap marker */
+    long            tmp_ptr;            /* temporary buffer ptr */
+    rvm_return_t    retval;
+
+    /* load last sectors of log */
+    if ((retval=init_buffer(log,&log->dev.num_bytes,
+                            REVERSE,synch))
+        != RVM_SUCCESS) return retval;
+
+    /* scan for wrap marker */
+    /* for the purpose of locating the wrap marker, we use the (duplicated)
+       struct_id2 which, while positions at the end of the record, guarantees
+       that we must interpret it first, otherwise, we may possibly 
+       mis-interpret other field of the record to have a struct_id of 
+       log_wrap_id ! */
+    for (tmp_ptr = (log_buf->ptr - sizeof(log_wrap_t));
+         tmp_ptr >= 0; tmp_ptr -= sizeof(rvm_length_t))
+        {
+        log_wrap = (log_wrap_t *)&log_buf->buf[tmp_ptr];
+        if (log_wrap->struct_id2 == log_wrap_id) 
+            {
+		assert( (log_wrap->rec_hdr.struct_id==log_wrap_id) || rvm_utlsw );
+		/* XXXX fix this */ 
+#if 0
+		if (!((log_wrap->struct_id == log_wrap_id) || rvm_utlsw)) {
+		    printf("not true!\n");
+		    assert(0);
+		}
+#endif
+	    break;
+            }
+        }
+
+    /* validate header if tmp_ptr legit */
+    if ((tmp_ptr >= 0) && (tmp_ptr < log_buf->r_length))
+        {
+	    log_buf->ptr = tmp_ptr;
+	    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+	    if (!validate_hdr(log,rec_hdr,NULL,REVERSE))
+		log_buf->ptr = -1;
+        }
+    else
+        /* no wrap marker found */
+        if (rvm_utlsw)
+            log_buf->ptr = -1;          /* utility can deal with it */
+        else assert(rvm_false);
+
+    return RVM_SUCCESS;
+    }
+/* validate current record in buffer in reverse scan */
+rvm_return_t validate_rec_reverse(log,synch)
+     rvm_bool_t      synch;              /* true ==> synchronization required */
+     log_t           *log;               /* log descriptor */
+{
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    log_status_t    *status = &log->status; /* status area */
+    rec_end_t       *rec_end = NULL;    /* temporary cast for record end */
+    rec_hdr_t       *rec_hdr;           /* temporary cast for record header */
+    long            tmp_ptr;            /* temporary buffer ptr */
+    rvm_length_t    tmp_len;
+    rvm_offset_t    offset;             /* temp for offset calculations */
+    rvm_return_t    retval;
+
+    /* get previous end marker into buffer */
+    if ((long)(log_buf->ptr-sizeof(rec_end_t)) < 0)
+        {
+	    offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+					      log_buf->ptr);
+	    if (RVM_OFFSET_EQL(offset,status->log_start))
+		{
+            retval=scan_wrap_reverse(log,synch);
+            return retval;              /* exit pointing to wrap marker */
+            }
+        else
+            {
+            if ((retval=init_buffer(log,&offset,REVERSE,synch))
+                != RVM_SUCCESS) return retval;
+            }
+        }
+    log_buf->ptr -= sizeof(rec_end_t);
+
+    /* check new end marker */
+    rec_end = (rec_end_t *)&log_buf->buf[log_buf->ptr];
+    if (rec_end->rec_hdr.struct_id != rec_end_id)
+        goto no_record;             /* no next record */
+    /* see if record will fit in buffer */
+    if ((ROUND_TO_SECTOR_SIZE(rec_end->rec_hdr.rec_length+sizeof(rec_end_t))
+        + SECTOR_SIZE) <= log_buf->length)
+        {
+        /* yes, get whole record in buffer */
+        if ((long)(log_buf->ptr - rec_end->rec_hdr.rec_length) < 0)
+            {
+            /* refill buffer (be sure end marker is included) */
+            log_buf->ptr += sizeof(rec_end_t);
+            if ((retval=refill_buffer(log,REVERSE,synch))
+                != RVM_SUCCESS) return retval;
+            log_buf->ptr -= sizeof(rec_end_t);
+            rec_end = (rec_end_t *)&log_buf->buf[log_buf->ptr];
+            }
+        tmp_ptr = log_buf->ptr - rec_end->rec_hdr.rec_length;
+        rec_hdr = (rec_hdr_t *)&log_buf->buf[tmp_ptr];
+        }
+    else
+        {
+        /* no, save rec_end for validation & get header in aux. buffer */
+        offset = RVM_SUB_LENGTH_FROM_OFFSET(log_buf->offset,
+                                            rec_end->rec_hdr.rec_length);
+        offset = RVM_ADD_LENGTH_TO_OFFSET(offset,log_buf->ptr);
+
+        /* check offset alignment to see if rec_end is trash */
+        tmp_ptr = RVM_OFFSET_TO_LENGTH(offset);
+        if (tmp_ptr != CHOP_TO_LENGTH(tmp_ptr))
+            goto no_record;             /* header alignment wrong */
+        retval = load_aux_buf(log, &offset, MAX_HDR_SIZE, &tmp_ptr, &tmp_len,
+                              synch, rvm_false);
+        if (retval != RVM_SUCCESS) return retval;
+        if (tmp_ptr == -1)
+            goto no_record;             /* record header not available */
+        rec_hdr = (rec_hdr_t *)&log_buf->aux_buf[tmp_ptr];
+        }
+
+    /* validate whole record now that header is available */
+    if (validate_hdr(log,rec_hdr,rec_end,REVERSE))
+        return RVM_SUCCESS;
+
+no_record:
+    log_buf->ptr = -1;               /* no next record */
+    return RVM_SUCCESS;
+    }
+/* scan backward from present position at a record structure
+   returns index of offset in ptr; -1 ==> no next rec. */
+rvm_return_t scan_reverse(log,synch)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    log_status_t    *status = &log->status; /* status area */
+    rec_hdr_t       *rec_hdr;           /* temporary cast for record header */
+    rvm_offset_t    offset;             /* temp for offset calculations */
+    rvm_return_t    retval;
+
+    assert(log_buf->ptr != -1);         /* can't reposition from this! */
+
+    /* test if scan starting from tail */
+    offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,log_buf->ptr);
+    if (RVM_OFFSET_EQL(offset,status->prev_log_tail)
+        || (rvm_utlsw && RVM_OFFSET_EQL(offset,status->log_tail)))
+        return validate_rec_reverse(log,synch);
+
+    /* test if at start of log & must wrap around */
+    if ((RVM_OFFSET_EQL(log_buf->offset,status->log_start)) &&
+        (log_buf->ptr == 0))
+        {
+        if ((retval=scan_wrap_reverse(log,synch)) != RVM_SUCCESS)
+            return retval;
+        return RVM_SUCCESS;             /* exit pointing to wrap marker */
+        }
+
+    /* move to previous record end marker */
+    rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+    switch (rec_hdr->struct_id)
+        {
+      case trans_hdr_id: case log_seg_id:
+      case log_wrap_id:
+        break;
+      case rec_end_id:
+        if (((rec_end_t *)rec_hdr)->rec_type != trans_hdr_id)
+            {                           /* record is always in buffer */
+            log_buf->ptr -= rec_hdr->rec_length;
+            break;
+            }
+      case nv_range_id:                 /* scan past remaining ranges */
+        DO_FOREVER
+            {
+            if ((retval=scan_nv_reverse(log,synch)) != RVM_SUCCESS)
+                return retval;
+            rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+            if (rec_hdr->struct_id == trans_hdr_id)
+                break;
+            }
+        break;
+      default:
+            {
+            if (rvm_utlsw)
+                {
+                log_buf->ptr = -1;      /* utl can recover */
+                return RVM_SUCCESS;
+                }
+            assert(rvm_false);          /* not at recognizable point in log */
+            }
+        }
+
+    /* validate new record and set log_buf->ptr */
+    return validate_rec_reverse(log,synch);
+    }
+/* Recovery: phase 1 -- locate current log tail from last status block
+     location */
+
+/* log_wrap status update for tail location */
+static void set_wrap_status(status,rec_hdr)
+    log_status_t    *status;            /* status descriptor */
+    rec_hdr_t       *rec_hdr;           /* current record scanned in buffer */
+    {
+    status->wrap_time = rec_hdr->timestamp;
+    status->n_special++;
+    status->tot_wrap++;
+    }
+/* range checksum computation & check */
+static rvm_return_t range_chk_sum(log,nv,chk_val,synch)
+    log_t           *log;               /* log descriptor */
+    nv_range_t      *nv;                /* range header */
+    rvm_bool_t      *chk_val;           /* result [out] */
+    rvm_bool_t      synch;              /* true ==> synchronization required */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_length_t    nv_chk_sum;         /* nv's check sum */
+    rvm_length_t    chk_sum_temp = 0;   /* check sum temp */
+    rvm_length_t    nv_length;          /* actual length of data */
+    rvm_length_t    chk_length;         /* length of check summed range */
+    rvm_length_t    align_skew;         /* initial alignment skew */
+    rvm_return_t    retval;             /* return value */
+
+    (*chk_val) = rvm_false;
+    nv_chk_sum = nv->chk_sum;
+    nv_length = nv->length;
+    align_skew = BYTE_SKEW(RVM_OFFSET_TO_LENGTH(nv->offset));
+    log_buf->ptr += sizeof(nv_range_t);
+
+    /* do checksum over as many buffer loads as needed */
+    DO_FOREVER
+        {
+        chk_length = log_buf->r_length - log_buf->ptr - align_skew;
+        if (chk_length > nv_length) chk_length = nv_length;
+        chk_sum_temp +=
+            chk_sum(&log_buf->buf[log_buf->ptr+align_skew],
+                    chk_length);
+        nv_length -= chk_length;
+        log_buf->ptr += (chk_length+align_skew);
+        if (nv_length == 0) break;  /* done */
+        if ((retval=refill_buffer(log,FORWARD,synch))
+            != RVM_SUCCESS) return retval;
+        align_skew = 0;             /* following buffers have no padding */
+        }
+    log_buf->ptr = ROUND_TO_LENGTH(log_buf->ptr);
+
+    /* report result */
+    if (nv_chk_sum == chk_sum_temp)
+        (*chk_val) = rvm_true;
+
+    return RVM_SUCCESS;
+    }
+/* transaction validation & status update for tail location */
+static rvm_return_t set_trans_status(log,rec_hdr)
+    log_t           *log;               /* log descriptor */
+    rec_hdr_t        *rec_hdr;           /* current trans record in buffer */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    log_status_t    *status = &log->status;   /* status descriptor */
+    trans_hdr_t     trans_hdr;          /* copy of header */
+    long            num_ranges = 0;     /* range scan counter */
+    nv_range_t      *nv;                /* range header */
+    rvm_bool_t      chk_val;            /* checksum test result */
+    rvm_return_t    retval;             /* return value */
+
+    /* keep copy of header to get status if ranges are OK */
+    BCOPY((char *)rec_hdr,(char *)&trans_hdr,sizeof(trans_hdr_t));
+
+    /* scan and check sum all ranges */
+    log_buf->ptr += sizeof(trans_hdr_t);
+    DO_FOREVER
+        {
+        if ((retval=scan_nv_forward(log,NO_SYNCH)) != RVM_SUCCESS)
+            return retval;
+        rec_hdr = (rec_hdr_t *)&(log_buf->buf[log_buf->ptr]);
+        if (rec_hdr->struct_id == rec_end_id)
+            break;                      /* done */
+        if (rec_hdr->struct_id != nv_range_id)
+            goto bad_record;            /* invalid record */
+        nv = (nv_range_t *)rec_hdr;
+        if (trans_hdr.rec_hdr.rec_num != nv->rec_hdr.rec_num)
+            goto bad_record;            /* wrong transaction */
+
+        /* test range's data check sum */
+        if ((retval=range_chk_sum(log,nv,&chk_val,NO_SYNCH))
+            != RVM_SUCCESS) return retval;
+        if (chk_val != rvm_true) goto bad_record; /* check sum failure */
+
+        num_ranges++;
+        }
+    /* be sure all ranges are present */
+    if (num_ranges != trans_hdr.num_ranges)
+        goto bad_record;                /* incomplete */
+
+    /* transaction complete, update status */
+    status->last_uname = trans_hdr.uname;
+    if (trans_hdr.flags & FLUSH_FLAG)
+        status->n_flush_commit++;
+    else status->n_no_flush_commit++;
+    if (((trans_hdr.flags & FIRST_ENTRY_FLAG) != 0)
+        && ((trans_hdr.flags & LAST_ENTRY_FLAG) == 0))
+        status->n_split++;
+    return RVM_SUCCESS;
+
+bad_record:
+    log_buf->ptr = -1;
+    return RVM_SUCCESS;
+    }
+/* Locate tail, update in-memory copy of status block; always reads forward */
+rvm_return_t locate_tail(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_status_t    *status = &log->status;   /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_offset_t    tail;               /* tail offset */
+    rvm_offset_t    temp_tail;          /* tail offset temp */
+    rvm_length_t    last_rec_num = 0;   /* record number of tail record */
+    rec_hdr_t       *rec_hdr;           /* current record scanned in buffer */
+    long            old_ptr;            /* buffer ptr to last record found */
+    struct timeval  save_last_trunc;
+    struct timeval  last_write = status->last_write; /* last write to log */
+    rvm_bool_t      save_rvm_utlsw = rvm_utlsw;
+    rvm_return_t    retval = RVM_SUCCESS; /* return value */
+
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) == ZERO);
+    status->trunc_state |= RVM_TRUNC_FIND_TAIL;
+
+    /* initialize scanner sequence checking state and buffers */
+    rvm_utlsw = rvm_false;
+    reset_hdr_chks(log);
+    clear_aux_buf(log);
+
+    /* if truncation caught in crash, reset head */
+    if (!RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+        {
+        status->log_head = status->prev_log_head;
+        status->last_rec_num = status->next_rec_num-1;
+        }
+
+    /* set temporary timestamp for record validation */
+    save_last_trunc = status->last_trunc;
+    make_uname(&status->last_trunc);
+    if (TIME_GTR(save_last_trunc,status->last_trunc))
+        {                               /* date/time wrong! */
+        retval = RVM_EINTERNAL;
+        rvm_errmsg = ERR_DATE_SKEW;
+        goto err_exit;
+        }
+
+    /* need to update status: init read buffer at head */
+    if ((retval=init_buffer(log,&status->log_head,
+                            FORWARD,NO_SYNCH))
+        != RVM_SUCCESS) goto err_exit;
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) == RVM_TRUNC_FIND_TAIL);
+
+    /* validate 1st record, none ==> log empty */
+    rec_hdr = (rec_hdr_t *)&(log_buf->buf[log_buf->ptr]);
+    if (!validate_hdr(log,rec_hdr,NULL,FORWARD))
+        {
+#ifdef RVM_LOG_TAIL_BUG
+        unprotect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+	assert(RVM_OFFSET_EQL(log_tail_shadow,status->log_tail));
+#endif /* RVM_LOG_TAIL_SHADOW */
+        status->log_tail = status->log_head;
+#ifdef RVM_LOG_TAIL_SHADOW
+	RVM_ASSIGN_OFFSET(log_tail_shadow,status->log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+#ifdef RVM_LOG_TAIL_BUG
+        protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+        clear_log_status(log);
+        goto exit;
+        }
+    /* update status block head info if necessary */
+    if (status->first_rec_num == 0)
+        status->first_rec_num = rec_hdr->rec_num;
+    if (TIME_EQL_ZERO(status->first_write))
+        status->first_write = rec_hdr->timestamp;
+    if (rec_hdr->struct_id == log_wrap_id)
+        status->wrap_time = rec_hdr->timestamp;
+
+    /* locate first transaction, if needed */
+    if (TIME_EQL_ZERO(status->first_uname))
+        do
+            {
+            /* update other status data */
+            rec_hdr = (rec_hdr_t *)&(log_buf->buf[log_buf->ptr]);
+            last_rec_num = rec_hdr->rec_num;
+            status->last_write = rec_hdr->timestamp;
+            if (rec_hdr->struct_id == log_wrap_id)
+                status->wrap_time = rec_hdr->timestamp;
+            
+            if (rec_hdr->struct_id == trans_hdr_id)
+                {                       /* transaction found */
+                status->first_uname = ((trans_hdr_t *)
+                                       rec_hdr)->uname;
+                status->last_uname = ((trans_hdr_t *)
+                                      rec_hdr)->uname;
+                break;
+                }
+            if (rec_hdr->struct_id == log_wrap_id)
+                status->wrap_time = rec_hdr->timestamp;
+            old_ptr = log_buf->ptr;
+            if ((retval=scan_forward(log,NO_SYNCH)) != RVM_SUCCESS)
+                goto err_exit;
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES)
+                   == RVM_TRUNC_FIND_TAIL);
+            if (rvm_chk_sigint != NULL) /* test for interrupt */
+                if ((*rvm_chk_sigint)(NULL)) goto err_exit;
+            }
+            while (log_buf->ptr != -1); /* tail found, no transactions */
+
+    /* re-init scanner sequence checking state since small logs can cause 
+       a few records to be rescanned and re-init read buffer at tail
+    */
+    tail = status->log_tail;
+    reset_hdr_chks(log);
+    if ((retval=init_buffer(log,&tail,FORWARD,NO_SYNCH))
+        != RVM_SUCCESS) goto err_exit;
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) == RVM_TRUNC_FIND_TAIL);
+    /* see if record at tail is valid, scan until bad record found */
+    if ((retval=validate_rec_forward(log,NO_SYNCH)) != RVM_SUCCESS)
+        goto err_exit;
+    DO_FOREVER
+        {
+        if (log_buf->ptr == -1) break; /* tail located */
+
+        /* compute provisional new tail offset, rec_num, timestamp */
+        rec_hdr = (rec_hdr_t *)(&log_buf->buf[log_buf->ptr]);
+        temp_tail = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                        (log_buf->ptr+rec_hdr->rec_length
+                        +sizeof(rec_end_t)));
+        last_rec_num = rec_hdr->rec_num;
+        last_write = rec_hdr->timestamp;
+
+        /* type-specific status data recovery */
+        switch (rec_hdr->struct_id)
+            {
+          case log_wrap_id:
+            set_wrap_status(status,rec_hdr);
+            tail = status->log_start;
+            break;
+
+          case trans_hdr_id:
+            if ((retval=set_trans_status(log,rec_hdr)) != RVM_SUCCESS)
+                goto err_exit;
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES)
+                   == RVM_TRUNC_FIND_TAIL);
+            if (log_buf->ptr != -1)
+                tail = temp_tail;       /* update if trans OK */
+            break;
+
+          case log_seg_id:
+            status->n_special++;
+            tail = temp_tail;
+            break;
+
+          default:  assert(rvm_false);  /* error - should have header */
+            }
+
+        /* scan to next record */
+        if (log_buf->ptr == -1) break; /* tail located */
+        if ((retval=scan_forward(log,NO_SYNCH)) != RVM_SUCCESS)
+            goto err_exit;
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) == RVM_TRUNC_FIND_TAIL);
+        if (rvm_chk_sigint != NULL)     /* test for interrupt */
+            if ((*rvm_chk_sigint)(NULL)) goto err_exit;
+        }
+    /* tail found, update in-memory status */
+#ifdef RVM_LOG_TAIL_BUG
+    unprotect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    assert(RVM_OFFSET_EQL(log_tail_shadow,status->log_tail));
+#endif /* RVM_LOG_TAIL_SHADOW */
+    status->log_tail = tail;
+#ifdef RVM_LOG_TAIL_SHADOW
+	RVM_ASSIGN_OFFSET(log_tail_shadow,status->log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+#ifdef RVM_LOG_TAIL_BUG
+    protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+    status->last_write = last_write;
+    if (RVM_OFFSET_EQL(status->log_head,status->log_tail))
+        clear_log_status(log);          /* log empty */
+    else
+        {                               /* log not empty */
+        status->log_empty = rvm_false;
+
+        if (status->next_rec_num <= last_rec_num)
+            status->next_rec_num = last_rec_num+1;
+        if (status->last_rec_num != last_rec_num)
+            status->last_rec_num = last_rec_num;
+        }
+
+exit:
+    status->valid = rvm_true;
+err_exit:
+    rvm_utlsw = save_rvm_utlsw;
+    status->last_trunc = save_last_trunc;
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) == RVM_TRUNC_FIND_TAIL);
+    return retval;
+    }
+/* add segment short id to dictionary */
+rvm_return_t enter_seg_dict(log,seg_code)
+    log_t           *log;
+    long            seg_code;
+    {
+    seg_dict_t      *seg_dict;
+    long            old_dict_size,new_dict_size;
+
+    /* lengthen seg_dict_vec if necessary */
+    if (log->seg_dict_len < seg_code)
+        {                              
+        new_dict_size = seg_code*sizeof(seg_dict_t);
+        old_dict_size = log->seg_dict_len*sizeof(seg_dict_t);
+        log->seg_dict_vec = (seg_dict_t *)
+            REALLOC((char *)log->seg_dict_vec,new_dict_size);
+        if (log->seg_dict_vec == NULL)
+            return RVM_ENO_MEMORY;
+        (void)BZERO((char *)((long)log->seg_dict_vec+old_dict_size),
+                             new_dict_size-old_dict_size);
+        log->seg_dict_len = seg_code;
+        }
+
+    /* enter in dictionary if not already defined */
+    seg_dict = &log->seg_dict_vec[SEG_DICT_INDEX(seg_code)];
+    if (seg_dict->struct_id != seg_dict_id)
+        {
+        seg_dict->struct_id = seg_dict_id;
+        seg_dict->seg_code = seg_code;
+        seg_dict->seg = NULL;
+        init_tree_root(&seg_dict->mod_tree);
+        (void)dev_init(&seg_dict->dev,NULL);
+        }
+    return RVM_SUCCESS;
+    }
+/* complete definition of seg_dict entry */
+rvm_return_t def_seg_dict(log,rec_hdr)
+    log_t           *log;               /* log descriptor */
+    rec_hdr_t       *rec_hdr;           /* log segment definition descriptor
+                                           (with log record header) */
+    {
+    log_seg_t       *log_seg;           /* log segment definition descriptor */
+    seg_dict_t      *seg_dict;          /* segment dictionary entry */
+    char            *seg_name;          /* ptr to segment name in seg_dict rec */
+    device_t        *dev;               /* device descriptor */
+    rvm_return_t    retval;
+
+    assert(rec_hdr->struct_id == log_seg_id);
+    log_seg = (log_seg_t *)RVM_ADD_LENGTH_TO_ADDR(rec_hdr,
+                                                  sizeof(rec_hdr_t));
+
+    /* create dictionary entry if necessary */
+    if ((retval=enter_seg_dict(log,log_seg->seg_code)) != RVM_SUCCESS)
+        return retval;
+    seg_dict = &log->seg_dict_vec[SEG_DICT_INDEX(log_seg->seg_code)];
+
+    /* if segment not defined, set device name (open later) */
+    seg_name = (char *)((rvm_length_t)rec_hdr+LOG_SPECIAL_SIZE);
+    seg_dict->seg = seg_lookup(seg_name,&retval);
+    if (seg_dict->seg == NULL)
+        {
+        assert(log->in_recovery || rvm_utlsw);
+        dev = &seg_dict->dev;
+        dev->name = malloc(log_seg->name_len+1);
+        if (dev->name == NULL)
+            return RVM_ENO_MEMORY;
+        (void)strcpy(dev->name,seg_name);
+        dev->num_bytes = log_seg->num_bytes;
+        }
+
+    return RVM_SUCCESS;
+    }
+/* change tree comparator for tree_insert */
+long cmp_partial_include(node1,node2)
+    dev_region_t    *node1;
+    dev_region_t    *node2;
+    {
+    return dev_partial_include(&node1->offset,&node1->end_offset,
+                               &node2->offset,&node2->end_offset);
+    }
+
+/* set length of change tree node from offsets */
+static void set_node_length(node)
+    dev_region_t    *node;              /* change tree node */
+    {
+    rvm_offset_t    offset_temp;        /* offset arithmetic temp */
+
+    offset_temp = RVM_SUB_OFFSETS(node->end_offset,node->offset);
+    assert(RVM_OFFSET_LEQ(offset_temp,node->end_offset)); /* overflow! */
+    node->length = RVM_OFFSET_TO_LENGTH(offset_temp);
+
+    }
+rvm_return_t change_tree_insert(seg_dict,node)
+    seg_dict_t      *seg_dict;          /* seg_dict for this nv */
+    dev_region_t    *node;              /* change tree node for this nv */
+    {
+    dev_region_t    *x_node;            /* existing node if conflict */
+    dev_region_t    *split_node;        /* ptr to created node, when used */
+    rvm_length_t    log_diff;           /* adjustment to log/nv_buf offset */
+    long            cmpval;             /* comparison return value */
+    char            *shadow_vmaddr;     /* vmaddr of shadowed data */
+    rvm_length_t    shadow_length = 0;  /* length of shadowed data */
+    rvm_length_t    shadow_skew = 0;    /* byte skew of shadowed data */
+    char            *shadow_ptr = NULL; /* ptr to shadowed data in vm */
+    rvm_offset_t    shadow_offset;      /* offset of shadowed data in log */
+    rvm_return_t    retval;
+
+    /* try to insert node & see if values already there */
+    if (node->length == 0) goto free_node; /* eliminate zero-length nodes */
+
+    if (num_nodes-- == 0)
+        {
+        num_nodes = NODES_PER_YIELD;
+        if (!(default_log->in_recovery || rvm_utlsw))
+            {
+            if (!rvm_no_yield) cthread_yield(); /* allow reschedule */
+            }
+        }
+    assert(default_log->trunc_thread == cthread_self());
+    assert((default_log->status.trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+
+    if (tree_insert(&seg_dict->mod_tree,node,cmp_partial_include))
+        {
+        if (rvm_chk_len != 0)           /* do monitoring */
+            monitor_vmaddr(node->vmaddr,node->length,node->nv_ptr,
+                           &node->log_offset,NULL,
+                           "change_tree_insert: inserting entire range");
+        return RVM_SUCCESS;             /* no shadowed values */
+        }
+    x_node = (dev_region_t *)           /* get existing node */
+        (seg_dict->mod_tree.traverse[seg_dict->mod_tree.level].ptr);
+
+    /* some values already there: test existing node spans new */
+    if (dev_total_include(&node->offset,&node->end_offset,
+                          &x_node->offset,&x_node->end_offset) == 0)
+        {
+        if (rvm_chk_len != 0)           /* do monitoring */
+            monitor_vmaddr(node->vmaddr,node->length,NULL,NULL,NULL,
+                           "change_tree_insert: all values shadowed");
+        goto free_node;                 /* yes, all values shadowed */
+        }
+    /* some shadowed, test if new values span existing node */
+    if ((cmpval=dev_total_include(&x_node->offset,&x_node->end_offset,
+                          &node->offset,&node->end_offset)) == 0)
+        if (RVM_OFFSET_LSS(node->offset,x_node->offset))
+            {                           /* make node for preceeding values */
+            if ((split_node=make_dev_region()) == NULL)
+                return RVM_ENO_MEMORY;
+            if (node->nv_buf != NULL)
+                {
+                assert(RVM_OFFSET_EQL_ZERO(node->log_offset));
+                assert(node->nv_buf->struct_id == nv_buf_id);
+                split_node->nv_buf = node->nv_buf;
+                node->nv_buf->ref_cnt++;
+                split_node->nv_ptr = node->nv_ptr;
+                }
+            else
+                assert(node->nv_ptr == NULL);
+
+            /* complete the new node */
+            split_node->offset = node->offset;
+            split_node->end_offset = x_node->offset;
+            split_node->log_offset = node->log_offset;
+            split_node->vmaddr = node->vmaddr;
+            set_node_length(split_node);
+            node->vmaddr += split_node->length;
+            node->offset = RVM_ADD_LENGTH_TO_OFFSET(node->offset,
+                                                  split_node->length);
+            log_diff = split_node->length +
+                BYTE_SKEW(RVM_OFFSET_TO_LENGTH(split_node->offset));
+
+            if (node->nv_ptr != NULL)
+                node->nv_ptr = (char *)CHOP_TO_LENGTH(
+                        RVM_ADD_LENGTH_TO_ADDR(node->nv_ptr,log_diff));
+            else
+                node->log_offset = CHOP_OFFSET_TO_LENGTH_SIZE(
+                    RVM_ADD_LENGTH_TO_OFFSET(split_node->log_offset,
+                                             log_diff));
+
+            /* insert split node in tree */
+            if (rvm_chk_len != 0)       /* do monitoring */
+                monitor_vmaddr(split_node->vmaddr,split_node->length,
+                               NULL,NULL,NULL,
+                               "change_tree_insert: inserting split node");
+            if ((retval=change_tree_insert(seg_dict,split_node))
+                != RVM_SUCCESS) return retval;
+            }
+    /* test if new values follow existing node */
+    shadow_skew = BYTE_SKEW(RVM_OFFSET_TO_LENGTH(node->offset));
+    if (cmpval <= 0)
+        {
+        /* yes, reset starting offset */
+        shadow_vmaddr = node->vmaddr;
+        shadow_length = RVM_OFFSET_TO_LENGTH(
+                   RVM_SUB_OFFSETS(x_node->end_offset,node->offset));
+        shadow_ptr = node->nv_ptr;
+        shadow_offset = node->log_offset;
+        node->offset = x_node->end_offset;
+        set_node_length(node);
+        if (node->nv_ptr != NULL)       /* adjust buffer pointer */
+            node->nv_ptr = (char *)CHOP_TO_LENGTH(
+                                RVM_ADD_LENGTH_TO_ADDR(node->nv_ptr,
+                                           shadow_length+shadow_skew));
+        else                            /* adjust log offset */
+            node->log_offset = CHOP_OFFSET_TO_LENGTH_SIZE(
+                RVM_ADD_LENGTH_TO_OFFSET(node->log_offset,
+                                         shadow_length+shadow_skew));
+        node->vmaddr = RVM_ADD_LENGTH_TO_ADDR(node->vmaddr,
+                                              shadow_length);
+        }
+    else
+        /* new values preceed existing node, but don't span it */
+        {                               /* reset end offset */
+        node->end_offset = x_node->offset;
+        shadow_length = node->length;   /* save old length */
+        set_node_length(node);
+        shadow_length -= node->length;  /* correct for new length */
+        shadow_vmaddr = RVM_ADD_LENGTH_TO_ADDR(node->vmaddr,
+                                               node->length);
+        if (node->nv_ptr != NULL)
+            shadow_ptr = (char *)CHOP_TO_LENGTH(
+                            RVM_ADD_LENGTH_TO_ADDR(node->nv_ptr,
+                                         shadow_length+shadow_skew));
+        shadow_offset = CHOP_OFFSET_TO_LENGTH_SIZE(
+                            RVM_ADD_LENGTH_TO_OFFSET(node->log_offset,
+                                          shadow_length+shadow_skew));
+        }
+    /* insert modified node */
+    if (rvm_chk_len != 0)               /* do monitoring */
+        {
+        if (shadow_length != 0)
+            monitor_vmaddr(shadow_vmaddr,shadow_length,shadow_ptr,
+                           &shadow_offset,NULL,
+                           "change_tree_insert: values shadowed");
+        monitor_vmaddr(node->vmaddr,node->length,NULL,NULL,NULL,
+                       "change_tree_insert: inserting non-shadowed values");
+        }
+    return change_tree_insert(seg_dict,node);
+
+free_node:
+    free_dev_region(node);
+    return RVM_SUCCESS;
+    }
+/* prepare new value record for seg_dict's mod_tree
+   if new values are <= nv_local_max, they must be in buffer */
+static rvm_return_t do_nv(log,nv)
+    log_t           *log;
+    nv_range_t      *nv;
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    seg_dict_t      *seg_dict;          /* seg_dict for this nv */
+    dev_region_t    *node;              /* change tree node for this nv */
+    rvm_length_t    aligned_len;        /* allocation temp */
+    rvm_offset_t    offset;             /* monitoring temp */
+    rvm_bool_t      chk_val;            /* checksum result */
+    rvm_return_t    retval;             /* return value */
+
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+    assert(nv->rec_hdr.struct_id == nv_range_id);  /* not a nv range header */
+    assert(TIME_EQL(log_buf->timestamp,nv->rec_hdr.timestamp));
+
+    if (rvm_chk_len != 0)               /* do monitoring */
+        {
+        offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                                    log_buf->ptr+sizeof(nv_range_t));
+        monitor_vmaddr(nv->vmaddr, nv->length, NULL, &offset,
+                       &nv->rec_hdr, "do_nv: data from log");
+        }
+
+    if (nv->length == 0) return RVM_SUCCESS; /* ignore null changes */
+
+    /* be sure in segment dictionary */
+    if ((retval=enter_seg_dict(log,nv->seg_code)) != RVM_SUCCESS)
+        return retval;
+    seg_dict = &log->seg_dict_vec[SEG_DICT_INDEX(nv->seg_code)];
+
+    /* make a tree node for changes */
+    if ((node = make_dev_region()) == NULL) return RVM_ENO_MEMORY;
+    node->offset = nv->offset;
+    node->end_offset = RVM_ADD_LENGTH_TO_OFFSET(nv->offset,nv->length);
+    node->length = nv->length;
+    node->vmaddr = nv->vmaddr;
+    /* see if mods small enough to keep in vm */
+    if (nv->length <= NV_LOCAL_MAX)
+        {                               /* yes, get some space for nv */
+        aligned_len = ALIGNED_LEN(RVM_OFFSET_TO_LENGTH(nv->offset),
+                                  nv->length);
+        if ((node->nv_buf=(nv_buf_t *)malloc(NV_BUF_SIZE(aligned_len)))
+            == NULL) return RVM_ENO_MEMORY;
+        node->nv_buf->struct_id = nv_buf_id;
+        node->nv_buf->alloc_len = NV_BUF_SIZE(aligned_len);
+        node->nv_buf->ref_cnt = 1;
+        node->nv_buf->chk_sum = nv->chk_sum;
+        node->nv_buf->data_len = nv->length;
+        node->nv_ptr = (char *)&node->nv_buf->buf;
+        assert(((rvm_length_t)nv+sizeof(nv_range_t))
+               >= (rvm_length_t)default_log->log_buf.buf);
+        assert(((rvm_length_t)nv+sizeof(nv_range_t))
+               < ((rvm_length_t)default_log->log_buf.buf
+                  +default_log->log_buf.r_length));
+
+        /* basic BCOPY will not change alignment since buffer padded */
+        (void)BCOPY(RVM_ADD_LENGTH_TO_ADDR(nv,sizeof(nv_range_t)),
+                    node->nv_ptr,aligned_len);
+        }
+    else
+        /* no, set offset in log for nv's */
+        node->log_offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                 (rvm_length_t)nv-(rvm_length_t)log_buf->buf
+                                 +sizeof(nv_range_t));
+
+    /* put in change tree */
+    if ((retval=change_tree_insert(seg_dict,node)) != RVM_SUCCESS)
+        return retval;
+
+    /* see if complete check sum test wanted */
+    if (rvm_chk_sum)
+        {
+        if ((retval=range_chk_sum(log,nv,&chk_val,SYNCH))
+            != RVM_SUCCESS) return retval;
+        assert(chk_val == rvm_true);        /* check sum failure */
+        if ((retval=scan_nv_reverse(log,SYNCH)) != RVM_SUCCESS)
+            return retval;
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+        }
+
+    return RVM_SUCCESS;
+    }
+/* scan modifications of transaction in reverse order & build tree */
+static rvm_return_t do_trans(log,skip_trans)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      skip_trans;         /* scan, but ignore if true */
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    
+    rec_hdr_t       *rec_hdr;           /* last record header scanned */
+    rec_end_t       *rec_end;           /* end marker for transaction */
+    trans_hdr_t     *trans_hdr;         /* transaction header ptr */
+    long            num_ranges = 0;     /* ranges processed */
+    long            prev_range = 0;     /* previous range number */
+    rvm_return_t    retval;             /* return value */
+
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+
+    /* remember the transaction's timestamp and scan ranges */
+    rec_end = (rec_end_t *)&log_buf->buf[log_buf->ptr];
+    assert(rec_end->rec_hdr.struct_id == rec_end_id);
+    log_buf->timestamp = rec_end->rec_hdr.timestamp;
+    DO_FOREVER
+        {
+        if ((retval=scan_nv_reverse(log,SYNCH)) != RVM_SUCCESS)
+            return retval;
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+        rec_hdr = (rec_hdr_t *)&log_buf->buf[log_buf->ptr];
+
+        /* test for end */
+        if (rec_hdr->struct_id == trans_hdr_id)
+            break;                      /* done */
+
+        /* check order and process the range */
+        assert(rec_hdr->struct_id == nv_range_id);
+        if (prev_range != 0)
+            assert(((nv_range_t *)rec_hdr)->range_num == (prev_range-1));
+        if (!skip_trans)
+            if ((retval=do_nv(log,(nv_range_t *)rec_hdr))
+                != RVM_SUCCESS) return retval;
+
+        /* tally ranges processed */
+        num_ranges++;
+        prev_range = ((nv_range_t *)rec_hdr)->range_num;
+        }
+
+    /* sanity checks at the end... */
+    trans_hdr = (trans_hdr_t *)rec_hdr;
+    assert(trans_hdr->rec_hdr.struct_id == trans_hdr_id);
+    assert(TIME_EQL(trans_hdr->rec_hdr.timestamp,log_buf->timestamp));
+    assert(trans_hdr->num_ranges == num_ranges);
+    if (num_ranges != 0) assert(prev_range == 1);
+
+    return RVM_SUCCESS;
+    }
+/* log wrap-around validation */
+static rvm_return_t chk_wrap(log,force_wrap_chk,skip_trans)
+    log_t           *log;               /* log descriptor */
+    rvm_bool_t      force_wrap_chk;     /* wrap check required if true */
+    rvm_bool_t      *skip_trans;        /* set true if bad split */
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_offset_t    offset;             /* offset temp */
+    rvm_offset_t    end_offset;         /* offset of last trans end marker */
+    rec_end_t       *rec_end;           /* last record scanned in buffer */
+    trans_hdr_t     last_trans_hdr;     /* last transaction record header */
+    trans_hdr_t     *trans_hdr;         /* header temporary */
+    log_wrap_t      *log_wrap;          /* wrap-around marker */
+    long            tmp_ptr;            /* buffer index temp */
+    long            data_len;           /* length temporary */
+    rvm_return_t    retval;             /* return value */
+
+    *skip_trans = rvm_false;
+    rec_end = (rec_end_t *)&log_buf->buf[log_buf->ptr];
+    offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,log_buf->ptr);
+    offset = RVM_SUB_LENGTH_FROM_OFFSET(offset,rec_end->rec_hdr.rec_length);
+
+    /* check if transaction header is at start of log data area */
+    if (!RVM_OFFSET_EQL(offset,status->log_start) && (!force_wrap_chk))
+        return RVM_SUCCESS;             /* no, nothing more needed */
+
+    /* get header */
+    if (force_wrap_chk)
+        {
+        /* header can be anywhere */
+        if (RVM_OFFSET_LSS(offset,log_buf->offset))
+            {
+            retval = load_aux_buf(log,&offset,sizeof(trans_hdr_t),
+                                  &tmp_ptr,&data_len,SYNCH,rvm_false);
+            if (retval != RVM_SUCCESS) return retval;
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES)
+                   == RVM_TRUNC_BUILD_TREE);
+            assert(data_len >= sizeof(trans_hdr_t));
+            trans_hdr = (trans_hdr_t *)&log_buf->aux_buf[tmp_ptr];
+            }
+        else
+            trans_hdr = (trans_hdr_t *)&log_buf->buf[log_buf->ptr
+                                                -rec_end->rec_hdr.rec_length];
+        }
+    else
+        /* header is at start of aux_buf or recovery buffer */
+        if (RVM_OFFSET_LSS(offset,log_buf->offset))
+            trans_hdr = (trans_hdr_t *)log_buf->aux_buf;
+        else
+            trans_hdr = (trans_hdr_t *)log_buf->buf;
+
+    /* check for split transaction */
+    assert(trans_hdr->rec_hdr.struct_id == trans_hdr_id);
+    if (TRANS_HDR(FIRST_ENTRY_FLAG)
+        && TRANS_HDR(LAST_ENTRY_FLAG))
+        return RVM_SUCCESS;             /* not split, nothing more needed */
+
+    /* split, see if must check further or skip record */
+    assert(TRANS_HDR(FIRST_ENTRY_FLAG) || TRANS_HDR(LAST_ENTRY_FLAG));
+    if (!TRANS_HDR(LAST_ENTRY_FLAG))
+        {
+        if (log_buf->split_ok)
+            {                           /* split previously checked */
+            log_buf->split_ok = rvm_false;
+            return RVM_SUCCESS;
+            }
+        if (force_wrap_chk)             /* if not last entry, trans not good */
+            {
+            *skip_trans = rvm_true;
+            return RVM_SUCCESS;
+            }
+        }
+
+    /* must make local copy and scan for first record of transaction */
+    end_offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,
+                                (log_buf->ptr+sizeof(rec_end_t)));
+    (void)BCOPY(trans_hdr,&last_trans_hdr,sizeof(trans_hdr_t));
+    if ((retval=scan_reverse(log,SYNCH)) != RVM_SUCCESS)
+        return retval;
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+
+    /* wrap-around had better be next... */
+    assert((long)log_buf->ptr >= 0);
+    log_wrap = (log_wrap_t *)&log_buf->buf[log_buf->ptr];
+    assert(log_wrap->rec_hdr.struct_id == log_wrap_id);
+    assert(log_wrap->rec_hdr.rec_num == (last_trans_hdr.rec_hdr.rec_num-1));
+
+    /* now scan for first record of transaction */
+    if ((retval=scan_reverse(log,SYNCH)) != RVM_SUCCESS)
+        return retval;
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+    assert((long)log_buf->ptr >= 0);
+    rec_end = (rec_end_t *)&log_buf->buf[log_buf->ptr];
+    assert(rec_end->rec_hdr.struct_id == rec_end_id);
+    /* check if the header is the first record of last transaction */
+    offset = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,log_buf->ptr);
+    offset = RVM_SUB_LENGTH_FROM_OFFSET(offset,rec_end->rec_hdr.rec_length);
+    if (RVM_OFFSET_LSS(offset,log_buf->offset))
+        {
+        /* header is in aux_buf */
+        tmp_ptr = OFFSET_TO_SECTOR_INDEX(offset);
+        trans_hdr = (trans_hdr_t *)&log_buf->aux_buf[tmp_ptr];
+        }
+    else
+        {
+        /* header is in recovery buffer */
+        tmp_ptr = RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(offset,
+                                           log_buf->offset));
+        assert(tmp_ptr >= 0);
+        trans_hdr = (trans_hdr_t *)&log_buf->buf[tmp_ptr];
+        }
+
+    /* sanity checks... */
+    assert(trans_hdr->rec_hdr.struct_id == trans_hdr_id);
+    assert(TRANS_HDR(FIRST_ENTRY_FLAG));
+    assert(TIME_EQL(trans_hdr->uname,last_trans_hdr.uname));
+    assert(trans_hdr->rec_hdr.rec_num == (last_trans_hdr.rec_hdr.rec_num-2));
+
+    /* all is well, restore last transaction record */
+    log_buf->prev_rec_num = 0;
+    ZERO_TIME(log_buf->prev_timestamp);
+    if ((retval=init_buffer(log,&end_offset,REVERSE,SYNCH))
+        != RVM_SUCCESS) return retval;
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES)
+           == RVM_TRUNC_BUILD_TREE);
+    log_buf->ptr -= sizeof(rec_end_t);
+    log_buf->split_ok = rvm_true;
+
+    return RVM_SUCCESS;
+    }
+/* Recovery: phase 2 -- build modification trees, and
+   construct dictionary of segment short names
+*/
+#define X(a) 
+static rvm_return_t build_tree(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_return_t    retval;             /* return value */
+    rvm_offset_t    tail;               /* tail offset temp */
+    rec_end_t       *rec_end;           /* last record scanned in buffer */
+    rvm_length_t    trans_cnt = 0;      /* transactions processed */
+    rvm_bool_t      force_wrap_chk = rvm_false; /* true if suspect bad wrap */
+    rvm_bool_t      skip_trans;         /* true if bad wrap trans to be skipped */
+
+    assert(log->trunc_thread == cthread_self());
+    assert(((status->trunc_state & RVM_TRUNC_PHASES) == RVM_TRUNC_FIND_TAIL)
+            || ((status->trunc_state & RVM_TRUNC_PHASES) == ZERO));
+    status->trunc_state = (status->trunc_state & (~RVM_TRUNC_FIND_TAIL))
+                           | RVM_TRUNC_BUILD_TREE;
+
+    /* reset sequence checks and init scan buffers */
+X(reset_hdr)
+    reset_hdr_chks(log);
+X(clear_aux)
+    clear_aux_buf(log);
+X(init_buf)
+    if (RVM_OFFSET_EQL(status->prev_log_tail, status->log_start))
+        retval = init_buffer(log,&status->log_start, FORWARD,SYNCH);
+    else
+        retval = init_buffer(log,&status->prev_log_tail, REVERSE,SYNCH);
+    assert(retval == RVM_SUCCESS);
+    assert(log->trunc_thread == cthread_self());
+X(done_init_buf)
+    /* scan in reverse from tail to find records for uncommitted changes */
+    num_nodes = NODES_PER_YIELD;
+    log_buf->split_ok = rvm_false;      /* split records not checked yet */
+    tail = status->prev_log_tail;       /* use previous epoch tail */
+    while (!RVM_OFFSET_EQL(tail,status->prev_log_head))
+        {
+X(start loop)
+        if ((retval=scan_reverse(log,SYNCH)) != RVM_SUCCESS)
+            return retval;
+X(done scan_reverse)
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES)
+               == RVM_TRUNC_BUILD_TREE);
+        if (rvm_chk_sigint != NULL)     /* test for interrupt */
+            if ((*rvm_chk_sigint)(NULL)) return RVM_SUCCESS;
+        assert((long)log_buf->ptr >= 0); /* log damage, invalid record */
+
+        /* check type of end marker, do type-dependent processing */
+        rec_end = (rec_end_t *)&log_buf->buf[log_buf->ptr];
+        if (rec_end->rec_hdr.struct_id == log_wrap_id)
+            {
+X(log_wrap)
+            if (!log_buf->split_ok)
+                force_wrap_chk = rvm_true;
+            }
+        else
+            {
+X(else)
+            assert(rec_end->rec_hdr.struct_id == rec_end_id);
+            switch (rec_end->rec_type)
+                {
+              case trans_hdr_id:        /* process transaction */
+X( trans_hdr_id: chk_wrap)
+                if ((retval=chk_wrap(log,force_wrap_chk,&skip_trans))
+                    != RVM_SUCCESS) return retval;
+                force_wrap_chk = rvm_false;
+X( trans_hdr_id: do_trans)
+                if ((retval=do_trans(log,skip_trans)) != RVM_SUCCESS)
+                    return retval;
+X( trans_hdr_id: end)
+                trans_cnt++;
+                break;
+              case log_seg_id:          /* enter seg short id in dictionary */
+X( log_seg_id: def_seg_dict)
+                if ((retval=def_seg_dict(log,(rec_hdr_t *)
+                    RVM_SUB_LENGTH_FROM_ADDR(rec_end,
+                                        rec_end->rec_hdr.rec_length)))
+                    != RVM_SUCCESS) return retval;
+X( log_seg_id: done)
+                log_buf->ptr -= rec_end->rec_hdr.rec_length;
+                break;
+              default:  assert(rvm_false); /* trouble, log damage? */
+                }
+            }
+
+        /* update local tail ptr */
+        tail = RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,log_buf->ptr);
+        }
+
+    /* leave buffer unprotected for later phases */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log_buf->buf, log_buf->length, FALSE, VM_PROT_WRITE | VM_PROT_READ);
+ */
+
+    return RVM_SUCCESS;
+    }
+/* pre-scan change tree to see how much to read to read into buffer */
+static dev_region_t *pre_scan(log,tree)
+    log_t           *log;               /* log descriptor */
+    tree_root_t     *tree;              /* current tree root */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    dev_region_t    *last_node = NULL; 
+    dev_region_t    *node;              /* current change tree node */
+    rvm_offset_t    temp;
+
+    /* find node with least offset */
+    node = (dev_region_t *)tree->root;
+    /* XXX - Can node ever be NULL?  If so, last_node can be random */
+    /* I currently believe it must be NON-null */
+    assert(node != NULL);
+    while (node != NULL)
+        {
+        assert(node->links.node.struct_id == dev_region_id);
+        last_node = node;
+        node = (dev_region_t *)node->links.node.lss;
+        }
+    log_buf->offset = CHOP_OFFSET_TO_SECTOR_SIZE(last_node->offset);
+
+    /* scan for maximum offset node that will fit in buffer */
+    node = (dev_region_t *)tree->root;
+    while (node != NULL)
+        {
+        assert(node->links.node.struct_id == dev_region_id);
+
+        /* compute buffer extension for this node */
+        temp = RVM_SUB_OFFSETS(node->end_offset,log_buf->offset);
+        temp = ROUND_OFFSET_TO_SECTOR_SIZE(temp);
+
+        /* see if will fit in log buffer */
+        if (RVM_OFFSET_GTR(temp,log_buf->buf_len))
+            node = (dev_region_t *)node->links.node.lss; /* try smaller */
+        else
+            {
+            /* see if there's another that will also fit */
+            last_node = node;
+            node = (dev_region_t *)node->links.node.gtr;
+            }
+        }
+
+    return last_node;
+    }
+/* merge large node disk-resident new values with segment data */
+static rvm_return_t disk_merge(log,node,preload)
+    log_t           *log;               /* log descriptor */
+    dev_region_t    *node;              /* node to merge */
+    rvm_bool_t      preload;            /* end sector preload done if true */
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_length_t    data_len=0;         /* actual nv data length read */
+    rvm_length_t    buf_ptr;            /* log buffer ptr */
+    rvm_length_t    aux_ptr;            /* aux buffer ptr
+                                           (compensates for sector alignment) */
+    rvm_length_t    tmp_ptr;            /* temporary buffer ptr */
+    long            rw_length;          /* actual i/o transfer length */
+    rvm_offset_t    end_offset;         /* end offset temporary */
+    rvm_return_t    retval;             /* return value */
+    rvm_bool_t      was_preloaded = preload; /* save preload state */
+
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) 
+           == RVM_TRUNC_APPLY);
+    assert(node->links.node.struct_id == dev_region_id);
+
+    /* set log buffer pointer and end offset */
+    end_offset = CHOP_OFFSET_TO_SECTOR_SIZE(node->end_offset);
+    buf_ptr = RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(node->offset,
+                                                   log_buf->offset));
+    node->log_offset = RVM_ADD_LENGTH_TO_OFFSET(node->log_offset,
+                                                BYTE_SKEW(buf_ptr));
+    DO_FOREVER
+        {                               /* fill log buffer from aux buf */
+        while ((buf_ptr < log_buf->length)
+               && (node->length != 0))
+            {
+            /* see how much to get in this pass & load aux_buf */
+            if ((log_buf->length-buf_ptr) < node->length)
+                rw_length = log_buf->length-buf_ptr; /* fill log_buf */
+            else                      
+                rw_length = node->length; /* get all remaining */
+            if ((retval=load_aux_buf(log,&node->log_offset,rw_length,
+                                     &aux_ptr,&data_len,SYNCH,rvm_true))
+                != RVM_SUCCESS) return retval;
+            /* sanity checks and monitoring */
+            assert((aux_ptr+data_len) <= log_buf->aux_rlength);
+            assert((buf_ptr+data_len) <= log_buf->length);
+            assert(BYTE_SKEW(aux_ptr) == BYTE_SKEW(node->vmaddr));
+            assert((long)(node->length-data_len) >= 0);
+            if (rvm_chk_len != 0)
+                monitor_vmaddr(node->vmaddr,data_len,
+                               &log_buf->aux_buf[aux_ptr],NULL,NULL,
+                               "disk_merge: data read from log:");
+
+            /* preload of last modified segment sector */
+            if (RVM_OFFSET_GTR(RVM_ADD_LENGTH_TO_OFFSET(
+                               node->offset,data_len),end_offset)
+                && (!preload))
+                {
+                /* must load last sector of mods from segment */
+                tmp_ptr = CHOP_TO_SECTOR_SIZE(buf_ptr+data_len);
+                if (!(log->in_recovery || rvm_utlsw || rvm_no_yield))
+                    {
+                    cthread_yield();    /* allow reschedule */
+                    assert(log->trunc_thread == cthread_self());
+                    }
+                if ((rw_length=read_dev(log->cur_seg_dev,&end_offset,
+                             &log_buf->buf[tmp_ptr],SECTOR_SIZE)) < 0)
+                    return RVM_EIO;
+                assert(log->trunc_thread == cthread_self());
+                assert((status->trunc_state & RVM_TRUNC_PHASES) 
+                       == RVM_TRUNC_APPLY);
+                assert(rw_length == SECTOR_SIZE);
+                preload = rvm_true;
+
+                /* monitor data from last sector */
+                if (rvm_chk_len != 0)
+                    monitor_vmaddr(node->vmaddr,data_len,
+                                   &log_buf->buf[buf_ptr],NULL,NULL,
+                                   "disk_merge: data read from segment:");
+                }
+
+            /* copy to segment (in log buffer) */
+            (void)BCOPY(&log_buf->aux_buf[aux_ptr],
+                        &log_buf->buf[buf_ptr],data_len);
+
+            /* tally bytes merged & do monitoring */
+            if (rvm_chk_len != 0)
+                {
+                monitor_vmaddr(node->vmaddr,data_len,
+                               &log_buf->buf[buf_ptr],NULL,NULL,
+                               "disk_merge: data merged to segment:");
+                }
+            node->length -= data_len;
+            node->vmaddr += data_len;
+            node->log_offset =
+                RVM_ADD_LENGTH_TO_OFFSET(node->log_offset,data_len);
+            node->offset =
+                RVM_ADD_LENGTH_TO_OFFSET(node->offset,data_len);
+            buf_ptr += data_len;
+            /* if done, set final write length */
+            if (node->length == 0)
+                {
+                assert(RVM_OFFSET_EQL(node->offset,
+                                      node->end_offset));
+                end_offset =
+                    RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,buf_ptr);
+                assert(RVM_OFFSET_EQL(end_offset,node->end_offset));
+                if (!was_preloaded)
+                    log_buf->r_length = ROUND_TO_SECTOR_SIZE(buf_ptr);
+                return RVM_SUCCESS;
+                }
+            }
+
+        /* write buffer to segment & monitor */
+        assert(buf_ptr == log_buf->length);
+        if ((rw_length=write_dev(log->cur_seg_dev,&log_buf->offset,
+                                 log_buf->buf,log_buf->length,
+                                 SYNCH))
+            < 0) return RVM_EIO;
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+               == RVM_TRUNC_APPLY);
+        assert(rw_length == log_buf->length);
+        if (rvm_chk_len != 0)
+            monitor_vmaddr(node->vmaddr-data_len,data_len,
+                           &log_buf->buf[buf_ptr-data_len],NULL,NULL,
+                           "disk_merge: data written to segment:");
+        if (!(log->in_recovery || rvm_utlsw || rvm_no_yield))
+            {
+            cthread_yield();            /* allow reschedule */
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES) 
+                   == RVM_TRUNC_APPLY);
+            }
+        log_buf->offset =
+            RVM_ADD_LENGTH_TO_OFFSET(log_buf->offset,buf_ptr);
+        buf_ptr = 0;
+        assert(OFFSET_TO_SECTOR_INDEX(log_buf->offset) == 0);
+        }
+    }
+/* merge node's new values with segment data in buffer */
+static rvm_return_t merge_node(log,node,preload)
+    log_t           *log;               /* log descriptor */
+    dev_region_t    *node;              /* current change tree node */
+    rvm_bool_t      preload;            /* end sector preload done if true */
+    {
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    rvm_length_t    temp;
+    rvm_return_t    retval;             /* return value */
+
+    /* do monitoring and merge node data into segment */
+    if (RVM_OFFSET_EQL_ZERO(node->log_offset))
+        {                               /* data in node */
+        if (rvm_chk_len != ZERO)
+            monitor_vmaddr(node->vmaddr,node->length,
+                           node->nv_ptr,NULL,NULL,
+                           "merge_node: data copied from node:");
+        temp = RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(node->offset,
+                                                    log_buf->offset));
+        assert((temp+node->length) <= log_buf->r_length);
+        dest_aligned_bcopy(node->nv_ptr,&log_buf->buf[temp],
+                           node->length);
+        }
+    else                                /* data on disk -- use aux_buf */
+        if ((retval=disk_merge(log,node,preload)) != RVM_SUCCESS)
+            return retval;
+
+    /* free node and check for yield */
+    (void) free_dev_region(node);
+    if (num_nodes-- == 0)
+        {
+        num_nodes = NODES_PER_YIELD;
+        if (!(log->in_recovery || rvm_utlsw || rvm_no_yield))
+            {
+            cthread_yield();            /* allow reschedule */
+            assert(log->trunc_thread == cthread_self());
+            }
+        }
+
+    return RVM_SUCCESS;
+    }
+
+static rvm_return_t update_seg(log,seg_dict,seg_dev)
+    log_t           *log;               /* log descriptor */
+    seg_dict_t      *seg_dict;          /* segment dictionary entry */
+    device_t        *seg_dev;           /* segment device descriptor */
+{
+    log_status_t    *status = &log->status; /* status descriptor */
+    log_buf_t       *log_buf = &log->log_buf; /* log buffer descriptor */
+    long            r_length;           /* length of data transfered */
+    rvm_bool_t      preload;            /* end sector preload done if true */
+    char            *addr=NULL;         /* monitoring address */
+    rvm_offset_t    temp;               /* offset temporary */
+    dev_region_t    *node;              /* current node */
+    dev_region_t    *last_node;         /* last node before buffer write */
+    rvm_return_t    retval = RVM_SUCCESS; /* return value */
+    long            nodes_done = 0;
+
+    /* sanity checks and initializations */
+    assert(&log->dev != seg_dev);
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) 
+           == RVM_TRUNC_APPLY);
+    rvm_num_nodes = seg_dict->mod_tree.n_nodes;
+    rvm_max_depth = seg_dict->mod_tree.max_depth;
+    clear_aux_buf(log);
+
+    /* process the change tree */
+    if (!(log->in_recovery || rvm_utlsw)) /* begin segment dev_lock crit sec
+                                             */
+        {
+        mutex_lock(&seg_dict->seg->dev_lock);
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+               == RVM_TRUNC_APPLY);
+        }
+    while (seg_dict->mod_tree.root != NULL)
+        {
+        /* pre-scan tree to determine how to fill buffer */
+        last_node = pre_scan(log,&seg_dict->mod_tree);
+
+        /* initialize buffer with segment data */
+        temp = RVM_SUB_OFFSETS(last_node->end_offset,
+                               log_buf->offset);
+        temp = ROUND_OFFSET_TO_SECTOR_SIZE(temp);
+        if (RVM_OFFSET_LEQ(temp,log_buf->buf_len))
+            {
+            /* node(s) fit in log buffer */
+            log_buf->r_length = RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(
+                                    last_node->end_offset,
+                                    log_buf->offset));
+            log_buf->r_length =
+                ROUND_TO_SECTOR_SIZE(log_buf->r_length);
+            assert(log_buf->r_length <= log_buf->length);
+            preload = rvm_true;
+            }
+        else
+            {
+            log_buf->r_length = SECTOR_SIZE; /* very large node!! */
+            preload = rvm_false;
+            }
+        /* allow reschedule & do the read */
+        if (!(log->in_recovery || rvm_utlsw || rvm_no_yield))
+            {
+            cthread_yield();
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES) 
+                   == RVM_TRUNC_APPLY);
+            }
+        if ((r_length=read_dev(seg_dev,&log_buf->offset,
+                           log_buf->buf,log_buf->r_length)) < 0)
+            {
+            retval = RVM_EIO;
+            goto err_exit;
+            }
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+               == RVM_TRUNC_APPLY);
+        assert(r_length == log_buf->r_length);
+
+        /* merge selected nodes into buffer */
+        num_nodes = NODES_PER_YIELD;
+        UNLINK_NODES_OF(seg_dict->mod_tree,dev_region_t,node)
+            {
+            assert(node->links.node.struct_id == dev_region_id);
+            nodes_done++;
+
+            /* do monitoring */
+            if (rvm_chk_len != 0)
+                {
+                temp = log_buf->offset;
+                addr = (char *)CHOP_TO_SECTOR_SIZE(node->vmaddr);
+                monitor_vmaddr(addr,log_buf->r_length,log_buf->buf,
+                               &log_buf->offset,NULL,
+                               "update_seg: data read from segment:");
+                }
+
+            /* merge data */
+            if ((retval=merge_node(log,node,preload))
+                != RVM_SUCCESS) goto err_exit;
+            if (rvm_chk_sigint != NULL) /* test for interrupt */
+                if ((*rvm_chk_sigint)(NULL)) goto err_exit;
+            if (node == last_node) break;
+            }
+
+        /* update the segment on disk */
+        if ((r_length=write_dev(seg_dev,&log_buf->offset,log_buf->buf,
+                                log_buf->r_length,rvm_true)) < 0)
+            {
+            retval = RVM_EIO;
+            goto err_exit;
+            }
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+               == RVM_TRUNC_APPLY);
+        assert(r_length == log_buf->r_length);
+        /* do monitoring */
+        if (rvm_chk_len != 0)
+            {
+            if (!RVM_OFFSET_EQL(temp,log_buf->offset))
+                addr=RVM_ADD_LENGTH_TO_ADDR(addr,RVM_OFFSET_TO_LENGTH(
+                         RVM_SUB_OFFSETS(log_buf->offset,temp)));
+            monitor_vmaddr(addr,log_buf->r_length,log_buf->buf,
+                           &log_buf->offset,NULL,
+                           "update_seg: data written to segment:");
+            }
+        }
+
+    /* tree checks and cleanup after unlinking */
+    assert(nodes_done == rvm_num_nodes);
+    assert(seg_dict->mod_tree.n_nodes == 0);
+
+err_exit:
+    if (!(log->in_recovery || rvm_utlsw)) /* end segment dev_lock crit sec */
+        {
+        mutex_unlock(&seg_dict->seg->dev_lock);
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+               == RVM_TRUNC_APPLY);
+        }
+    return retval;
+    }
+/* Recovery: phase 3 -- apply modifications to segments */
+rvm_return_t apply_mods(log)
+    log_t           *log;               /* log descriptor */
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    seg_dict_t      *seg_dict;          /* current segment dictionary entry */
+    device_t        *seg_dev;           /* segment device descriptor */
+    rvm_return_t    retval = RVM_SUCCESS; /* return value */
+    long            i;                  /* loop counter */
+    rvm_length_t    flags = O_RDWR;
+
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) 
+           == RVM_TRUNC_BUILD_TREE);
+    status->trunc_state = (status->trunc_state & ~RVM_TRUNC_BUILD_TREE)
+                           | RVM_TRUNC_APPLY;
+
+    /* iterate through segment dictionary */
+    for (i=0;i<log->seg_dict_len;i++)
+        {
+        seg_dict = &log->seg_dict_vec[i];
+        assert(seg_dict->struct_id == seg_dict_id);
+
+        if (seg_dict->mod_tree.root == NULL)
+            continue;                   /* no changes to this seg */
+
+        /* open device and get characteristics if necessary */
+        if (log->in_recovery)
+            {
+            seg_dev = &seg_dict->dev;
+            if (rvm_no_update) flags = O_RDONLY;
+            if (open_dev(seg_dev,flags,0) < 0)
+                return RVM_EIO;
+            assert(log->trunc_thread == cthread_self());
+            if (set_dev_char(seg_dev,&seg_dev->num_bytes) < 0)
+                {
+                close_dev(seg_dev);
+                return RVM_EIO;
+                }
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES) 
+                   == RVM_TRUNC_APPLY);
+            }
+        else
+            {
+            assert(seg_dict->seg->links.struct_id == seg_id);
+            seg_dev = &(seg_dict->seg->dev); /* already open */
+            }
+        log->cur_seg_dev = seg_dev;
+
+        /* read segment data and merge new values */
+        if ((retval=update_seg(log,seg_dict,seg_dev))
+            != RVM_SUCCESS) return retval;
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+               == RVM_TRUNC_APPLY);
+
+        /* close segment device if in recovery */
+        if (log->in_recovery)
+            if (close_dev(seg_dev) < 0)
+                return RVM_EIO;
+        }
+
+    /* re-protect buffer */
+/* MACH_RVM_PROTECT
+ *
+ * protect(log->log_buf.buf, log->log_buf.length, FALSE, VM_PROT_READ);
+ */
+
+    return retval;
+    }
+/* Recovery: phase 4 -- update head/tail of log */
+static rvm_return_t status_update(log, new_1st_rec_num)
+    log_t           *log;               /* log descriptor */
+    rvm_length_t    new_1st_rec_num;
+    {
+    log_status_t    *status = &log->status; /* status descriptor */
+    struct timeval  end_time;           /* end of action time temp */
+    int             kretval;
+    rvm_return_t    retval = RVM_SUCCESS; /* return value */
+
+    assert(log->trunc_thread == cthread_self());
+    assert((status->trunc_state & RVM_TRUNC_PHASES) 
+           == RVM_TRUNC_APPLY);
+    status->trunc_state = (status->trunc_state & ~RVM_TRUNC_APPLY)
+                           | RVM_TRUNC_UPDATE;
+
+    /* update the status block on disk */
+    CRITICAL(log->dev_lock,             /* begin log device lock crit sec */
+        {
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+           == RVM_TRUNC_UPDATE);
+        status->prev_trunc = status->last_trunc;
+
+        if (RVM_OFFSET_EQL(status->log_head,status->log_tail))
+            clear_log_status(log);      /* log empty */
+        else
+            {
+            RVM_ZERO_OFFSET(status->prev_log_head);
+            RVM_ZERO_OFFSET(status->prev_log_tail);
+            status->first_rec_num = new_1st_rec_num;
+            }
+        
+        /* end timings */
+        kretval= gettimeofday(&end_time,(struct timezone *)NULL);
+        if (kretval != 0) goto err_exit;
+        end_time = sub_times(&end_time,&trunc_start_time);
+        status->tot_truncation_time =
+            add_times(&status->tot_truncation_time,&end_time);
+        status->last_truncation_time = round_time(&end_time);
+        enter_histogram(status->last_truncation_time,
+                            log->status.tot_truncation_times,
+                            truncation_times_vec,truncation_times_len);
+        status->last_tree_build_time = last_tree_build_time;
+        enter_histogram(last_tree_build_time,
+                        log->status.tot_tree_build_times,
+                        truncation_times_vec,truncation_times_len);
+        status->last_tree_apply_time = last_tree_apply_time;
+        enter_histogram(last_tree_apply_time,
+                        log->status.tot_tree_apply_times,
+                        truncation_times_vec,truncation_times_len);
+
+        retval = write_log_status(log,NULL);
+err_exit:;
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES) 
+           == RVM_TRUNC_UPDATE);
+        });                             /* end log device lock crit sec */
+    if (kretval != 0) return RVM_EIO;
+    if (retval != RVM_SUCCESS) return retval;
+
+    if (log->in_recovery && (!rvm_utlsw)) /* do recovery-only processing */
+        {
+        /* kill segment dictionary */
+        free_seg_dict_vec(log);
+
+        log->in_recovery = rvm_false;
+        }
+
+    return retval;
+    }
+/* switch truncation epochs */
+static rvm_return_t new_epoch(log,count)
+    log_t           *log;               /* log descriptor */
+    rvm_length_t    *count;             /* ptr to statistics counter */
+    {
+    log_status_t    *status = &log->status; /* log status descriptor */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* be sure last records in truncation are in log */
+    assert(log->trunc_thread == cthread_self());
+    if (sync_dev(&log->dev) < 0)
+        return RVM_EIO;
+    assert(log->trunc_thread == cthread_self());
+
+    /* count truncations & accumulate statistics */
+    (*count)++;
+    copy_log_stats(log);
+
+    /* set up head/tail pointers for truncation */
+    status->prev_log_head = status->log_head;
+    status->log_head = status->log_tail;
+    status->prev_log_tail = status->log_tail;
+    status->last_rec_num = status->next_rec_num-1;
+
+    /* set epoch time stamp and write status block */
+    make_uname(&status->last_trunc);
+    if ((retval=write_log_status(log,NULL)) != RVM_SUCCESS)
+        return retval;
+    assert(log->trunc_thread == cthread_self());
+
+    /* restore log segment definitions */
+    retval = define_all_segs(log);
+    assert(log->trunc_thread == cthread_self());
+    return retval;
+    }
+
+/* recover committed state from log */
+rvm_return_t log_recover(log,count,is_daemon,flag)
+    log_t           *log;               /* log descriptor */
+    rvm_length_t    *count;             /* ptr to statistics counter */
+    rvm_bool_t      is_daemon;          /* true if called by daemon */
+    rvm_length_t    flag;               /* truncation type flag */
+{
+    log_status_t    *status = &log->status; /* log status descriptor */
+    log_daemon_t    *daemon = &log->daemon; /* log daemon descriptor */
+    struct timeval  end_time;           /* end of action time temp */
+    struct timeval  tmp_time;           /* local timing temp */
+    int             kretval;
+    rvm_bool_t      do_truncation = rvm_false;
+    rvm_return_t    retval = RVM_SUCCESS;
+    rvm_length_t    new_1st_rec_num=0; 
+X(start)
+    CRITICAL(log->truncation_lock,      /* begin truncation lock crit sec */
+        {
+        /* capture truncation thread & flag for checking */
+        assert(log->trunc_thread == (cthread_t)NULL);
+        assert(status->trunc_state == ZERO);
+        log->trunc_thread = cthread_self();
+        status->trunc_state = flag;
+X(dev_lock)
+        CRITICAL(log->dev_lock,         /* begin dev_lock crit sec */
+            {
+            /* process statistics */
+            assert(log->trunc_thread == cthread_self());
+            kretval= gettimeofday(&trunc_start_time,
+                                  (struct timezone *)NULL);
+            if (kretval != 0)
+                {
+                retval = RVM_EIO;
+                goto err_exit1;
+                }
+            last_tree_build_time = 0;
+            last_tree_apply_time = 0;
+X(in_recovery)
+            /* phase 1: locate tail & start new epoch */
+            if (log->in_recovery)
+                {
+                if ((retval=locate_tail(log)) != RVM_SUCCESS)
+                    goto err_exit1;
+                assert((status->trunc_state & RVM_TRUNC_PHASES)
+                       == RVM_TRUNC_FIND_TAIL);
+                }
+            assert(log->trunc_thread == cthread_self());
+            if (rvm_chk_sigint != NULL) /* test for interrupt */
+                if ((*rvm_chk_sigint)(NULL)) goto err_exit1;
+            /* see if truncation actually needed */
+            if (RVM_OFFSET_EQL(status->log_tail,status->log_head))
+                status->log_empty = rvm_true;
+            else
+                {
+                status->log_empty = rvm_false;
+                do_truncation = rvm_true;
+                new_1st_rec_num = status->next_rec_num;
+
+                /* switch epochs */
+                if ((retval=new_epoch(log,count)) != RVM_SUCCESS)
+                    goto err_exit1;
+                assert(log->trunc_thread == cthread_self());
+                }
+
+X(err_exit1)
+err_exit1:;
+	    /* signal `initiate_truncation' that the first part is done */
+	    if (is_daemon)
+		{
+		mutex_lock(&daemon->lock);
+		assert(log->daemon.thread == cthread_self());
+		assert(daemon->state == truncating);
+		assert((status->trunc_state & RVM_ASYNC_TRUNCATE) != 0);
+		condition_signal(&daemon->flush_flag);
+		mutex_unlock(&daemon->lock);
+		}
+            });                         /* end dev_lock crit sec */
+
+        if (retval != RVM_SUCCESS) goto err_exit;
+        if (rvm_chk_sigint != NULL)     /* test for interrupt */
+            if ((*rvm_chk_sigint)(NULL)) goto err_exit;
+        /* do log scan if truncation actually needed */
+        if (do_truncation)
+            {
+X(do_trunc)
+            /* build tree and time */
+            kretval= gettimeofday(&tmp_time,(struct timezone *)NULL);
+            if (kretval != 0) assert(0); /* return RVM_EIO; */
+X(build_tree)
+            if ((retval=build_tree(log)) != RVM_SUCCESS) /* phase 2 */
+                assert(0); /* return retval; */
+X(build_tree done)
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES)
+                   == RVM_TRUNC_BUILD_TREE);
+	    
+            kretval= gettimeofday(&end_time,(struct timezone *)NULL);
+            if (kretval != 0) assert(0); /* return RVM_EIO; */
+            end_time = sub_times(&end_time,&tmp_time);
+            last_tree_build_time = round_time(&end_time);
+            if (rvm_chk_sigint != NULL) /* test for interrupt */
+                if ((*rvm_chk_sigint)(NULL)) goto err_exit;
+
+            /* apply tree and time */
+            kretval= gettimeofday(&tmp_time,(struct timezone *)NULL);
+            if (kretval != 0) assert(0); /* return RVM_EIO; */
+X(apply_mods)
+            if ((retval=apply_mods(log)) != RVM_SUCCESS) /* phase 3 */
+                goto err_exit;
+X(apply_mods end)
+            assert(log->trunc_thread == cthread_self());
+            assert((status->trunc_state & RVM_TRUNC_PHASES)
+                   == RVM_TRUNC_APPLY);
+            kretval= gettimeofday(&end_time,(struct timezone *)NULL);
+            if (kretval != 0) assert(0); /* return RVM_EIO; */
+            end_time = sub_times(&end_time,&tmp_time);
+            last_tree_apply_time = round_time(&end_time);
+            if (rvm_chk_sigint != NULL) /* test for interrupt */
+                if ((*rvm_chk_sigint)(NULL)) goto err_exit;
+            }
+        else
+            status->trunc_state =
+                (status->trunc_state & ~RVM_TRUNC_PHASES)
+                    | RVM_TRUNC_APPLY;
+X(status_upd)
+        /* always update the status */
+        retval = status_update(log, new_1st_rec_num);    /* phase 4 */
+        assert(log->trunc_thread == cthread_self());
+        assert((status->trunc_state & RVM_TRUNC_PHASES)
+               == RVM_TRUNC_UPDATE);
+        /* wake up any threads waiting on a truncation */
+err_exit:
+        assert(log->trunc_thread == cthread_self());
+        CRITICAL(daemon->lock,          /* begin daemon->lock crit sec */
+            {
+            assert(log->trunc_thread == cthread_self());
+            if (is_daemon)
+                {
+                assert(log->daemon.thread == cthread_self());
+                assert((status->trunc_state & RVM_ASYNC_TRUNCATE) != 0);
+                assert(daemon->state == truncating);
+                if (retval != RVM_SUCCESS)
+                    daemon->state = error;
+                }
+            assert(log->trunc_thread == cthread_self());
+            });                         /* end daemon->lock crit sec */
+
+        log->trunc_thread = (cthread_t)NULL;
+        status->trunc_state = ZERO;
+        });                             /* end truncation lock crit sec */
+
+    return retval;
+}
+#undef X
+
+
+/* rvm_truncate */
+rvm_return_t rvm_truncate()
+{
+	rvm_return_t    retval;
+
+	/* initial checks */
+	if (bad_init()) 
+		return RVM_EINIT;
+	if (default_log == NULL) 
+		return RVM_ELOG;
+
+    /* flush any queued records */
+	if ((retval=flush_log(default_log,
+			      &default_log->status.n_flush))
+	    != RVM_SUCCESS) return retval;
+
+	/* do truncation */
+	retval = log_recover(default_log,
+			     &default_log->status.tot_rvm_truncate,
+			     rvm_false,RVM_TRUNCATE_CALL);
+	return retval;
+}
+
+
+/* map & flush <--> truncation synchronization functions */
+
+/* initiate asynchronous truncation */
+rvm_bool_t initiate_truncation(log,threshold)
+    log_t           *log;               /* log descriptor */
+    rvm_length_t    threshold;          /* log % truncation threshold */
+{
+    log_daemon_t    *daemon = &log->daemon; /* daemon control descriptor */
+    rvm_bool_t      did_init = rvm_false; /* true if initiated truncation */
+
+    /* test threshold for asynch truncation */
+    if (!daemon->truncate || threshold < daemon->truncate)
+	return rvm_false;
+
+    /* trigger a truncation if log at threshold */
+    CRITICAL(daemon->lock,              /* begin daemon->lock crit sec */
+        {
+            /* wake up daemon if idle */
+            if (daemon->state == rvm_idle)
+	    {
+                did_init = rvm_true;
+                daemon->state = truncating;
+                condition_signal(&daemon->code);
+                condition_wait(&daemon->flush_flag,&daemon->lock);
+	    }
+        });                             /* end daemon->lock crit sec */
+
+    return did_init;
+}
+/* wait until truncation has processed all records up to time_stamp */
+rvm_return_t wait_for_truncation(log,time_stamp)
+    log_t           *log;               /* log descriptor */
+    struct timeval  *time_stamp;        /* time threshold */
+    {
+    log_daemon_t    *daemon = &log->daemon; /* deamon control descriptor */
+    log_status_t    *status = &log->status; /* log status descriptor */
+    rvm_bool_t      force_trunc = rvm_false; /* do syncronous truncation */
+    rvm_bool_t      exit_sw = rvm_false;
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    while (!exit_sw)
+        {
+        CRITICAL(daemon->lock,          /* begin daemon lock crit sec */
+            {
+            /* synchronous truncation if daemon not in use */
+            if ((daemon->truncate == 0) || (daemon->state == rvm_idle))
+                {
+                force_trunc = rvm_true;
+                goto exit_wait;
+                }
+
+            /* wait for concurrent truncation completion */
+            while (daemon->state == truncating)
+                {
+                condition_wait(&daemon->wake_up,&daemon->lock);
+                }
+            if (daemon->state == error)
+                {
+                retval = RVM_EINTERNAL; /* quit if daemon had error */
+                goto exit_wait;
+                }
+
+            /* see if records up to time threshold have been processed */
+            if ((time_stamp == NULL) ||
+                (TIME_GEQ(status->last_trunc,*time_stamp)))
+                goto exit_wait;         /* yes, exit */
+
+            /* no, must trigger another truncation */
+            daemon->state = truncating;
+            condition_signal(&daemon->code);
+            goto exit_crit_sec;
+
+exit_wait:  exit_sw = rvm_true;
+exit_crit_sec:;
+            });                         /* end daemon lock crit sec */
+        }
+
+    /* do synchronous truncation */
+    if (force_trunc)
+        retval = log_recover(log,&log->status.tot_sync_truncation,
+                             rvm_false,RVM_SYNC_TRUNCATE);
+
+    return retval;
+    }
+/* truncation daemon */
+void log_daemon(void *arg)
+    {
+    log_t           *log = arg;               /* log descriptor */
+    log_daemon_t    *daemon = &log->daemon; /* deamon control descriptor */
+    daemon_state_t  state;              /* daemon state code */
+    rvm_return_t    retval;
+
+#ifdef RVM_USELWP
+    PRE_Concurrent(1);
+#endif
+
+    DO_FOREVER
+        {
+        /* wait to be awakened by request */
+        CRITICAL(daemon->lock,          /* begin daemon lock crit sec */
+            {
+		daemon->state = rvm_idle;
+		condition_broadcast(&daemon->wake_up);
+		while (daemon->state == rvm_idle) {
+		    condition_wait(&daemon->code, &daemon->lock);
+		}
+		state = daemon->state;      /* end daemon lock crit sec */
+            });
+
+        /* process request */
+        switch (state)
+            {
+          case truncating:                /* do a truncation */
+            retval = log_recover(log,&log->status.tot_async_truncation,
+                                 rvm_true,RVM_ASYNC_TRUNCATE);
+
+            CRITICAL(daemon->lock, state = daemon->state);
+            if (state == error)
+                cthread_exit(retval);   /* error -- return code */
+            if (state != terminate) break;
+
+          case terminate:
+#ifdef RVM_USELWP
+	    daemon->thread = NULL;
+#endif
+            cthread_exit(RVM_SUCCESS);  /* normal exit */
+
+          default:    assert(rvm_false);    /* error */
+            }
+        }
+    }
diff --git a/rvm/rvm_logstatus.c b/rvm/rvm_logstatus.c
new file mode 100644
index 0000000..226ef5b
--- /dev/null
+++ b/rvm/rvm_logstatus.c
@@ -0,0 +1,1139 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                            RVM log status area support
+*
+*/
+#include <unistd.h>
+#include <sys/file.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include "rvm_private.h"
+
+#ifdef RVM_LOG_TAIL_BUG
+#include <rvmtesting.h>
+extern unsigned long *ClobberAddress;
+#endif /* RVM_LOG_TAIL_BUG */
+
+/* global variables */
+
+rvm_bool_t          rvm_utlsw;          /* true iff RVM called by rvmutl,
+                                           permits certain structures to be
+                                           retained after errors are discovered
+                                           */
+extern rvm_bool_t   rvm_no_update;      /* no segment or log update if true */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+
+extern rvm_length_t page_size;          /* system page size */
+extern rvm_length_t page_mask;          /* mask for rounding down to page size */
+extern rvm_length_t flush_times_vec[flush_times_len]; /* flush timing histogram defs */
+extern rvm_length_t truncation_times_vec[truncation_times_len]; /* truncation timing 
+                                                                   histogram defs */
+extern rvm_length_t range_lengths_vec[range_lengths_len]; /* range length
+                                                             histogram defs */
+extern rvm_length_t range_overlaps_vec[range_overlaps_len]; /* range coalesce
+                                                             histogram defs */
+extern rvm_length_t trans_overlaps_vec[trans_overlaps_len]; /* trans coalesce
+                                                             histogram defs */
+extern rvm_length_t range_elims_vec[range_elims_len]; /* ranges eliminated by range
+                                                         coalesce histogram defs */
+extern rvm_length_t trans_elims_vec[trans_elims_len]; /* ranges eliminated by trans
+                                                         coalesce histogram defs */
+extern rvm_length_t trans_coalesces_vec[trans_coalesces_len]; /* transactions
+                                                                 coalesed per flush */
+
+/* root of global log device list */
+log_t               *default_log;       /* default log descriptor ptr */
+
+#ifdef RVM_LOG_TAIL_SHADOW
+rvm_offset_t        log_tail_shadow;        /* shadow log tail pointer */
+rvm_offset_t        last_log_tail;          /* last committed log tail value */
+rvm_bool_t          last_log_valid = rvm_false; /* validity of last_log_tail */
+rvm_bool_t          has_wrapped = rvm_false;    /* whether or not we wrapped */
+char *log_tail_shadow_in_object = "Compiled with a shadow log tail offset\n";
+#endif /* RVM_LOG_TAIL_SHADOW */
+
+/* locals */
+
+static RVM_MUTEX    log_root_lock;      /* for list header, links & default */
+list_entry_t        log_root;           /* header for log descriptor list */
+
+static rvm_offset_t file_status_offset = /* log status area offset in files */
+    RVM_OFFSET_INITIALIZER(0,FILE_STATUS_OFFSET);
+
+static rvm_offset_t raw_status_offset = /* log status area offset in partitions */
+    RVM_OFFSET_INITIALIZER(0,RAW_STATUS_OFFSET);
+
+static rvm_offset_t min_trans_size =    /* minimum usable log size as offset */
+    RVM_OFFSET_INITIALIZER(0,MIN_TRANS_SIZE);
+/* log_root initialization */
+void init_log_list()
+    {
+    init_list_header(&log_root,log_id);
+    mutex_init(&log_root_lock);
+    default_log = (log_t *)NULL;
+    }
+
+/* enter new log in log list and establish default log if necessary */
+/* 
+  if we are looking for the RVM_LOG_TAIL_BUG, there can only ever
+  be one log.  I *believe* that it is possibly to only have one log
+  open at a time.  But, I'm not going to coda_assert that in the general 
+  case -bnoble 7/30/94
+*/
+
+void enter_log(log)
+    log_t           *log;               /* log descriptor */
+    {
+
+    assert(log != NULL);
+#ifdef RVM_LOG_TAIL_BUG
+    assert(default_log == NULL);
+#endif /* RVM_LOG_TAIL_BUG */
+    CRITICAL(log_root_lock,
+        {
+        (void)move_list_entry(NULL,(list_entry_t *)&log_root,
+                              log);
+        if (default_log == NULL)
+            default_log = log;
+        });
+
+#ifdef RVM_LOG_TAIL_BUG
+    /* 
+      this is massively unportable: for the moment, coda_assert we are
+      on pmax_mach. 
+    */
+#ifndef	__MACH__
+    assert(0);
+#endif	/* __MACH__ */
+#ifndef mips
+    assert(0);
+#endif /* mips */
+    ClobberAddress = &(default_log->status.log_tail.low);
+    protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    RVM_ASSIGN_OFFSET(log_tail_shadow,default_log->status.log_tail);
+    RVM_ASSIGN_OFFSET(last_log_tail,log_tail_shadow);
+    last_log_valid = rvm_true;
+#endif /* RVM_LOG_TAIL_SHADOW */
+
+    }
+
+/* find an existing log -- returns descriptor ptr or NULL */
+log_t *find_log(log_dev)
+    char            *log_dev;
+    {
+    log_t           *log;
+    char            *log_dev_fullname = log_dev;
+
+    assert(log_dev != NULL);
+    CRITICAL(log_root_lock,
+        {
+        FOR_ENTRIES_OF(log_root,log_t,log)
+            if (strcmp(log->dev.name,log_dev_fullname) == 0)
+                goto found;
+
+        log = NULL;
+found:;
+        });
+
+    return log;
+    }
+/* log daemon control */
+
+/* create daemon */
+static rvm_return_t fork_daemon(log_t *log) 
+{
+    log_daemon_t *daemon = &log->daemon; /* truncation daemon descriptor */
+
+    /* create daemon thread */
+    if (daemon->thread == (cthread_t)NULL)
+    {
+	mutex_lock(&daemon->lock);
+	daemon->truncate = 0;
+        daemon->state = rvm_idle;
+        daemon->thread = cthread_fork(log_daemon, log);
+	mutex_unlock(&daemon->lock);
+
+        if (daemon->thread == (cthread_t)NULL)
+            return RVM_ELOG;
+    }
+    return RVM_SUCCESS;
+}
+
+/* terminate daemon */
+static rvm_return_t join_daemon(log)
+    log_t           *log;
+    {
+    log_daemon_t    *daemon = &log->daemon; /* truncation daemon descriptor */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    if (daemon->thread != (cthread_t)NULL)
+        {
+        /* terminate the daemon */
+        CRITICAL(daemon->lock,          /* begin daemon lock crit sec */
+	    {
+            if (daemon->state != error)
+		{
+                daemon->state = terminate;
+		condition_signal(&daemon->code);
+		}
+            });                         /* end daemon lock crit sec */
+
+        /* wait for daemon thread to terminate */
+        retval = (rvm_return_t)cthread_join(daemon->thread);
+#ifdef RVM_USELWP
+        while(daemon->thread) cthread_yield();
+#endif
+	daemon->thread = (cthread_t)NULL;
+        }
+    daemon->truncate = 0;
+
+    return retval;
+    }
+/* set log truncation options */
+static rvm_return_t set_truncate_options(log,rvm_options)
+    log_t           *log;               /* log descriptor ptr */
+    rvm_options_t   *rvm_options;       /* optional options descriptor */
+    {
+    log_daemon_t    *daemon = &log->daemon; /* truncation daemon descriptor */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    if (rvm_utlsw)                      /* no log options allowed */
+        return RVM_SUCCESS;
+
+    /* set truncation threshold if parameter within range and
+       thread package installed */
+    if ((rvm_options->truncate > 0) && (rvm_options->truncate <= 100)
+        && (cthread_self() != (cthread_t)NULL))
+        {
+        /* update daemon thread */
+        retval = fork_daemon(log);      /* create daemon if necessary */
+        daemon->truncate = rvm_options->truncate;
+        }
+    else
+        retval = join_daemon(log);      /* terminate daemon */
+
+    return retval;
+    }
+/* close log device */
+rvm_return_t close_log(log)
+    log_t           *log;
+    {
+    log_special_t   *special;
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* make sure all transactions ended */
+    CRITICAL(log->tid_list_lock,        /* begin tid_list_lock crit sec */
+        {
+        if (LIST_NOT_EMPTY(log->tid_list))
+            retval = RVM_EUNCOMMIT;
+        });                             /* end tid_list_lock crit sec */
+    if (retval != RVM_SUCCESS) return retval;
+
+    /* issue terminate to daemon */
+    (void)join_daemon(log);             /* can we do something on error? */
+
+    /* flush log and close */
+    CRITICAL(log->truncation_lock,
+        {
+        if ((retval=flush_log(log,&log->status.n_flush))
+            == RVM_SUCCESS)
+            CRITICAL(log->dev_lock,
+                {
+                if ((retval=write_log_status(log,NULL))
+                    == RVM_SUCCESS)
+                    if (close_dev(&log->dev) < 0)
+                        retval = RVM_EIO;
+                });
+        });
+    if (retval != RVM_SUCCESS) return retval;
+    if (default_log == log) {
+#ifdef RVM_LOG_TAIL_BUG
+	unprotect_page__Fi(ClobberAddress);
+	ClobberAddress = 0;
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+	RVM_ZERO_OFFSET(log_tail_shadow);
+	RVM_ZERO_OFFSET(last_log_tail);
+	last_log_valid = rvm_false;
+#endif /* RVM_LOG_TAIL_SHADOW */
+	default_log = NULL;
+    }
+    /* kill unflushed log_special records */
+    UNLINK_ENTRIES_OF(log->special_list,log_special_t,special)
+        free_log_special(special);
+
+    /* free descriptor */
+    free_log(log);
+
+    return retval;
+    }
+/* termination close of all log devices */
+rvm_return_t close_all_logs()
+    {
+    log_t           *log;               /* log device descriptor ptr */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* cycle through log list */
+    CRITICAL(log_root_lock,             /* begin log_root_lock crit sec */
+        {
+        UNLINK_ENTRIES_OF(log_root,log_t,log)
+            {
+            if ((retval=close_log(log)) != RVM_SUCCESS)
+                break;
+            }
+        });                             /* end log_root_lock crit sec */
+
+    return retval;
+    }
+/* pre-load log raw i/o gather write buffer with tail log sector */
+static rvm_return_t preload_wrt_buf(log)
+    log_t           *log;               /* log descriptor */
+    {
+    device_t        *dev = &log->dev;   /* device descriptor ptr */
+    log_status_t    *status = &log->status; /* log status descriptor */
+    rvm_offset_t    tail_sector;        /* log tail sector */
+
+    tail_sector = CHOP_OFFSET_TO_SECTOR_SIZE(status->log_tail);
+    if (read_dev(dev,&tail_sector,dev->wrt_buf,SECTOR_SIZE) < 0)
+        return RVM_EIO;
+
+    dev->ptr = RVM_ADD_LENGTH_TO_ADDR(dev->wrt_buf,
+                   OFFSET_TO_SECTOR_INDEX(status->log_tail));
+    dev->buf_start = dev->ptr;
+    dev->sync_offset = status->log_tail;
+
+    return RVM_SUCCESS;
+    }
+/* create log descriptor and open log device */
+rvm_return_t open_log(dev_name,log_ptr,status_buf,rvm_options)
+    char            *dev_name;          /* name of log storage device */
+    log_t           **log_ptr;          /* addr of log descriptor ptr */
+    char            *status_buf;        /* optional i/o buffer */
+    rvm_options_t   *rvm_options;       /* optional options descriptor */
+    {
+    log_t           *log;               /* log descriptor ptr */
+    log_buf_t       *log_buf;           /* log buffer descriptor ptr */
+    device_t        *dev;               /* device descriptor ptr */
+    rvm_length_t    flags = O_RDWR;     /* device open flags */
+    rvm_options_t   local_options;      /* local options record */
+    rvm_return_t    retval;
+
+    /* build internal log structure */
+    if ((log = make_log(dev_name,&retval)) == NULL)
+        goto err_exit2;
+    dev = &log->dev;
+    log_buf = &log->log_buf;
+
+    /* allocate recovery buffers */
+    if (rvm_options == NULL)
+        {
+        rvm_options = &local_options;
+        rvm_init_options(rvm_options);
+        }
+    if ((long)(rvm_options->recovery_buf_len) < MIN_RECOVERY_BUF_LEN)
+        rvm_options->recovery_buf_len = MIN_RECOVERY_BUF_LEN;
+    log_buf->length=ROUND_TO_PAGE_SIZE(rvm_options->recovery_buf_len);
+    log_buf->aux_length = ROUND_TO_PAGE_SIZE(log_buf->length/2);
+    if ((retval=alloc_log_buf(log)) != RVM_SUCCESS)
+        return retval;
+
+    /* open the device and determine characteristics */
+    if (rvm_no_update) flags = O_RDONLY;
+    if (open_dev(dev,flags,0) != 0)
+        {
+        retval = RVM_EIO;
+        goto err_exit2;
+        }
+    if (set_dev_char(dev,NULL) < 0)
+        {
+        retval = RVM_EIO;
+        goto err_exit;
+        }
+    if (dev->raw_io) dev->num_bytes =   /* enought to read status area */
+        RVM_ADD_LENGTH_TO_OFFSET(raw_status_offset,
+                                 LOG_DEV_STATUS_SIZE);
+    /* open status area */
+    if ((retval=read_log_status(log,status_buf)) != RVM_SUCCESS)
+        {
+        if (rvm_utlsw) goto keep_log; /* keep damaged status */
+        goto err_exit;
+        }
+    log->status.trunc_state = 0;
+    log->status.flush_state = 0;
+        
+    /* create daemon truncation thread */
+    if ((retval=set_truncate_options(log,rvm_options))
+        != RVM_SUCCESS) goto err_exit;
+    /* raw i/o support */
+    if (dev->raw_io)
+        {
+        /* assign gather write buffer */
+        if ((long)(rvm_options->flush_buf_len) < MIN_FLUSH_BUF_LEN)
+            rvm_options->flush_buf_len = MIN_FLUSH_BUF_LEN;
+        dev->wrt_buf_len =
+            ROUND_TO_PAGE_SIZE(rvm_options->flush_buf_len);
+        dev->wrt_buf = page_alloc(dev->wrt_buf_len);
+        if (dev->wrt_buf == NULL)
+            {
+            retval = RVM_ENO_MEMORY;
+            goto err_exit;
+            }
+        dev->buf_end = RVM_ADD_LENGTH_TO_ADDR(dev->wrt_buf,
+                                              dev->wrt_buf_len);
+        
+        /* pre-load write buffer */
+        if ((retval=preload_wrt_buf(log)) != RVM_SUCCESS)
+            goto err_exit;
+        }
+
+    /* enter in log list*/
+keep_log:
+    enter_log(log);
+    *log_ptr = log;
+    return retval;
+
+err_exit:
+    (void)close_dev(dev);
+err_exit2:
+    free_log(log);
+    *log_ptr = (log_t *)NULL;
+    return retval;
+    }
+/* log options processing */
+rvm_return_t do_log_options(log_ptr,rvm_options)
+    log_t           **log_ptr;          /* addr of log descriptor ptr */
+    rvm_options_t   *rvm_options;       /* ptr to rvm options descriptor */
+    {
+    rvm_return_t    retval;
+    log_t           *log = NULL;
+    char            *log_dev;
+
+    if ((rvm_options == NULL) || (rvm_options->log_dev == NULL))
+        return RVM_SUCCESS;
+
+    /* see if need to build a log descriptor */
+    log_dev = rvm_options->log_dev;
+    if ((log=find_log(log_dev)) == NULL)
+        {
+        /* see if already have a log */
+        if (default_log != NULL)
+            return RVM_ELOG;
+        
+        /* build log descriptor */
+        if ((retval=open_log(log_dev,&log,NULL,rvm_options))
+            != RVM_SUCCESS) {
+		printf("open_log failed.\n");
+		return retval;
+	}
+        /* do recovery processing for log */
+        log->in_recovery = rvm_true;
+        if ((retval = log_recover(log,&log->status.tot_recovery,
+                                  rvm_false,RVM_RECOVERY)) != RVM_SUCCESS) {
+		printf("log_recover failed.\n");
+		return retval;
+	}
+
+        /* pre-load write buffer with new tail sector */
+        if (log->dev.raw_io)
+            {
+            CRITICAL(log->dev_lock,retval=preload_wrt_buf(log));
+            if (retval != RVM_SUCCESS) {
+		    return retval;
+		    printf("preload_wrt_buff failed\n");
+	    }
+            }
+        }
+
+    /* process options and return log descriptor if wanted */
+    retval = set_truncate_options(log,rvm_options);
+    if (log_ptr != NULL)
+        *log_ptr = log;
+
+    return retval;
+    }
+/* accumulate running statistics totals */
+void copy_log_stats(log)
+    log_t           *log;
+    {
+    log_status_t    *status = &log->status; /* status area descriptor */
+    rvm_length_t    i;
+    rvm_offset_t    temp;
+
+    assert(((&log->dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    /* sum epoch counts */
+    status->tot_abort += status->n_abort;
+    status->n_abort = 0;
+    status->tot_flush_commit += status->n_flush_commit;
+    status->n_flush_commit = 0;
+    status->tot_no_flush_commit += status->n_no_flush_commit;
+    status->n_no_flush_commit = 0;
+    status->tot_split += status->n_split;
+    status->n_split = 0;
+    status->tot_flush += status->n_flush;
+    status->n_flush = 0;
+    status->tot_rvm_flush += status->n_rvm_flush;
+    status->n_rvm_flush = 0;
+    status->tot_special += status->n_special;
+    status->n_special = 0;
+    status->tot_truncation_wait += status->n_truncation_wait;
+    status->n_truncation_wait = 0;
+    status->tot_range_elim += status->n_range_elim;
+    status->n_range_elim = 0;
+    status->tot_trans_elim += status->n_trans_elim;
+    status->n_trans_elim = 0;
+    status->tot_trans_coalesced += status->n_trans_coalesced;
+    status->n_trans_coalesced = 0;
+    status->tot_range_overlap =
+        RVM_ADD_OFFSETS(status->tot_range_overlap,
+                        status->range_overlap);
+    RVM_ZERO_OFFSET(status->range_overlap);
+    status->tot_trans_overlap =
+        RVM_ADD_OFFSETS(status->tot_trans_overlap,
+                        status->trans_overlap);
+    RVM_ZERO_OFFSET(status->trans_overlap);
+
+    /* sum length of log writes */
+    log_tail_length(log,&temp);
+    status->tot_log_written = RVM_ADD_OFFSETS(status->tot_log_written,
+                                              status->log_size);
+    status->tot_log_written = RVM_SUB_OFFSETS(status->tot_log_written,
+                                              temp);
+    /* sum cumulative histograms and zero current */
+    for (i=0; i < flush_times_len; i++)
+        {
+        status->tot_flush_times[i] += status->flush_times[i];
+        status->flush_times[i] = 0;
+        }
+    status->tot_flush_time = add_times(&status->tot_flush_time,
+                                       &status->flush_time);
+    for (i=0; i < range_lengths_len; i++)
+        {
+        status->tot_range_lengths[i] += status->range_lengths[i];
+        status->range_lengths[i] = 0;
+        status->tot_range_overlaps[i] += status->range_overlaps[i];
+        status->range_overlaps[i] = 0;
+        status->tot_trans_overlaps[i] += status->trans_overlaps[i];
+        status->trans_overlaps[i] = 0;
+        }
+
+    for (i=0; i < range_elims_len; i++)
+        {
+        status->tot_range_elims[i] += status->range_elims[i];
+        status->range_elims[i] = 0;
+        status->tot_trans_elims[i] += status->trans_elims[i];
+        status->trans_elims[i] = 0;
+        }
+    ZERO_TIME(status->flush_time);
+    }
+/* clear non-permenant log status area fields */
+void clear_log_status(log)
+    log_t           *log;
+    {
+    log_status_t    *status = &log->status; /* status area descriptor */
+ 
+    assert(((&log->dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    status->valid = rvm_true;
+    status->log_empty = rvm_true;
+    status->first_rec_num = 0;
+    status->last_rec_num = 0;
+    ZERO_TIME(status->first_uname);
+    ZERO_TIME(status->last_uname);
+    ZERO_TIME(status->last_commit);
+    ZERO_TIME(status->first_write);
+    ZERO_TIME(status->last_write);
+    ZERO_TIME(status->wrap_time);
+    ZERO_TIME(status->flush_time);
+    RVM_ZERO_OFFSET(status->prev_log_head);
+    RVM_ZERO_OFFSET(status->prev_log_tail);
+
+    copy_log_stats(log);
+    }
+/* log status block initialization */
+rvm_return_t init_log_status(log)
+    log_t           *log;               /* log descriptor */
+    {
+    rvm_length_t    i;
+    log_status_t    *status = &log->status; /* status area descriptor */
+    rvm_offset_t    *status_offset;     /* offset of status area */
+
+    /* initialize boundaries & size */
+    if (log->dev.raw_io) status_offset = &raw_status_offset;
+    else status_offset = &file_status_offset;
+    status->log_start = RVM_ADD_LENGTH_TO_OFFSET(*status_offset,
+                                                 LOG_DEV_STATUS_SIZE);
+    status->log_size = RVM_SUB_OFFSETS(log->dev.num_bytes,
+                                       status->log_start);
+
+    /* initialize head and tail pointers */
+    status->log_head = status->log_start;
+#ifdef RVM_LOG_TAIL_BUG
+    unprotect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    assert(RVM_OFFSET_EQL(log_tail_shadow,status->log_tail));
+#endif /* RVM_LOG_TAIL_SHADOW */
+    status->log_tail = status->log_start;
+#ifdef RVM_LOG_TAIL_SHADOW
+	RVM_ASSIGN_OFFSET(log_tail_shadow,status->log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+#ifdef RVM_LOG_TAIL_BUG
+    protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+    RVM_ZERO_OFFSET(status->prev_log_head);
+    RVM_ZERO_OFFSET(status->prev_log_tail);
+
+    /* init status variables */
+    clear_log_status(log);
+    make_uname(&status->status_init);   /* initialization timestamp */
+    status->last_trunc = status->status_init;
+    status->prev_trunc = status->status_init;
+    status->next_rec_num = 1;
+    status->log_dev_max = 0;
+    status->last_flush_time = 0;
+    status->last_truncation_time = 0;
+    status->last_tree_build_time = 0;
+    status->last_tree_apply_time = 0;
+
+    /* clear cumulative statistics */
+    status->tot_rvm_truncate = 0;
+    status->tot_async_truncation = 0;
+    status->tot_sync_truncation = 0;
+    status->tot_truncation_wait = 0;
+    status->tot_recovery = 0;
+    status->tot_abort = 0;
+    status->tot_flush_commit = 0;
+    status->tot_no_flush_commit = 0;
+    status->tot_split = 0;
+    status->tot_rvm_flush = 0;
+    status->tot_flush = 0;
+    status->tot_special = 0;
+    status->tot_wrap = 0;
+    status->tot_range_elim = 0;
+    status->tot_trans_elim = 0;
+    status->tot_trans_coalesced = 0;
+    RVM_ZERO_OFFSET(status->tot_range_overlap);
+    RVM_ZERO_OFFSET(status->tot_trans_overlap);
+    RVM_ZERO_OFFSET(status->tot_log_written);
+    /* clear timings and histograms */
+    ZERO_TIME(status->tot_flush_time);
+    ZERO_TIME(status->tot_truncation_time);
+    for (i=0; i < flush_times_len; i++)
+        status->tot_flush_times[i] = 0;
+    for (i=0; i < truncation_times_len; i++)
+        {
+        status->tot_tree_build_times[i] = 0;
+        status->tot_tree_apply_times[i] = 0;
+        status->tot_truncation_times[i] = 0;
+        }
+    for (i=0; i < range_lengths_len; i++)
+        {
+        status->tot_range_lengths[i] = 0;
+        status->tot_range_overlaps[i] = 0;
+        status->tot_trans_overlaps[i] = 0;
+        }
+    for (i=0; i < range_elims_len; i++)
+        {
+        status->tot_range_elims[i] = 0;
+        status->tot_trans_elims[i] = 0;
+        status->tot_trans_coalesces[i] = 0;
+        }
+
+    /* write the device areas */
+    return write_log_status(log,NULL);
+    }
+/* read log status area from log device */
+rvm_return_t read_log_status(log,status_buf)
+    log_t               *log;           /* log descriptor */
+    char                *status_buf;    /* optional i/o buffer */
+    {
+    log_status_t        *status = &log->status; /* status area descriptor */
+    rvm_offset_t        *status_offset; /* device status area offset */
+    log_dev_status_t    *dev_status;    /* status i/o area typed ptr */
+    char                status_io[LOG_DEV_STATUS_SIZE]; /* i/o buffer */
+    rvm_length_t        saved_chk_sum;  /* save area for checksum read */
+
+    /* read the status areas */
+    if (status_buf != NULL)
+        dev_status = (log_dev_status_t *)status_buf;
+    else {
+	BZERO(status_io, LOG_DEV_STATUS_SIZE); /* clear buffer */
+        dev_status = (log_dev_status_t *)status_io;
+    }
+    if (log->dev.raw_io) status_offset = &raw_status_offset;
+    else status_offset = &file_status_offset;
+    if (read_dev(&log->dev,status_offset,
+                  dev_status,LOG_DEV_STATUS_SIZE) < 0)
+        return RVM_EIO;
+
+    /* save old checksum and compute new */
+    saved_chk_sum = dev_status->chk_sum;
+    dev_status->chk_sum = 0;
+    dev_status->chk_sum = chk_sum((char *)dev_status,
+                                  LOG_DEV_STATUS_SIZE);
+
+    /* copy to log descriptor */
+    (void)BCOPY(&dev_status->status,(char *)status,
+                sizeof(log_status_t));  
+    status->valid = rvm_false;          /* status not valid until tail found */
+
+    /* compare checksum, struct_id, and version */
+    if ((dev_status->chk_sum != saved_chk_sum)
+        || (dev_status->struct_id != log_dev_status_id))
+        return RVM_ELOG;                /* status area damaged */
+    if (strcmp(dev_status->version,RVM_VERSION) != 0)
+        return RVM_ELOG_VERSION_SKEW;
+    if (strcmp(dev_status->log_version,RVM_LOG_VERSION) != 0)
+        return RVM_ELOG_VERSION_SKEW;
+    if (strcmp(dev_status->statistics_version,RVM_STATISTICS_VERSION) != 0)
+        return RVM_ESTAT_VERSION_SKEW;
+
+    /* set log device length to log size at creation */
+    if (log->dev.raw_io)
+        log->dev.num_bytes = RVM_ADD_OFFSETS(status->log_size,
+                                             status->log_start);
+    status->update_cnt = UPDATE_STATUS;
+    return RVM_SUCCESS;
+    }
+/* write log status area on log device */
+rvm_return_t write_log_status(log,dev)
+    log_t               *log;
+    device_t            *dev;           /* optional device */
+    {
+    log_status_t        *status = &log->status; /* status area descriptor */
+    rvm_offset_t        *status_offset; /* device status area offset */
+    log_dev_status_t    *dev_status;    /* status i/o area typed ptr */
+    char                status_io[LOG_DEV_STATUS_SIZE]; /* i/o buffer */
+
+    /* initializations */
+#ifdef RVM_LOG_TAIL_SHADOW
+    assert(RVM_OFFSET_EQL(log_tail_shadow,log->status.log_tail));
+    /* we'll check to see whether this log offest is before the
+       previous one.  If so, assert.  Some false assertions, but hey. */
+    if (last_log_valid == rvm_true) {
+	if (has_wrapped == rvm_true) {
+	    /* this log value should be LESS than the previous one */
+	    assert(RVM_OFFSET_GEQ(last_log_tail,log->status.log_tail));
+	    /* We've accounted for the log_wrap; reset it. */
+	    has_wrapped = rvm_false;
+	} else {
+	    /* this log value should be GREATER than the previous one */
+	    assert(RVM_OFFSET_LEQ(last_log_tail,log->status.log_tail));
+	}
+    } else {
+	last_log_valid = rvm_true;
+    }
+    RVM_ASSIGN_OFFSET(last_log_tail,log->status.log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+    if (dev == NULL) dev = &log->dev;
+    (void) BZERO(status_io, LOG_DEV_STATUS_SIZE); /* clear buffer */
+
+    /* set up device status i/o area */
+    status->update_cnt = UPDATE_STATUS;
+    make_uname(&status->status_write);
+    dev_status = (log_dev_status_t *)status_io;
+    dev_status->struct_id = log_dev_status_id;
+    (void)BCOPY((char *)status,&dev_status->status,
+                sizeof(log_status_t));
+    (void)strcpy(dev_status->version,RVM_VERSION);
+    (void)strcpy(dev_status->log_version,RVM_LOG_VERSION);
+    (void)strcpy(dev_status->statistics_version,
+                 RVM_STATISTICS_VERSION);
+
+    /* compute checksum */
+    dev_status->chk_sum = 0;
+    dev_status->chk_sum = chk_sum((char *)dev_status,
+                                  LOG_DEV_STATUS_SIZE);
+
+    /* write the status areas */
+    if (dev->raw_io) status_offset = &raw_status_offset;
+    else status_offset = &file_status_offset;
+    if (write_dev(dev,status_offset,dev_status,
+                  LOG_DEV_STATUS_SIZE,SYNCH) < 0)
+        return RVM_EIO;
+
+    return RVM_SUCCESS;
+    }
+/* consistency check for log head/tail ptrs */
+static rvm_bool_t chk_tail(log)
+    log_t           *log;
+    {
+    log_status_t    *status = &log->status; /* status area descriptor */
+
+    /* basic range checks -- current epoch */
+    assert(RVM_OFFSET_GEQ(status->log_tail,status->log_start));
+    assert(RVM_OFFSET_LEQ(status->log_tail,log->dev.num_bytes));
+    assert(RVM_OFFSET_GEQ(status->log_head,status->log_start));
+    assert(RVM_OFFSET_LEQ(status->log_head,log->dev.num_bytes));
+
+    /* basic range checks -- previous epoch */
+    if (!RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+        {
+        assert(RVM_OFFSET_EQL(status->log_head,
+                              status->prev_log_tail));
+        assert(RVM_OFFSET_GEQ(status->prev_log_tail,
+                              status->log_start));
+        assert(RVM_OFFSET_LEQ(status->prev_log_tail,
+                              log->dev.num_bytes));
+        assert(RVM_OFFSET_GEQ(status->prev_log_head,
+                              status->log_start));
+        assert(RVM_OFFSET_LEQ(status->prev_log_head,
+                              log->dev.num_bytes));
+        assert(RVM_OFFSET_EQL(status->prev_log_tail,
+                              status->log_head));
+        }
+    /* current <==> previous epoch consistency checks */
+    if (RVM_OFFSET_GTR(status->log_head,status->log_tail))
+        {                               /* current epoch wrapped */
+        assert(RVM_OFFSET_GEQ(status->log_head,status->log_tail));
+        if (!RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+            {                           /* check previous epoch */
+            assert(RVM_OFFSET_LEQ(status->prev_log_head,
+                                  status->prev_log_tail));
+            assert(RVM_OFFSET_GEQ(status->prev_log_head,
+                                  status->log_tail));
+            assert(RVM_OFFSET_GEQ(status->prev_log_head,
+                                  status->log_tail));
+            }
+        }
+    else
+        {                               /* current epoch not wrapped */
+        if (!RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+            {                           /* check previous epoch */
+            if (RVM_OFFSET_GTR(status->prev_log_head,
+                               status->prev_log_tail))
+                {                       /* previous epoch wrapped */
+                assert(RVM_OFFSET_GTR(status->prev_log_head,
+                                      status->log_tail));
+                assert(RVM_OFFSET_GEQ(status->prev_log_head,
+                                      status->log_tail));
+                }
+            else
+                {                       /* previous epoch not wrapped */
+                assert(RVM_OFFSET_GTR(status->log_head,
+                                      status->prev_log_head));
+                }
+            }
+        }
+
+    /* raw i/o buffer checks */
+    if (log->dev.raw_io)
+        {
+        assert((SECTOR_INDEX((long)log->dev.ptr)) ==
+               (OFFSET_TO_SECTOR_INDEX(status->log_tail)));
+        }
+
+    return rvm_true;
+    }
+rvm_return_t update_log_tail(log,rec_hdr)
+    log_t           *log;
+    rec_hdr_t       *rec_hdr;           /* header of last record */
+    {
+    log_status_t    *status = &log->status; /* status area descriptor */
+    rvm_length_t    temp;
+
+    assert(((&log->dev == &default_log->dev) && (!rvm_utlsw)) ?
+           (!LOCK_FREE(default_log->dev_lock)) : 1);
+
+    /* update unique name timestamps */
+    status->last_write = rec_hdr->timestamp;
+    if (TIME_EQL_ZERO(status->first_write))
+        status->first_write = status->last_write;
+
+    status->log_empty = rvm_false;
+    if (rec_hdr->struct_id != log_wrap_id)
+        {
+        /* update and check tail length */
+        temp = rec_hdr->rec_length+sizeof(rec_end_t);
+        assert(temp == log->dev.io_length);
+#ifdef RVM_LOG_TAIL_BUG
+	unprotect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    assert(RVM_OFFSET_EQL(log_tail_shadow,status->log_tail));
+#endif /* RVM_LOG_TAIL_SHADOW */
+        status->log_tail = RVM_ADD_LENGTH_TO_OFFSET(status->log_tail,
+                                                    temp);
+#ifdef RVM_LOG_TAIL_SHADOW
+	RVM_ASSIGN_OFFSET(log_tail_shadow,status->log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+#ifdef RVM_LOG_TAIL_BUG
+	protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+        assert(chk_tail(log));
+
+        /* update unames if transaction */
+        if (rec_hdr->struct_id == trans_hdr_id)
+            {
+            status->last_uname = ((trans_hdr_t *)rec_hdr)->uname;
+            if (TIME_EQL_ZERO(status->first_uname))
+                status->first_uname = status->last_uname;
+            }
+
+        /* count updates & update disk copies if necessary */
+        if (--status->update_cnt != 0)
+            return RVM_SUCCESS;
+        }
+
+    if (sync_dev(&log->dev) < 0)        /* sync file buffers before status write */
+        return RVM_EIO;
+
+    /* if tail wrapped around, correct pointers */
+    if (rec_hdr->struct_id == log_wrap_id)
+        {
+#ifdef RVM_LOG_TAIL_BUG
+        unprotect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    assert(RVM_OFFSET_EQL(log_tail_shadow,status->log_tail));
+#endif /* RVM_LOG_TAIL_SHADOW */
+        status->log_tail = status->log_start;
+#ifdef RVM_LOG_TAIL_SHADOW
+	RVM_ASSIGN_OFFSET(log_tail_shadow,status->log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+#ifdef RVM_LOG_TAIL_BUG
+        protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+        log->dev.sync_offset = status->log_start;
+        assert(chk_tail(log));
+        }
+
+    return write_log_status(log,NULL);  /* update disk status block */
+    }
+/* determine total length of log tail area */
+void log_tail_length(log,tail_length)
+    log_t           *log;               /* log descriptor */
+    rvm_offset_t    *tail_length;       /* length [out] */
+    {
+    log_status_t    *status = &log->status; /* status area descriptor */
+    rvm_offset_t    temp;
+
+    /* determine effective head */
+    if (!RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+        temp = CHOP_OFFSET_TO_SECTOR_SIZE(status->prev_log_head);
+    else                                /* no previous epoch */
+        temp = CHOP_OFFSET_TO_SECTOR_SIZE(status->log_head);
+
+    /* determine usable area */
+    if (RVM_OFFSET_GEQ(status->log_tail,status->log_head) &&
+        RVM_OFFSET_GEQ(status->log_tail,status->prev_log_head))
+        {
+        /* current not wrapped & previous not wrapped */
+        *tail_length = RVM_SUB_OFFSETS(log->dev.num_bytes,
+                                       status->log_tail);
+        if (RVM_OFFSET_LSS(*tail_length,min_trans_size))
+            RVM_ZERO_OFFSET(*tail_length);
+        *tail_length = RVM_ADD_OFFSETS(*tail_length,temp);
+        *tail_length = RVM_SUB_OFFSETS(*tail_length,status->log_start);
+        }
+    else
+        /* all other cases */
+        *tail_length = RVM_SUB_OFFSETS(temp,status->log_tail);
+
+    }
+/* determine length of log tail area usable in single write */
+void log_tail_sngl_w(log_t *log, rvm_offset_t *tail_length)
+{
+    log_status_t    *status = &log->status; /* status area descriptor */
+
+    /* determine effective head */
+    if (!RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+        *tail_length = CHOP_OFFSET_TO_SECTOR_SIZE(status->prev_log_head);
+    else                            /* no previous epoch */
+        *tail_length = CHOP_OFFSET_TO_SECTOR_SIZE(status->log_head);
+
+    /* determine effective end of useable area if
+        neither current nor previous wrapped */
+    if (RVM_OFFSET_GEQ(status->log_tail,status->log_head) &&
+        RVM_OFFSET_GEQ(status->log_tail,status->prev_log_head))
+        *tail_length = log->dev.num_bytes;
+
+    /* subtract current current tail & verify log ptrs */
+    *tail_length = RVM_SUB_OFFSETS(*tail_length,status->log_tail);
+    assert(chk_tail(log));
+}
+/* determine length of log currently in use */
+void cur_log_length(log,length)
+    log_t           *log;               /* log descriptor */
+    rvm_offset_t    *length;            /* length [out] */
+    {
+    log_status_t    *status = &log->status; /* log status area descriptor */
+
+    if (RVM_OFFSET_GEQ(status->log_tail,status->log_head))
+        *length = RVM_SUB_OFFSETS(status->log_tail,status->log_head);
+    else
+        {
+        *length = RVM_SUB_OFFSETS(log->dev.num_bytes,status->log_head);
+        *length = RVM_ADD_OFFSETS(*length,status->log_tail);
+        *length = RVM_SUB_OFFSETS(*length,status->log_start);
+        }
+    }
+
+/* determine percentage of log currently in use */
+long cur_log_percent(log,space_needed)
+    log_t           *log;               /* log descriptor */
+    rvm_offset_t    *space_needed;      /* space neded immediately */
+    {
+    log_status_t    *status = &log->status; /* log status area descriptor */
+    float           cur_size;           /* current size of log as float */
+    rvm_length_t    cur_percent;        /* current franction of log used (%) */
+    rvm_offset_t    temp;               /* log free space calculation temp */
+
+    CRITICAL(log->dev_lock,             /* begin dev_lock crit sec */
+        {
+        /* find out how much space is there now & set high water mark */
+        log_tail_length(log,&temp);
+        temp = RVM_SUB_OFFSETS(status->log_size,temp);
+        cur_size = OFFSET_TO_FLOAT(temp);
+        cur_percent = (long)(100.0*(cur_size/
+                                    OFFSET_TO_FLOAT(status->log_size)));
+        assert((cur_percent >= 0) && (cur_percent <= 100));
+        if (cur_percent > status->log_dev_max)
+            status->log_dev_max = cur_percent;
+
+        /* if space_needed specified, recompute percentage */
+        if (space_needed != NULL)
+            {
+            temp = RVM_ADD_OFFSETS(temp,*space_needed);
+            cur_size = OFFSET_TO_FLOAT(temp);
+            cur_percent = (long)(100.0*(cur_size/
+                                    OFFSET_TO_FLOAT(status->log_size)));
+            }
+        });                             /* end dev_lock crit sec */
+
+    return cur_percent;
+    }
+/* rvm_create_log application interface */
+rvm_return_t rvm_create_log(rvm_options,log_len,mode)
+    rvm_options_t   *rvm_options;       /* ptr to options record */
+    rvm_offset_t    *log_len;           /* length of log data area */
+    long            mode;               /* file creation protection mode */
+    {
+    log_t           *log;               /* descriptor for log */
+    rvm_offset_t    offset;             /* offset temporary */
+    char            *end_mark = "end";
+    long            save_errno;
+    rvm_return_t    retval;
+
+    if ((retval=bad_options(rvm_options,rvm_true)) != RVM_SUCCESS)
+        return retval;                  /* bad options ptr or record */
+    if (rvm_options == NULL)
+        return RVM_EOPTIONS;            /* must have an options record */
+
+    /* check length of file name */
+    if (strlen(rvm_options->log_dev) >= MAXPATHLEN)
+        return RVM_ENAME_TOO_LONG;
+
+    /* check that log file length is legal */
+    offset = RVM_ADD_LENGTH_TO_OFFSET(*log_len,
+    	    	 LOG_DEV_STATUS_SIZE+FILE_STATUS_OFFSET);
+    offset = CHOP_OFFSET_TO_SECTOR_SIZE(offset);
+    if (RVM_OFFSET_HIGH_BITS_TO_LENGTH(offset) != 0)
+        return RVM_ETOO_BIG;
+
+    /* be sure not an already declared log */
+    if (find_log(rvm_options->log_dev) != NULL)
+        return RVM_ELOG;
+
+    /* build a log descriptor and create log file*/
+    if ((log=make_log(rvm_options->log_dev,&retval)) == NULL)
+        return retval;
+#ifdef RVM_LOG_TAIL_BUG
+    /* 
+      We only need to track the log descriptor while we are 
+      building it.  It isn't going to be inserted into the list
+      until later, so ClobberAddress won't be set properly.
+    */
+    ClobberAddress = &(log->status.log_tail.low);
+    protect_page__Fi(ClobberAddress);
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    RVM_ASSIGN_OFFSET(log_tail_shadow,log->status.log_tail);
+#endif /* RVM_LOG_TAIL_SHADOW */
+    if (open_dev(&log->dev,O_WRONLY,mode) == 0) /* don't allow create yet */
+        {
+        retval = RVM_ELOG;              /* error -- file already exists */
+        goto err_exit;
+        }
+    if (errno != ENOENT)
+        {
+        retval = RVM_EIO;               /* other i/o error, errno specifies */
+        goto err_exit;
+        }
+    if (open_dev(&log->dev,O_WRONLY | O_CREAT,mode) != 0)
+        {                               /* do real create */
+        retval = RVM_EIO;
+        goto err_exit;
+        }
+    /* force file length to specified size by writting last byte */
+    log->dev.num_bytes = offset;
+    offset = RVM_SUB_LENGTH_FROM_OFFSET(offset,strlen(end_mark));
+    if (write_dev(&log->dev,&offset,end_mark,
+                  strlen(end_mark),NO_SYNCH) < 0)
+        {
+        retval = RVM_EIO;
+        goto err_exit;
+        }
+
+    /* complete initialization */
+    retval = init_log_status(log);
+
+err_exit:
+    if (log->dev.handle != 0)
+        {
+        save_errno = errno;
+        (void)close_dev(&log->dev);
+        errno = save_errno;
+        }
+#ifdef RVM_LOG_TAIL_BUG
+    /* drop the "temporary" clobber address */
+    unprotect_page__Fi(ClobberAddress);
+    ClobberAddress = 0;
+#endif /* RVM_LOG_TAIL_BUG */
+#ifdef RVM_LOG_TAIL_SHADOW
+    RVM_ZERO_OFFSET(log_tail_shadow);
+#endif /* RVM_LOG_TAIL_SHADOW */
+    free_log(log);
+
+    return retval;
+    }
+/* special routines for basher */
+rvm_offset_t rvm_log_head()
+    {
+    return default_log->status.log_head;
+    }
+
+rvm_offset_t rvm_log_tail()
+    {
+    return default_log->status.log_tail;
+    }
+
+rvm_length_t rvm_next_rec_num()
+    {
+    return default_log->status.next_rec_num;
+    }
diff --git a/rvm/rvm_lwp.h b/rvm/rvm_lwp.h
new file mode 100644
index 0000000..350e578
--- /dev/null
+++ b/rvm/rvm_lwp.h
@@ -0,0 +1,122 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/* LWP compatability for RVM */
+#pragma once
+
+#ifndef __MERO_RVM_RVM_LWP_H__
+#define __MERO_RVM_RVM_LWP_H__
+
+#include <lwp/lwp.h>
+#include <lwp/lock.h>
+
+#ifndef MACRO_BEGIN
+#define MACRO_BEGIN			do {
+#define MACRO_END			} while(0)
+#endif /* MACRO_BEGIN */
+
+#define RVM_STACKSIZE	1024 * 16
+#define BOGUSCODE 	(BOGUS_USE_OF_CTHREADS)  /* force compilation error */
+
+#define RVM_MUTEX			struct Lock
+#define RVM_CONDITION			char
+#define	MUTEX_INITIALIZER		{0, 0, 0, 0}
+/* Supported cthread definitions */
+
+#define cthread_t			PROCESS
+static inline PROCESS cthread_fork(void (*fname)(void *), void *arg)
+{
+    PROCESS rvm_lwppid;
+    LWP_CreateProcess(fname, RVM_STACKSIZE, LWP_NORMAL_PRIORITY,
+		      arg, "rvm_thread", &rvm_lwppid);
+    return rvm_lwppid;
+}
+#define cthread_init()			MACRO_BEGIN \
+                                          LWP_Init(LWP_VERSION, \
+                                                   LWP_NORMAL_PRIORITY, \
+                                                   NULL); \
+                                          IOMGR_Initialize(); \
+                                        MACRO_END
+#define cthread_exit(retval)		return
+#define cthread_yield()			MACRO_BEGIN \
+                                          IOMGR_Poll(); \
+                                          LWP_DispatchProcess(); \
+                                        MACRO_END
+#define cthread_join(thread)		(0)
+
+#define condition_wait(c, m)		MACRO_BEGIN \
+                                          ReleaseWriteLock((m)); \
+					  LWP_WaitProcess((c)); \
+					  ObtainWriteLock((m)); \
+                                        MACRO_END
+#define condition_signal(c)		(LWP_SignalProcess((c)))
+#define condition_broadcast(c)		(LWP_SignalProcess((c)))
+#define condition_clear(c)		/* nop  */
+#define condition_init(c)		/* nop */
+#define mutex_init(m)			(Lock_Init(m))
+#define mutex_clear(m)			/* nop */
+#define LOCK_FREE(m)			(!WriteLocked(&(m)))
+
+static inline PROCESS cthread_self(void)
+{
+    PROCESS rvm_lwppid;
+    LWP_CurrentProcess(&rvm_lwppid);
+    return rvm_lwppid;
+}
+/* synchronization tracing definitions of lock/unlock */
+#ifdef DEBUGRVM
+#define mutex_lock(m)			MACRO_BEGIN \
+                                         printf("mutex_lock OL(0x%x)%s:%d...", \
+                                          (m), __FILE__, __LINE__); \
+                                         ObtainWriteLock((m)); \
+                                         printf("done\n"); \
+                                        MACRO_END
+#define mutex_unlock(m)			MACRO_BEGIN \
+                                         printf("mutex_unlock RL(0x%x)%s:%d...",\
+                                          (m), __FILE__, __LINE__); \
+                                         ReleaseWriteLock((m)); \
+                                         printf("done\n"); \
+                                        MACRO_END
+#else /* !DEBUGRVM */
+#define mutex_lock(m)			ObtainWriteLock((m))
+#define mutex_unlock(m)			ReleaseWriteLock((m))
+#endif /* !DEBUGRVM */
+/* Unsupported cthread calls */
+
+#define	mutex_alloc()			BOGUSCODE
+#define	mutex_set_name(m, x)		BOGUSCODE
+#define	mutex_name(m)			BOGUSCODE
+#define	mutex_free(m)			BOGUSCODE
+
+#define	condition_alloc()		BOGUSCODE
+#define	condition_set_name(c, x)	BOGUSCODE
+#define	condition_name(c)		BOGUSCODE
+#define	condition_free(c)		BOGUSCODE
+
+#define cthread_detach()		BOGUSCODE
+#define cthread_sp()			BOGUSCODE
+#define	cthread_assoc(id, t)		BOGUSCODE
+#define cthread_set_name		BOGUSCODE
+#define cthread_name			BOGUSCODE
+#define cthread_count()			BOGUSCODE
+#define cthread_set_limit		BOGUSCODE
+#define cthread_limit()			BOGUSCODE
+#define	cthread_set_data(t, x)		BOGUSCODE
+#define	cthread_data(t)			BOGUSCODE
+
+#endif /* __MERO_RVM_RVM_LWP_H__ */
diff --git a/rvm/rvm_map.c b/rvm/rvm_map.c
new file mode 100644
index 0000000..5d6dfb7
--- /dev/null
+++ b/rvm/rvm_map.c
@@ -0,0 +1,1091 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                   RVM Mapping and Unmapping
+*
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#ifdef __STDC__
+#include <stdlib.h>
+#else
+#include <libc.h>
+#endif
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <sys/file.h>
+#include <sys/mman.h>
+#ifndef HAVE_GETPAGESIZE /* defined(__linux__) && defined(sparc) */
+#include <asm/page.h>
+#define getpagesize() PAGE_SIZE
+#endif
+#if defined(hpux) || defined(__hpux)
+#include <hp_bsd.h>
+#endif /* hpux */
+#include <unistd.h>
+#include <stdlib.h>
+#include <errno.h>
+#include "rvm/rvm_private.h"
+
+#ifdef __CYGWIN32__
+#include <windows.h>
+#endif
+
+/* global variables */
+
+extern log_t        *default_log;       /* default log descriptor ptr */
+extern rvm_bool_t   rvm_no_update;      /* no segment or log update if true */
+extern rvm_bool_t   rvm_map_private;    /* Do we want to map private? */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+
+/* root of segment list and region tree */
+list_entry_t        seg_root;           /* global segment list */
+rw_lock_t           seg_root_lock;      /* lock for segment list header & links */
+
+rw_lock_t           region_tree_lock;   /* lock for region tree */
+tree_root_t         region_tree;        /* root of mapped region tree */
+
+list_entry_t        page_list;          /* list of usable pages */
+RVM_MUTEX           page_list_lock;     /* lock for usable page list */
+rvm_length_t        page_size;          /* system page size */
+rvm_length_t        page_mask;          /* mask for rounding down to page size */
+
+/* locals */
+static long         seg_code = 1;       /* segment short names */
+static RVM_MUTEX    seg_code_lock;      /* lock for short names generator */
+list_entry_t        page_list;          /* list of usable pages */
+RVM_MUTEX           page_list_lock;     /* lock for usable page list */
+
+/* basic page, segment lists and region tree initialization */
+void init_map_roots()
+{
+    init_list_header(&seg_root,seg_id);
+    init_rw_lock(&seg_root_lock);
+    init_rw_lock(&region_tree_lock);
+    init_tree_root(&region_tree);
+    mutex_init(&seg_code_lock);
+
+#ifdef HAVE_MMAP
+    /* get page size */
+    page_size = (rvm_length_t)getpagesize();
+#else
+    { SYSTEM_INFO nt_info;
+      GetSystemInfo(&nt_info);
+      page_size = (rvm_length_t)nt_info.dwAllocationGranularity;
+    }
+#endif
+    page_mask = ~(page_size - 1);
+    mutex_init(&page_list_lock);
+    init_list_header(&page_list,free_page_id);
+}
+
+/* check validity of rvm_region record & ptr */
+rvm_return_t bad_region(rvm_region)
+    rvm_region_t    *rvm_region;
+{
+    if (rvm_region == NULL)
+        return RVM_EREGION;
+    if (rvm_region->struct_id != rvm_region_id)
+        return RVM_EREGION;
+
+    if (rvm_region->data_dev != NULL)
+        if (strlen(rvm_region->data_dev) > (MAXPATHLEN-1))
+            return RVM_ENAME_TOO_LONG;
+
+    return RVM_SUCCESS;
+}
+
+#define PAGE_ALLOC_DEFINED
+#include <sys/types.h>
+#include <sys/mman.h>
+#include "rvm/coda_mmap_anon.h"
+
+/*
+ * Page table management code
+ *
+ * This code is used by the page allocation code in RVM to track what
+ * regions of memory have been allocated for use in the persistent heap.
+ *
+ * In the original Mach specific code, this was gotten for "free" via
+ * a hack which called vm_allocate to reallocate the block in question.
+ * if the reallocation failed, the block had been allocated. if it
+ * succeeded, the block had not been allocated (and, since we had just
+ * allocated it, we quickly reallocated it and wiped the egg off of our
+ * faces).
+ *
+ * The original BSD44 port of this attempted to take advantage of the
+ * fact that if mmap() is called with the MAP_FIXED flag, it would
+ * attempt to allocate exactly the region of memory in question. Supposedly,
+ * if the region was already allocated, this mmap() call would fail.
+ *
+ * This solution turns out to be NOT CORRECT. Not only does BSD44 not
+ * perform in this fashion (it will deallocate whatever was there beforehand,
+ * silently), but there is another complication. If the application has
+ * allocated memory in that space, it could cause an erroneous result from
+ * the mem_chk() function. Since mmap() (if it behaved as originally beleived)
+ * would not be able to allocate the space, it would assume it is a mapped
+ * region. But, since it ISN'T a mapped region, just an allocated region,
+ * the result is incorrect.
+ *
+ * One more factor which complicates adding what would otherwise be a
+ * fairly straightforward list of allocated regions is that there are
+ * two places in RVM where memory is allocated. One is in the RVM
+ * library (page_alloc() and page_free(), both in rvm_map.c), and the
+ * other is in the SEG segment loader library (allocate_vm() and
+ * deallocate_vm(), both in rvm_segutil.c).
+ *
+ * --tilt, Nov 19 1996
+ */
+
+/* This is a doubly-linked list of allocated regions of memory. The regions
+   are stored in increasing order, so that once you have passed the area
+   where a questionable region has been stored, you can stop looking. */
+rvm_page_entry_t *rvm_allocations      = NULL; /* allocated pages */
+rvm_page_entry_t *rvm_allocations_tail = NULL; /* tail of list */
+
+/*
+ * rvm_register_page -- registers a page as being allocated.
+ *                      returns rvm_true if the page is successfully registered
+ *                      returns rvm_false is the page is already allocated
+ *
+ * TODO: should add optimization which coalesces page records.
+ *       should round end up to be at end of page boundary.
+ */
+rvm_bool_t rvm_register_page(char *vmaddr, rvm_length_t length)
+{
+    rvm_page_entry_t *bookmark, *entry;
+    char *end = vmaddr + length - 1;
+
+    if(rvm_allocations == NULL) {
+	/* There are no other allocated pages, so this is the trivial case */
+	entry = (rvm_page_entry_t *) malloc(sizeof(rvm_page_entry_t));
+	assert(entry != NULL);
+	entry->start    = vmaddr;
+	entry->end      = end;
+	entry->prev     = NULL;	 /* indicates beginning of list */
+	entry->next     = NULL;	 /* indicates end of list */
+	rvm_allocations      = entry; /* set this to be head of list */
+	rvm_allocations_tail = entry; /* also set it to be the tail */
+	return(rvm_true);
+    }
+
+    /* XXX check if tail is before this region for "quick" verification */
+
+    /* search through the rvm_allocations list to find either
+       a) where this should go, or b) a region which has already been
+       registered which contains this region. */
+
+    bookmark = rvm_allocations;
+    while(bookmark != NULL) {
+	/* check for various bad conditions: */
+
+	/* case one: the start of the new region falls within
+	   a previously allocated region */
+	if( (bookmark->start <= vmaddr) && (vmaddr <= bookmark->end) ) {
+	    printf("Case one.\n");
+	    return(rvm_false);
+	}
+
+	/* case two: the end of the new region falls within
+	   a previously allocated region */
+	if ( (bookmark->start <= end)   && (end    <= bookmark->end) ) {
+	    printf("Case two.\n");
+	    return(rvm_false);
+	}
+
+	/* case three: the new region spans a currently allocated region
+	   (n.b.: the case where the new region is contained within a
+	          currently allocated region is handled by case one) */
+	if ( (vmaddr <= bookmark->start) && (bookmark->end <= end) ) {
+	    printf("Case three.\n");
+	    return(rvm_false);
+	}
+
+	/* check to see if we're at the right place to insert this page.
+	   we can do this by seeing if the end of the new region is
+	   before the beginning of this one. if so, insert the new
+	   region before the one we're currently looking at. */
+	if(end < bookmark->start) {
+	    entry = (rvm_page_entry_t *) malloc(sizeof(rvm_page_entry_t));
+	    assert(entry != NULL);
+	    entry->start    = vmaddr;
+	    entry->end      = end;
+	    entry->prev     = bookmark->prev; /* insert the new entry */
+	    entry->next     = bookmark;	      /* between bookmark and */
+	    if (bookmark->prev != NULL)
+		bookmark->prev->next = entry;
+	    else
+		/* bookmark must be the head of the list */
+		rvm_allocations = entry;
+	    bookmark->prev  = entry;          /* the entry before bookmark */
+	    return(rvm_true);
+	}
+
+	/* if we're at the end, and we haven't tripped yet, we should
+	   put the entry at the end */
+	if(bookmark->next == NULL) {
+	    entry = (rvm_page_entry_t *) malloc(sizeof(rvm_page_entry_t));
+	    assert(entry != NULL);
+	    entry->start    = vmaddr;
+	    entry->end      = end;
+	    entry->prev     = bookmark;       /* insert the new entry */
+	    entry->next     = NULL;	      /* after bookmark */
+	    bookmark->next  = entry;
+	    rvm_allocations_tail = entry;     /* set the new tail */
+	    return(rvm_true);
+	} else {
+	    bookmark = bookmark->next;
+	}
+    } /* end while */
+
+    /* we shouldn't be able to get here. */
+    assert(rvm_false);
+    return(rvm_false);
+}
+
+/*
+ * rvm_unregister_page -- removes a previously registered page from the
+ *                        list of registered pages. returns true if the page is
+ *                        successfully unregistered; returns false if the
+ *                        page was not previously allocated.
+ */
+rvm_bool_t rvm_unregister_page(char *vmaddr, rvm_length_t length)
+{
+    rvm_page_entry_t *entry, *previous_entry, *next_entry;
+
+    entry = find_page_entry(vmaddr);
+    if(entry == NULL)
+	return(rvm_false);
+
+    if ( (entry->start != vmaddr) ||
+         (entry->end   != (vmaddr + length - 1)) ) {
+	/* this isn't an exact match.
+	   as long we don't do coalescing of region entries,
+	   this means we should return false */
+	return(rvm_false);
+    }
+
+    /* if entry != NULL, we've found the page we're unregistering.
+       remove it from the list. */
+    previous_entry = entry->prev;
+    next_entry     = entry->next;
+
+    /* set the entries before and after this one skip over this one */
+    if(previous_entry == NULL) {
+	/* this is at the beginning of the list of allocated pages */
+	rvm_allocations = next_entry;
+    } else {
+	previous_entry->next = next_entry;
+    }
+
+    if(next_entry != NULL)
+	next_entry->prev = previous_entry;
+
+    /* free this entry */
+    free(entry);
+
+    return(rvm_true);
+}
+
+/*
+ * find_page_entry -- this returns the first entry which contains
+ *                    the beginning of the requested region.
+ *                    these somewhat peculiar semantics allow
+ *                    us to support both rvm_unregister_page and
+ *                    chk_mem, which need slightly different things.
+ */
+rvm_page_entry_t *find_page_entry(char *vmaddr)
+{
+    rvm_page_entry_t *bookmark;
+
+    bookmark = rvm_allocations;
+
+    while(bookmark != NULL) {
+	if( (bookmark->start <= vmaddr) && (vmaddr <= bookmark->end) ) {
+	    return(bookmark);
+	}
+
+	bookmark = bookmark->next;
+    }
+
+    return(NULL);
+}
+
+
+/* BSD44 page allocator */
+char *page_alloc(len)
+    rvm_length_t    len;
+    {
+    char           *vmaddr;
+    /* printf ("page_alloc(%ul)\n", len); */
+#ifdef HAVE_MMAP
+    mmap_anon(vmaddr, NULL, len, PROT_READ | PROT_WRITE);
+#else
+    {
+      HANDLE hMap = CreateFileMapping((HANDLE)0xFFFFFFFF, NULL,
+                                      PAGE_READWRITE, 0, len, NULL);
+      assert(hMap != NULL);
+      vmaddr = MapViewOfFile(hMap, FILE_MAP_READ | FILE_MAP_WRITE, 0, 0, 0);
+      assert(vmaddr != NULL);
+      CloseHandle(hMap);
+    }
+#endif
+    if (vmaddr == (char *)-1)
+        {
+	if (errno == ENOMEM)
+            {
+	    vmaddr = NULL;
+	    }
+	else
+	    {
+	    assert(rvm_false);  /* Unknown error condition */
+   	    }
+        }
+
+    /* modified by tilt, Nov 19 1996.
+       When we allocate a page (or range of pages) we register
+       it in an internal table we're keeping around to keep
+       track of pages. (The previous solution was to try to
+       re-allocate the page, and see if it fails, which is
+       not only wrong [since we don't if it's allocated, or
+       actually allocated in the RVM heap!!], but doesn't
+       work with mmap()). */
+    if (rvm_register_page(vmaddr, len) == rvm_false)
+        {
+	assert(rvm_false);	/* Registering shouldn't have failed */
+	}
+
+    return vmaddr;
+    }
+
+
+/* BSD44 page deallocator */
+void page_free(vmaddr, length)
+    char            *vmaddr;
+    rvm_length_t     length;
+    {
+#ifdef HAVE_MMAP
+	if (munmap(vmaddr, length)) {
+	    assert(0); /* should never fail */
+	}
+#else
+	UnmapViewOfFile(vmaddr);
+#endif
+
+	if (rvm_unregister_page(vmaddr, length) == rvm_false) {
+	    assert(0); /* should never fail */
+	}
+    }
+
+/*
+ * mem_chk -- verifies that the memory region in question
+ *            is actually addressable as part of RVM.
+ *            this means either that it is on the list,
+ *            or it is wholly contained by one or more list entries.
+ */
+rvm_bool_t mem_chk(char *vmaddr, rvm_length_t length)
+{
+    rvm_page_entry_t *entry;
+    char *start = vmaddr;
+    char *end   = vmaddr + length - 1;
+
+    while(rvm_true) {
+	entry = find_page_entry(start);
+	if(entry == NULL)
+	    return(rvm_false);
+
+	if(end <= entry->end)
+	    return(rvm_true);
+
+	start = entry->end + 1;	/* XXX possible problems with
+				       pages that aren't fully
+				       allocated. burn that
+				       bridge when we get to it. */
+    }
+
+    assert(rvm_false);
+    return(rvm_false);		/* shouldn't be able to get here */
+}
+
+/* segment short name generator */
+static long make_seg_code()
+{
+	long            retval;
+
+	CRITICAL(seg_code_lock,            /* begin seg_code_lock crit sec */
+		 {
+                                        /* probably indivisible on CISC */
+			 retval = seg_code++;            /* machines, but we can't RISC it, */
+			 /* so we lock it... */
+
+		 });                             /* end seg_code_lock crit sec */
+
+	return retval;
+}
+
+/* open segment device and set device characteristics */
+long open_seg_dev(seg,dev_length)
+    seg_t           *seg;               /* segment descriptor */
+    rvm_offset_t    *dev_length;        /* optional device length */
+    {
+    rvm_length_t    flags = O_RDWR;     /* device open flags */
+    long            retval;
+
+    if (rvm_no_update) flags = O_RDONLY;
+    if ((retval=open_dev(&seg->dev,flags,0)) < 0)
+        return retval;
+    if ((retval=set_dev_char(&seg->dev,dev_length)) < 0)
+        close_dev(&seg->dev);
+
+    return retval;
+    }
+
+long close_seg_dev(seg)
+    seg_t           *seg;               /* segment descriptor */
+    {
+
+    return close_dev(&seg->dev);
+
+    }
+
+/* close segment devices at termination time */
+rvm_return_t close_all_segs()
+    {
+    seg_t           *seg;               /* segment desriptor */
+    rvm_return_t    retval=RVM_SUCCESS; /* return value */
+
+    RW_CRITICAL(seg_root_lock,w,        /* begin seg_root_lock crit section */
+        {
+        FOR_ENTRIES_OF(seg_root,seg_t,seg)
+            {
+            CRITICAL(seg->dev_lock,     /* begin seg->dev_lock crit section */
+                {
+                if (close_seg_dev(seg) < 0)
+                    retval = RVM_EIO;
+                });                     /* end seg->dev_lock crit section */
+            if (retval != RVM_SUCCESS)
+                break;
+            }
+        });                             /* end seg_root_lock crit section */
+
+    return retval;
+    }
+
+/* segment lookup via device name */
+seg_t *seg_lookup(dev_name,retval)
+    char            *dev_name;          /* segment device name */
+    rvm_return_t    *retval;
+    {
+    char            full_name[MAXPATHLEN+1];
+    seg_t           *seg = NULL;
+
+    /* get full path name for segment device */
+    (void)make_full_name(dev_name,full_name,retval);
+    if (*retval != RVM_SUCCESS)
+        return NULL;
+
+    /* search segment list for full_name */
+    RW_CRITICAL(seg_root_lock,r,        /* begin seg_root_lock crit section */
+        {
+        FOR_ENTRIES_OF(seg_root,seg_t,seg)
+            if (!strcmp(seg->dev.name,full_name))
+                break;                  /* found */
+        });                             /* end seg_root_lock crit section */
+
+    if (!seg->links.is_hdr)
+        return seg;                     /* return found seg descriptor */
+    else
+        return NULL;
+    }
+
+/* enter segment short name definition in log */
+rvm_return_t define_seg(log,seg)
+    log_t           *log;               /* log descriptor */
+    seg_t           *seg;               /* segment descriptor */
+    {
+    log_seg_t       *log_seg;           /* special log segment entry */
+    log_special_t   *special;           /* allocation for log_seg */
+    long            name_len;           /* byte length of segment name */
+    rvm_return_t    retval;             /* return code */
+
+    /* make segment definition record */
+    name_len = strlen(seg->dev.name);
+    special=make_log_special(log_seg_id,name_len+1);
+    if (special == NULL)
+        return RVM_ENO_MEMORY;          /* can't get descriptor */
+
+    /* complete record and enter in log */
+    log_seg = &special->special.log_seg;
+    log_seg->seg_code = seg->seg_code;
+    log_seg->num_bytes = seg->dev.num_bytes;
+    log_seg->name_len = name_len;
+    (void)strcpy(log_seg->name,seg->dev.name);
+    if ((retval=queue_special(log,special)) != RVM_SUCCESS)
+        free_log_special(log_seg);
+
+    return retval;
+    }
+
+/* write new segment dictionary entries for all segments */
+rvm_return_t define_all_segs(log)
+    log_t           *log;
+    {
+    seg_t           *seg;               /* segment descriptor */
+    rvm_return_t    retval = RVM_SUCCESS; /* return value */
+
+    RW_CRITICAL(seg_root_lock,r,        /* begin seg_root_lock crit sec */
+        {
+        FOR_ENTRIES_OF(seg_root,seg_t,seg)
+            {
+            if ((retval=define_seg(log,seg)) != RVM_SUCCESS)
+                break;
+            }
+        });                             /* end seg_root_lock crit sec */
+
+    return retval;
+    }
+
+/* segment builder */
+static seg_t *build_seg(rvm_region,log,retval)
+    rvm_region_t    *rvm_region;        /* segment's region descriptor */
+    log_t           *log;               /* log descriptor */
+    rvm_return_t    *retval;            /* ptr to return code */
+    {
+    seg_t           *seg;               /* new segment descriptor */
+
+    /* build segment descriptor */
+    seg = make_seg(rvm_region->data_dev,retval);
+    if (*retval != RVM_SUCCESS)
+        goto err_exit;
+
+    /* open device and set characteristics */
+    seg->log = log;
+    log->ref_cnt += 1;
+    if (open_seg_dev(seg,&rvm_region->dev_length) < 0)
+        {
+        *retval = RVM_EIO;
+        goto err_exit;
+        }
+
+    /* raw devices require length */
+    if ((seg->dev.raw_io) &&
+        (RVM_OFFSET_EQL_ZERO(seg->dev.num_bytes)))
+        {
+        *retval = RVM_ENOT_MAPPED;
+        goto err_exit;
+        }
+
+    /* define short name for log & queue log entry */
+    seg->seg_code = make_seg_code();
+    if ((*retval=define_seg(log,seg)) != RVM_SUCCESS)
+        goto err_exit;
+
+    /* put segment on segment list */
+    RW_CRITICAL(seg_root_lock,w,        /* begin seg_root_lock crit sec */
+        {
+        (void)move_list_entry(NULL,&seg_root,seg);
+        });                             /* end seg_root_lock crit sec */
+    return seg;
+
+err_exit:
+    log->ref_cnt -= 1;                  /* log seg dict entry not */
+    if (seg != NULL) free_seg(seg);     /* deallocated since the seg_code is */
+    return NULL;                        /* unique -- to the log, it's just like */
+    }                                   /* a segment used read-only  */
+
+/* device region conflict comparator */
+long dev_partial_include(base1,end1,base2,end2)
+    rvm_offset_t    *base1,*end1;
+    rvm_offset_t    *base2,*end2;
+    {
+    if (RVM_OFFSET_GEQ(*base1,*end2))
+        return 1;                       /* region1 above region2 */
+    if (RVM_OFFSET_LEQ(*end1,*base2))
+        return -1;                      /* region1 below region2 */
+
+    return 0;                           /* regions at least partially overlap */
+    }
+
+/* device region within other region comparator */
+long dev_total_include(base1,end1,base2,end2)
+    rvm_offset_t    *base1,*end1;
+    rvm_offset_t    *base2,*end2;
+    {
+    if ((RVM_OFFSET_GEQ(*base1,*base2) && RVM_OFFSET_LEQ(*base1,*end2))
+        &&
+        (RVM_OFFSET_GEQ(*end1,*base2) && RVM_OFFSET_LEQ(*end1,*end2))
+        ) return 0;                     /* region1 included in region2 */
+    if (RVM_OFFSET_LSS(*base1,*base2))
+        return -1;                      /* region1 below region2, may overlap */
+
+    return 1;                           /* region1 above region2, may overlap */
+    }
+
+/* vm range conflict comparator */
+long mem_partial_include(tnode1,tnode2)
+    tree_node_t     *tnode1;            /* range1 */
+    tree_node_t     *tnode2;            /* range2 */
+    {
+    rvm_length_t    addr1;              /* start of range 1 */
+    rvm_length_t    addr2;              /* start of range 2 */
+    rvm_length_t    end1;               /* end of range1 */
+    rvm_length_t    end2;               /* end of range2 */
+
+    /* rebind types and compute end points */
+    addr1 = (rvm_length_t)(((mem_region_t *)tnode1)->vmaddr);
+    addr2 = (rvm_length_t)(((mem_region_t *)tnode2)->vmaddr);
+    end1 = addr1 + ((mem_region_t *)tnode1)->length - 1;
+    end2 = addr2 + ((mem_region_t *)tnode2)->length - 1;
+
+    if (addr1 > end2) return 1;        /* range1 above range2 */
+    if (end1 < addr2) return -1;       /* range1 below range2 */
+    return 0;                          /* ranges at least partially overlap */
+    }
+
+/* vm range within other range comparator */
+long mem_total_include(tnode1,tnode2)
+    tree_node_t     *tnode1;            /* range1 */
+    tree_node_t     *tnode2;            /* range2 */
+    {
+    rvm_length_t    addr1;              /* start of range 1 */
+    rvm_length_t    addr2;              /* start of range 2 */
+    rvm_length_t    end1;               /* end of range1 */
+    rvm_length_t    end2;               /* end of range2 */
+
+    /* rebind types and compute end points */
+    addr1 = (rvm_length_t)(((mem_region_t *)tnode1)->vmaddr);
+    addr2 = (rvm_length_t)(((mem_region_t *)tnode2)->vmaddr);
+    end1 = addr1 + ((mem_region_t *)tnode1)->length - 1;
+    end2 = addr2 + ((mem_region_t *)tnode2)->length - 1;
+
+    if ((addr1 >= addr2) && (addr1 <= end2) && (end1 <= end2))
+        return 0;                       /* range1 included in range2 */
+/* This test does not correspond to the comment, changed it. -JH */
+//    if (end1 < addr2) return -1;        /* range1 below range2, may overlap */
+    if (addr1 < addr2) return -1;       /* range1 below range2, may overlap */
+    return 1;                           /* range1 above range2, may overlap */
+    }
+
+/* find and lock a region record iff vm range
+   entirely within a single mapped region
+   -- region tree is left lock if mode = w
+   -- used by transaction functions and unmap
+*/
+region_t *find_whole_range(dest,length,mode)
+    char            *dest;
+    rvm_length_t    length;
+    rw_lock_mode_t  mode;               /* lock mode for region descriptor */
+    {
+    mem_region_t    range;              /* dummy node for lookup */
+    mem_region_t    *node;              /* ptr to node found */
+    region_t        *region = NULL;     /* ptr to region for found node */
+
+    range.vmaddr = dest;
+    range.length = length;
+    range.links.node.struct_id = mem_region_id;
+
+    RW_CRITICAL(region_tree_lock,mode,  /* begin region_tree_lock crit sect */
+        {
+        node = (mem_region_t *)tree_lookup(&region_tree,
+                                          (tree_node_t *)&range,
+                                          mem_total_include);
+        if (node != NULL)
+            {
+            region = node->region;
+            if (region != NULL)
+                {                       /* begin region_lock crit sect */
+                rw_lock(&region->region_lock,mode); /* (ended by caller) */
+                if (mode == w)          /* retain region_tree_lock */
+                    return region;      /* caller will unlock */
+                }
+            }
+        });                             /* end region_tree_lock crit sect */
+
+    return region;
+    }
+/* find and lock a region record if vm range is at least partially
+   within a single mapped region; return code for inclusion
+*/
+region_t *find_partial_range(dest,length,code)
+    char            *dest;
+    rvm_length_t    length;
+    long            *code;
+    {
+    mem_region_t    range;              /* dummy node for lookup */
+    mem_region_t    *node;              /* ptr to node found */
+    region_t        *region = NULL;     /* ptr to region for found node */
+
+    range.vmaddr = dest;
+    range.length = length;
+    range.links.node.struct_id = mem_region_id;
+
+    RW_CRITICAL(region_tree_lock,r,     /* begin region_tree_lock crit sect */
+        {
+        node = (mem_region_t *)tree_lookup(&region_tree,
+                                          (tree_node_t *)&range,
+                                          mem_partial_include);
+        if (node != NULL)
+            {
+            region = node->region;
+            assert(region != NULL);
+
+            /* begin region_lock crit sect (ended by caller) */
+            rw_lock(&region->region_lock,r);
+            *code = mem_total_include((tree_node_t *)&range,
+                                      (tree_node_t *)node);
+            }
+        });                             /* end region_tree_lock crit sect */
+
+    return region;
+    }
+
+/* apply mapping options, compute region size, and round to page size */
+static rvm_return_t round_region(rvm_region,seg)
+    rvm_region_t    *rvm_region;        /* user region specs [in/out] */
+    seg_t           *seg;               /* segment descriptor */
+    {
+    rvm_offset_t    big_len;
+
+    /* see if region within segment */
+    if (RVM_OFFSET_GTR(rvm_region->offset,seg->dev.num_bytes))
+        return RVM_EOFFSET;
+    big_len = RVM_ADD_LENGTH_TO_OFFSET(rvm_region->offset,
+                                       rvm_region->length);
+    if (RVM_OFFSET_LSS(big_len,rvm_region->offset))
+        return RVM_EOFFSET;             /* overflow */
+
+    /* round offset, length up and down to integral page size */
+    big_len = RVM_LENGTH_TO_OFFSET(ROUND_TO_PAGE_SIZE(
+                  RVM_OFFSET_TO_LENGTH(big_len)));
+    rvm_region->offset = RVM_MK_OFFSET(
+        RVM_OFFSET_HIGH_BITS_TO_LENGTH(rvm_region->offset),
+        CHOP_TO_PAGE_SIZE(RVM_OFFSET_TO_LENGTH(rvm_region->offset)));
+
+    /* see if at end of segment */
+    if ((rvm_region->length == 0)
+        || RVM_OFFSET_GTR(big_len,seg->dev.num_bytes))
+        big_len = seg->dev.num_bytes;
+
+    /* calculate actual length to map (only 32 bit lengths for now) */
+    big_len = RVM_SUB_OFFSETS(big_len,rvm_region->offset);
+    if (RVM_OFFSET_HIGH_BITS_TO_LENGTH(big_len) != 0)
+        return RVM_ERANGE;
+    rvm_region->length = RVM_OFFSET_TO_LENGTH(big_len);
+
+    /* check page aligned buffer or allocate virtual memory region */
+    if (rvm_region->vmaddr != NULL)
+        {
+        if (rvm_region->vmaddr != (char *)
+                             CHOP_TO_PAGE_SIZE(rvm_region->vmaddr))
+            return RVM_ERANGE;          /* buffer not page aligned */
+        if (!mem_chk(rvm_region->vmaddr,rvm_region->length))
+            return RVM_ERANGE;          /* buffer not within task's vm */
+        }
+    else
+        {
+        rvm_region->vmaddr =
+            page_alloc(ROUND_TO_PAGE_SIZE(rvm_region->length));
+        if (rvm_region->vmaddr == NULL) return RVM_ENO_MEMORY;
+        }
+
+    return RVM_SUCCESS;
+    }
+
+/* validate region and construct descriptors */
+static rvm_return_t establish_range(rvm_region,region,mem_region,seg)
+    rvm_region_t    *rvm_region;        /* user request region descriptor */
+    region_t        **region;           /* internal region descriptor [out]*/
+    mem_region_t    **mem_region;       /* region tree descriptor [out] */
+    seg_t           *seg;               /* segment ptr */
+    {
+    mem_region_t    *mem_node;
+    region_t        *new_region;
+    rvm_return_t    retval;
+
+    /* get exact region size, address */
+    *region = NULL; *mem_region = NULL;
+    if ((retval=round_region(rvm_region,seg)) != RVM_SUCCESS)
+        return retval;
+
+    /* build new region descriptor */
+    *region = new_region = make_region();
+    if (new_region == NULL) return RVM_ENO_MEMORY;
+    new_region->no_copy = rvm_region->no_copy;
+    new_region->offset = rvm_region->offset;
+    new_region->end_offset =
+        RVM_ADD_LENGTH_TO_OFFSET(rvm_region->offset,
+                                 rvm_region->length);
+
+    /* build range tree node */
+    *mem_region = mem_node = make_mem_region();
+    if (mem_node == NULL) return RVM_ENO_MEMORY;
+    new_region->mem_region = mem_node;
+    mem_node->vmaddr = new_region->vmaddr = rvm_region->vmaddr;
+    mem_node->length = new_region->length
+                     = (rvm_length_t)rvm_region->length;
+    mem_node->region = NULL;
+
+    /* put range tree node in tree to reserve range */
+    RW_CRITICAL(region_tree_lock,w,     /* begin region_tree_lock crit sect */
+        {
+        if (!tree_insert(&region_tree,(tree_node_t *)mem_node,
+                            mem_partial_include))
+            retval = RVM_EVM_OVERLAP;         /* vm range already mapped */
+        });                             /* end region_tree_lock crit sect */
+
+    return retval;
+    }
+
+/* check for mapping dependencies on previously
+   mapped regions, or conflict with presently mapped region
+   -- caller provides list locking
+   returns true if dependency detected
+*/
+static region_t *chk_seg_mappings(chk_region,list_root)
+    region_t        *chk_region;        /* region descriptor to chk*/
+    list_entry_t    *list_root;         /* root of list to check */
+    {
+    region_t        *region;            /* internal region descriptor */
+
+    FOR_ENTRIES_OF(*list_root,region_t,region)
+        {
+        /* test for overlap */
+        if (dev_partial_include(&chk_region->offset,
+                          &chk_region->end_offset,
+                          &region->offset,&region->end_offset
+                          ) == 0)
+            return region;              /* overlap */
+        }
+
+    return NULL;
+    }
+
+/* check mapping dependencies within segment */
+static rvm_return_t chk_dependencies(seg,region)
+    seg_t           *seg;
+    region_t        *region;
+    {
+    region_t        *x_region;          /* conflicting or dependent region */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* check for multiple mappings of same segment region */
+    CRITICAL(seg->seg_lock,            /* begin seg_lock crit sect */
+        {
+        if ((x_region=chk_seg_mappings(region,&seg->map_list))
+            == NULL)
+            {
+            /* enter region in map_list */
+            region->seg = seg;
+            (void)move_list_entry(NULL,&seg->map_list,
+                                  &region->links);
+
+            /* check for overlap with modified and unmapped regions of segment
+               if found, must wait for truncation to get committed image of region */
+            DO_FOREVER
+                if ((x_region=chk_seg_mappings(region,
+                                               &seg->unmap_list))
+                    != NULL)
+                    {
+                    (void)initiate_truncation(seg->log,100);
+                    if ((retval=wait_for_truncation(seg->log,
+                                                &x_region->unmap_ts))
+                        != RVM_SUCCESS) goto err_exit;
+                    free_region(x_region); /* can free now */
+                    }
+                else break;             /* no further dependencies */
+            }
+        else
+            retval = RVM_EOVERLAP;      /* multiply mapped */
+err_exit:;
+        });                             /* end seg_lock crit sect */
+
+    return retval;
+    }
+
+/* make data from segment available from mapped region */
+static rvm_return_t map_data(rvm_options,region)
+    rvm_options_t   *rvm_options;
+    region_t        *region;
+    {
+    seg_t           *seg = region->seg;
+    rvm_return_t    retval = RVM_SUCCESS;
+#if defined(__NetBSD__) || defined(__FreeBSD__)
+    char            *addr;
+#endif
+    /* check for pager mapping */
+    if (rvm_options != NULL)
+        if (rvm_options->pager != NULL)
+            {
+            /* external pager interface not implemented yet */
+            return RVM_EPAGER;
+            }
+
+#if defined(__NetBSD__) || defined(__FreeBSD__)
+		/* NetBSD has a kernel bug that will panic if we
+		   try to read from a raw device and copy it to address
+		   on or above 0x10400000.  This is known to be a problem
+		   with vm_fault() of NetBSD kernel that panics when it
+		   finds that the pte (page directory table entry) does
+		   not exist in page dir table (instead of trying to
+		   create it). Before that is fixed, we work around it
+		   by manually touching one byte of address space of
+		   every pte's that we'll need.  This will get the pte
+		   created and we'll be fine.  This is proposed by rvb.
+		     -- clement */
+		if (seg->dev.raw_io) {
+		    for (addr=region->vmaddr;
+			 addr < ( (region->vmaddr)+(region->length) );
+			 addr+=0x400000) { /* each pte is for 0x400000 of vm */
+			*addr = 0; /* this will force kernel to create
+				   the pte*/
+		    }
+		}
+#endif /* __BSD44__ */
+    /* read data directly from segment */
+    if (!region->no_copy)
+        CRITICAL(seg->dev_lock,
+            {
+            if (read_dev(&seg->dev,&region->offset,
+                         region->vmaddr,region->length) < 0)
+                retval = RVM_EIO;
+            });
+
+    return retval;
+    }
+
+/* error exit cleanup */
+static void clean_up(region,mem_region)
+    region_t        *region;
+    mem_region_t    *mem_region;
+{
+    seg_t           *seg;
+
+    /* kill region descriptor if created */
+    if (region != NULL)
+        {
+        seg = region->seg;
+        if (seg != NULL)
+            CRITICAL(seg->seg_lock,
+                {
+                (void)move_list_entry(&seg->map_list,NULL,
+                                       &region->links);
+                });
+        free_region(region);
+        }
+
+    /* kill region tree node if created */
+    if (mem_region != NULL)
+        {
+        RW_CRITICAL(region_tree_lock,w,
+            {
+            (void)tree_delete(&region_tree,(tree_node_t *)mem_region,
+                              mem_partial_include);
+            });
+        free_mem_region(mem_region);
+        }
+}
+
+/* rvm_map */
+rvm_return_t rvm_map(rvm_region_t *rvm_region, rvm_options_t *rvm_options)
+{
+    seg_t               *seg;              /* segment descriptor */
+    region_t            *region = NULL;    /* new region descriptor */
+    mem_region_t        *mem_region= NULL; /* new region's tree node */
+    rvm_return_t        retval;
+    rvm_region_t        save_rvm_region;
+    int fd;				   /* For private mappings */
+    void *addr;
+
+    /* preliminary checks & saves */
+    if (bad_init()) return RVM_EINIT;
+    if ((retval=bad_region(rvm_region)) != RVM_SUCCESS)
+        return retval;
+    if (rvm_options != NULL)
+        if ((retval=do_rvm_options(rvm_options)) != RVM_SUCCESS)
+            return retval;
+    if (default_log == NULL) return RVM_ELOG;
+    (void)BCOPY((char *)rvm_region,(char *)&save_rvm_region,
+               sizeof(rvm_region_t));
+
+    /* find or build segment */
+    seg = seg_lookup(rvm_region->data_dev,&retval);
+    if (retval != RVM_SUCCESS) goto err_exit;
+    if (seg == NULL)
+        {                               /* must build a new segment */
+        if ((seg=build_seg(rvm_region,default_log,&retval))
+            == NULL) goto err_exit;
+        }
+    else
+        /* test if segment closed by earlier (failing) rvm_terminate */
+        if (seg->dev.handle == 0) return RVM_EIO;
+
+    /* check for vm overlap with existing mappings & build descriptors */
+    if ((retval = establish_range(rvm_region,&region,&mem_region,seg))
+                != RVM_SUCCESS)
+        goto err_exit;
+
+    /* check for overlap with existing mappings in segment, check
+       for truncation dependencies, and enter region in map_list */
+    if ((retval=chk_dependencies(seg,region)) != RVM_SUCCESS)
+        goto err_exit;
+
+    /* Do the private map or get the data from the segment */
+    if (rvm_map_private) {
+	fd = open(rvm_region->data_dev, O_RDONLY | O_BINARY);
+	if ( fd < 0 ) {
+	    retval = RVM_EIO;
+	    goto err_exit;
+	}
+	addr = mmap(rvm_region->vmaddr, rvm_region->length,
+		    PROT_READ | PROT_WRITE, MAP_FIXED | MAP_PRIVATE,
+		    fd, region->offset.low);
+	if (!rvm_region->vmaddr)
+	    rvm_region->vmaddr = addr;
+
+	if (addr != rvm_region->vmaddr) {
+	    retval = RVM_ENOT_MAPPED;
+	    goto err_exit;
+	}
+	if (close(fd)) {
+	    retval = RVM_EIO;
+	    goto err_exit;
+	}
+    } else {
+        /* get the data from the segment */
+        if ((retval = map_data(rvm_options,region)) != RVM_SUCCESS) {
+            rvm_region->length = 0;
+	    goto err_exit;
+	}
+    }
+
+    /* complete region tree node and exit*/
+    mem_region->region = region;
+    return RVM_SUCCESS;
+
+  err_exit:
+    clean_up(region,mem_region);
+    (void)BCOPY((char *)&save_rvm_region,(char *)rvm_region,
+               sizeof(rvm_region_t));
+    return retval;
+}
diff --git a/rvm/rvm_printers.c b/rvm/rvm_printers.c
new file mode 100644
index 0000000..91013dc
--- /dev/null
+++ b/rvm/rvm_printers.c
@@ -0,0 +1,771 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                       RVM structure printers
+*
+*/
+
+#include "rvm_private.h"
+
+/* global variables */
+
+extern log_t        *default_log;       /* default log descriptor ptr */
+extern rvm_bool_t   rvm_utlsw;          /* true if running in rvmutl */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+extern rvm_length_t flush_times_vec[flush_times_len]; /* flush timing histogram defs */
+extern rvm_length_t truncation_times_vec[truncation_times_len]; /* truncation timing 
+                                                                   histogram defs */
+extern rvm_length_t range_lengths_vec[range_lengths_len]; /* range length
+                                                             histogram defs */
+extern rvm_length_t range_overlaps_vec[range_overlaps_len]; /* range coalesce
+                                                             histogram defs */
+extern rvm_length_t trans_overlaps_vec[trans_overlaps_len]; /* trans coalesce
+                                                             histogram defs */
+extern rvm_length_t range_elims_vec[range_elims_len]; /* ranges eliminated by range
+                                                         coalesce histogram defs */
+extern rvm_length_t trans_elims_vec[trans_elims_len]; /* ranges eliminated by trans
+                                                         coalesce histogram defs */
+extern rvm_length_t trans_coalesces_vec[trans_coalesces_len]; /* transactions
+                                                                 coalesed per flush */
+/* print rvm_offset_t */
+static int pr_offset(offset,stream)   
+    rvm_offset_t    *offset;
+    FILE            *stream;
+    {
+    int             tot_chars = 0;
+    float           flt_val;
+
+    if (RVM_OFFSET_HIGH_BITS_TO_LENGTH(*offset) != 0)
+        {
+        flt_val = OFFSET_TO_FLOAT(*offset);
+        tot_chars = fprintf(stream,"%10.3f",flt_val);
+        }
+    else
+        tot_chars = fprintf(stream,"%10.1lu",RVM_OFFSET_TO_LENGTH(*offset));
+
+    return tot_chars;
+    }
+
+/* histogram value printer -- handles placement of <= for definition values */
+static int pr_histo_val(out_stream,val,width,is_def,gtr,us)
+    FILE            *out_stream;        /* target stream */
+    rvm_length_t    val;                /* histogram value */
+    int             width;              /* print width of histogram data */
+    rvm_bool_t      is_def;             /* value is bucket size if true */
+    rvm_bool_t      gtr;                /* print '> ' if true */
+    rvm_bool_t      us;                 /* true if value unsigned */
+    {
+    char            str[20];            /* string buffer */
+    int             pad;
+    int             err;
+
+    /* convert value and get width of padding */
+    if (us)
+        err = (int)sprintf(str,"%lu",val);
+    else
+        err = (int)sprintf(str,"%ld",val);
+    if (err == EOF) return err;
+    pad = width - strlen(str);
+    if (!is_def) pad += 2;              /* compensate for '<=' */
+
+    /* print padding and relational op */
+    err = fprintf(out_stream,"%*c",pad,' ');
+    if (err == EOF) return err;
+    if (is_def)
+        {
+        if (gtr)
+            err = fprintf(out_stream,"> ");
+        else
+            err = fprintf(out_stream,"<=");
+        }
+    if (err == EOF) return err;
+
+    /* print converted value */
+    err = fprintf(out_stream,"%s",str);
+    return err;
+    }
+/* histogram printer */
+static int pr_histogram(out_stream,histo,histo_def,length,
+                        width,leading,gtr,us)
+    FILE            *out_stream;        /* target stream */
+    rvm_length_t    *histo;             /* histogram data */
+    rvm_length_t    *histo_def;         /* histogram bucket sizes */
+    rvm_length_t    length;             /* length of histogram vectors */
+    int             width;              /* print width of histogram data */
+    int             leading;            /* number of leading spaces */
+    rvm_bool_t      gtr;                /* print final > bucket if true */
+    rvm_bool_t      us;                 /* values unsigned if true */
+    {
+    int             err;
+    rvm_length_t    i;
+
+    /* print buckets */
+    err = fprintf(out_stream,"%*c",leading,' ');
+    if (err == EOF) return err;
+
+    for (i=0; i<length-1; i++)
+        {
+        err = pr_histo_val(out_stream,histo_def[i],width,
+			   rvm_true,rvm_false,us);
+        if (err == EOF) return err;
+        }
+    if (gtr)
+        err = pr_histo_val(out_stream,histo_def[length-2],
+                           width,rvm_true,rvm_true,us);
+    else
+        err = pr_histo_val(out_stream,histo_def[length-1],
+                           width,rvm_true,rvm_false,us);
+    if (err == EOF) return err;
+    err = putc('\n',out_stream);
+    if (err == EOF) return err;
+
+    /* print data */
+    err = fprintf(out_stream,"%*c",leading,' ');
+    if (err == EOF) return err;
+
+    for (i=0; i<length; i++)
+        {
+        err = pr_histo_val(out_stream,histo[i],
+                           width,rvm_false,rvm_false,us);
+        if (err == EOF) return err;
+        }
+    err = putc('\n',out_stream);
+    return err;
+    }
+/* print transaction statistics */
+static rvm_return_t pr_trans_stats(stats,out_stream,n_trans,tot_trans)
+    rvm_statistics_t    *stats;         /* ptr top statistics record */
+    FILE                *out_stream;    /* output stream */
+    rvm_length_t        n_trans;
+    rvm_length_t        tot_trans;
+    {
+    rvm_length_t        n_trans_started;
+    int                 err;
+
+    /* print header for transaction statistics*/
+    err = fprintf(out_stream,
+            "Transaction statistics               current %s\n\n",
+            "cumulative");
+    if (err == EOF) return RVM_EIO;
+    n_trans_started = stats->n_flush_commit + stats->n_no_flush_commit
+                        + stats->n_uncommit + stats->n_abort;
+    err = fprintf(out_stream,
+                  "  Started:                        %10ld %10ld\n",
+                  n_trans_started,
+                  stats->tot_flush_commit+stats->tot_no_flush_commit
+                  + stats->tot_abort+n_trans_started);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Aborted:                        %10ld %10ld\n",
+                  stats->n_abort,
+                  stats->tot_abort + stats->n_abort);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Committed, flush:               %10ld %10ld\n",
+                  stats->n_flush_commit,
+                  stats->tot_flush_commit + stats->n_flush_commit);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Committed, no_flush:            %10ld %10ld\n",
+                  stats->n_no_flush_commit,
+                  stats->tot_no_flush_commit+stats->n_no_flush_commit);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Total committed:                %10ld %10ld\n",
+                  stats->n_no_flush_commit+stats->n_flush_commit,
+                  stats->tot_no_flush_commit+stats->tot_flush_commit
+                  +stats->n_no_flush_commit+stats->n_flush_commit);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Committed, but not flushed:     %10ld\n",
+                  stats->n_no_flush);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Unflushed transactions length:  %10ld\n",
+                  RVM_OFFSET_TO_LENGTH(stats->no_flush_length));
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Not committed:                  %10ld\n",
+                  stats->n_uncommit);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Split by log wrap:              %10ld %10ld\n",
+                  stats->n_split,
+                  stats->tot_split + stats->n_split);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Delayed by truncation:          %10ld %10ld\n",
+                  stats->n_truncation_wait,
+                  stats->tot_truncation_wait+stats->n_truncation_wait);
+    if (err == EOF) return RVM_EIO;
+
+    return RVM_SUCCESS;
+    }
+/* print log statistics */
+static rvm_return_t pr_log_stats(stats,out_stream,n_trans,tot_trans)
+    rvm_statistics_t    *stats;         /* ptr top statistics record */
+    FILE                *out_stream;    /* output stream */
+    rvm_length_t        n_trans;
+    rvm_length_t        tot_trans;
+    {
+    rvm_length_t    n_flush;
+    rvm_length_t    tot_flush;
+    rvm_length_t    n_recs;
+    rvm_length_t    tot_recs;
+    rvm_length_t    tot_truncations;
+    rvm_length_t    len_temp1;
+    rvm_length_t    len_temp2;
+    int             err;
+
+    /* print header for log function statistics*/
+    err = fprintf(out_stream,
+                  "\nLog function statistics              current %s\n\n",
+                  "cumulative");
+    if (err == EOF) return RVM_EIO;
+
+    err = fprintf(out_stream,
+                  "  rvm_flush calls:                %10ld %10ld\n",
+                  stats->n_rvm_flush,
+                  stats->tot_rvm_flush + stats->n_rvm_flush);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Internal flushes, incl. commit: %10ld %10ld\n",
+                  stats->n_flush,
+                  stats->tot_flush + stats->n_flush);
+    n_flush = stats->n_flush + stats->n_rvm_flush;
+    tot_flush = stats->tot_flush + stats->tot_rvm_flush + n_flush;
+    err = fprintf(out_stream,
+                  "  Total flushes:                  %10ld %10ld\n",
+                  n_flush,tot_flush);
+    if (err == EOF) return RVM_EIO;
+    len_temp1 = len_temp2 = 0;
+    if (n_flush > 0) len_temp1 =
+        (1000*round_time(&stats->flush_time))/n_flush;
+    if (tot_flush > 0) len_temp2 =
+        (1000*round_time(&stats->tot_flush_time))/tot_flush;
+    err = fprintf(out_stream,
+                  "  Average flush time (msec):      %10ld %10ld\n",
+                  len_temp1,len_temp2);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Last flush time (msec):         %10ld\n\n",
+                  stats->last_flush_time);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  rvm_truncate calls:                        %10ld\n",
+            stats->tot_rvm_truncate);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Automatic truncations:                     %10ld\n",
+            stats->tot_async_truncation);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Forced synch. truncations:                 %10ld\n",
+            stats->tot_sync_truncation);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Recovery truncations:                      %10ld\n",
+            stats->tot_recovery);
+    if (err == EOF) return RVM_EIO;
+    tot_truncations = stats->tot_async_truncation
+        + stats->tot_sync_truncation + stats->tot_recovery
+        + stats->tot_rvm_truncate;
+    err = fprintf(out_stream,
+                  "  Total truncations:                         %10ld\n",
+                  tot_truncations);
+    if (err == EOF) return RVM_EIO;
+    len_temp1 = 0;
+    if (tot_truncations > 0) len_temp1 =
+        round_time(&stats->tot_truncation_time)/tot_truncations;
+    err = fprintf(out_stream,
+                  "  Average truncation time (sec):             %10ld\n",
+                  len_temp1);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Last truncation time (sec):                %10ld\n",
+                  stats->last_truncation_time);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Last tree build time (sec):                %10ld\n",
+                  stats->last_tree_build_time);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Last tree apply time (sec):                %10ld\n\n",
+                  stats->last_tree_apply_time);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Transaction records:            %10ld %10ld\n",
+                  n_trans,
+                  tot_trans+n_trans);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Bookeeping records:             %10ld %10ld\n",
+                  stats->n_special,
+                  stats->tot_special + stats->n_special);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Log wrap-arounds:               %10ld %10ld\n",
+                  stats->n_wrap,
+                  stats->tot_wrap+stats->n_wrap);
+    n_recs = n_trans + stats->n_special + stats->n_wrap;
+    tot_recs = tot_trans + stats->tot_special + stats->tot_wrap;
+    err = fprintf(out_stream,
+                  "  Total records:                  %10ld %10ld\n\n",
+                  n_recs,
+                  tot_recs+n_recs);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Log used:                       %10ld%%%10ld%%\n",
+                  stats->log_dev_cur,
+                  stats->log_dev_max);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Log written (bytes):            ");
+    if (err == EOF) return RVM_EIO;
+    err = pr_offset(&stats->log_written,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream," ");
+    if (err == EOF) return RVM_EIO;
+    err = pr_offset(&stats->tot_log_written,out_stream);
+    if (err == EOF) return RVM_EIO;
+
+    return RVM_SUCCESS;
+    }
+/* print timing histograms */
+static rvm_return_t pr_time_histos(stats,out_stream,n_trans,tot_trans)
+    rvm_statistics_t    *stats;         /* ptr top statistics record */
+    FILE                *out_stream;    /* output stream */
+    rvm_length_t        n_trans;
+    rvm_length_t        tot_trans;
+    {
+    int             err;
+
+    err = fprintf(out_stream,"\n\nTiming Histograms\n");
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,"\n  Current Flush Timings (msec):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->flush_times,flush_times_vec,
+                       flush_times_len,6,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,"\n  Cummulative Flush Timings (msec):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_flush_times,flush_times_vec,
+                       flush_times_len,6,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,"\n\n  Truncation Timings for Tree Build (sec):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_tree_build_times,
+                       truncation_times_vec,truncation_times_len,
+                       4,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,"\n  Truncation Timings for Tree Apply (sec):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_tree_apply_times,
+                       truncation_times_vec,truncation_times_len,
+                       4,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,"\n  Total Truncation Timings (sec):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_truncation_times,
+                       truncation_times_vec,truncation_times_len,
+                       4,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    return RVM_SUCCESS;
+    }
+/* print transaction optimization statistics */
+static rvm_return_t pr_opt_stats(stats,out_stream,n_trans,tot_trans)
+    rvm_statistics_t    *stats;         /* ptr top statistics record */
+    FILE                *out_stream;    /* output stream */
+    rvm_length_t        n_trans;
+    rvm_length_t        tot_trans;
+    {
+    rvm_length_t        n_flush;
+    rvm_length_t        tot_flush;
+    rvm_offset_t        off_temp;
+    rvm_length_t        len_temp1 = 0;
+    rvm_length_t        len_temp2 = 0;
+    int                 err;
+
+    err = fprintf(out_stream,
+            "\n\nTransaction Optimization Statistics          current %s\n\n",
+            "cumulative");
+    if (err == EOF) return RVM_EIO;
+    n_trans -= stats->n_split;
+    tot_trans -= stats->tot_split;
+    err = fprintf(out_stream,
+                  "  Ranges eliminated\n");
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Range coalesce:                        %10lu %10lu\n",
+                  stats->n_range_elim,stats->tot_range_elim);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Trans coalesce:                        %10lu %10lu\n",
+                  stats->n_trans_elim,stats->tot_trans_elim);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "    Totals:                               %10lu %10lu\n",
+                  stats->n_range_elim+stats->n_trans_elim,
+                  stats->tot_range_elim+stats->tot_trans_elim);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "  Avg. number eliminated per transaction\n");
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Range coalesce:                        %10ld %10ld\n",
+                  (n_trans != 0 ? stats->n_range_elim/n_trans : 0),
+                  (tot_trans != 0 ? stats->tot_range_elim/tot_trans : 0));
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Trans coalesce:                        %10ld %10ld\n",
+                  (n_trans != 0 ? stats->n_trans_elim/n_trans : 0),
+                  (tot_trans != 0 ? stats->tot_trans_elim/tot_trans : 0));
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "    Totals:                               %10ld %10ld\n",
+                  (n_trans != 0
+                   ? (stats->n_range_elim+stats->n_trans_elim)/
+                      n_trans : 0),
+                  (tot_trans != 0
+                   ? (stats->tot_range_elim+stats->tot_trans_elim)/
+                      tot_trans : 0));
+    if (err == EOF) return RVM_EIO;
+
+    err = fprintf(out_stream,
+                  "  Range length eliminated\n");
+    if (err == EOF) return RVM_EIO;
+    off_temp = RVM_ADD_OFFSETS(stats->tot_range_overlap,
+                               stats->tot_trans_overlap);
+    err = fprintf(out_stream,
+                  "   Range coalesce:                        %10lu ",
+                  RVM_OFFSET_TO_LENGTH(stats->range_overlap));
+    if (err == EOF) return RVM_EIO;
+    pr_offset(&stats->tot_range_overlap,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Trans coalesce:                        %10lu ",
+                  RVM_OFFSET_TO_LENGTH(stats->trans_overlap));
+    if (err == EOF) return RVM_EIO;
+    pr_offset(&stats->tot_trans_overlap,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "    Totals:                               %10lu ",
+                  RVM_OFFSET_TO_LENGTH(stats->range_overlap)
+                  + RVM_OFFSET_TO_LENGTH(stats->trans_overlap));
+    if (err == EOF) return RVM_EIO;
+    pr_offset(&off_temp,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+
+    err = fprintf(out_stream,"  Log savings\n");
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Range coalesce:                        %10lu ",
+                  RVM_OFFSET_TO_LENGTH(stats->range_overlap)
+                  + stats->n_range_elim*sizeof(nv_range_t));
+    if (err == EOF) return RVM_EIO;
+    off_temp =
+        RVM_ADD_LENGTH_TO_OFFSET(stats->tot_range_overlap,
+                                 stats->tot_range_elim*sizeof(nv_range_t));
+    pr_offset(&off_temp,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "   Trans coalesce:                        %10lu ",
+                  RVM_OFFSET_TO_LENGTH(stats->trans_overlap)
+                  + stats->n_trans_elim*sizeof(nv_range_t));
+    if (err == EOF) return RVM_EIO;
+    off_temp =
+        RVM_ADD_LENGTH_TO_OFFSET(stats->tot_trans_overlap,
+                                 stats->tot_trans_elim*sizeof(nv_range_t));
+    pr_offset(&off_temp,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = fprintf(out_stream,
+                  "    Totals:                               %10lu ",
+                  RVM_OFFSET_TO_LENGTH(stats->trans_overlap)
+                  + stats->n_trans_elim*sizeof(nv_range_t)
+                  + RVM_OFFSET_TO_LENGTH(stats->range_overlap)
+                  + stats->n_range_elim*sizeof(nv_range_t));
+    if (err == EOF) return RVM_EIO;
+    off_temp =
+        RVM_ADD_LENGTH_TO_OFFSET(off_temp,
+                                 stats->tot_range_elim*sizeof(nv_range_t));
+    off_temp = RVM_ADD_OFFSETS(stats->tot_range_overlap,off_temp);
+    pr_offset(&off_temp,out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+
+    n_flush = stats->n_flush + stats->n_rvm_flush;
+    tot_flush = stats->tot_flush + stats->tot_rvm_flush + n_flush;
+    if (n_flush > 0)
+        len_temp1 = stats->n_trans_coalesced/n_flush;
+    if (tot_flush > 0)
+        len_temp2 = stats->tot_trans_coalesced/tot_flush;
+    err = fprintf(out_stream,
+                  "  Transactions coalesced per flush:       %10lu %10lu\n",
+                  len_temp1,len_temp2);
+    if (err == EOF) return RVM_EIO;
+
+    return RVM_SUCCESS;
+    }
+/* print transaction optimization histograms */
+static rvm_return_t pr_opt_histos(stats,out_stream,n_trans,tot_trans)
+    rvm_statistics_t    *stats;         /* ptr top statistics record */
+    FILE                *out_stream;    /* output stream */
+    rvm_length_t        n_trans;
+    rvm_length_t        tot_trans;
+    {
+    int                 err;
+    int                 i;
+    rvm_length_t        overlaps_totals_vec[range_overlaps_len];
+    rvm_length_t        elims_totals_vec[range_elims_len];
+
+    err=fprintf(out_stream,
+                "\n\nTranasction Modification Range Distributions\n\n");
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,"  Current Range Lengths (bytes):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->range_lengths,
+                       range_lengths_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&stats->range_lengths[7],
+                       &range_lengths_vec[7],range_lengths_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,"\n  Cumulative Range Lengths (bytes):\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_range_lengths,
+                       range_lengths_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&stats->tot_range_lengths[7],
+                       &range_lengths_vec[7],range_lengths_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,"\n\n  %s (bytes):\n",
+                "Current Range Lengths Eliminated");
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "   Range coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->range_overlaps,
+                       range_overlaps_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&stats->range_overlaps[7],
+                       &range_overlaps_vec[7],range_overlaps_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,
+                "   Trans coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->trans_overlaps,
+                       trans_overlaps_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&stats->trans_overlaps[7],
+                       &trans_overlaps_vec[7],trans_overlaps_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,
+                "    Totals:\n");
+    if (err == EOF) return RVM_EIO;
+    for (i=0;i<range_overlaps_len;i++)
+        overlaps_totals_vec[i] = stats->range_overlaps[i]
+                                 + stats->trans_overlaps[i];
+    err = pr_histogram(out_stream,overlaps_totals_vec,
+                       range_overlaps_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&overlaps_totals_vec[7],
+                       &range_overlaps_vec[7],range_overlaps_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,"\n  %s (bytes):\n",
+                "Cumulative Range Lengths Eliminated");
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "   Range coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_range_overlaps,
+                       range_overlaps_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&stats->tot_range_overlaps[7],
+                       &range_overlaps_vec[7],range_overlaps_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,
+                "   Trans coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_trans_overlaps,
+                       trans_overlaps_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&stats->tot_trans_overlaps[7],
+                       &trans_overlaps_vec[7],trans_overlaps_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,
+                "    Totals:\n");
+    if (err == EOF) return RVM_EIO;
+    for (i=0;i<range_overlaps_len;i++)
+        overlaps_totals_vec[i] = stats->tot_range_overlaps[i]
+                                 + stats->tot_trans_overlaps[i];
+    err = pr_histogram(out_stream,overlaps_totals_vec,
+                       range_overlaps_vec,7,10,2,rvm_false,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,&overlaps_totals_vec[7],
+                       &range_overlaps_vec[7],range_overlaps_len-7,
+                       10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,
+                "\n  Current Number of Eliminated Ranges per Transaction\n");
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "   Range coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->range_elims,range_elims_vec,
+                       range_elims_len,10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "   Trans coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->trans_elims,trans_elims_vec,
+                       trans_elims_len,10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "    Totals:\n");
+    if (err == EOF) return RVM_EIO;
+    for (i=0;i<range_elims_len;i++)
+        elims_totals_vec[i] = stats->range_elims[i]
+                              + stats->trans_elims[i];
+    err = pr_histogram(out_stream,elims_totals_vec,
+                       range_elims_vec,range_elims_len,10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,"\n  %s\n",
+                "Cummulative Number of Eliminated Ranges per Transaction");
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "   Range coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_range_elims,
+                       range_elims_vec,range_elims_len,10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "   Trans coalesce:\n");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_trans_elims,
+                       trans_elims_vec,trans_elims_len,10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err=fprintf(out_stream,
+                "    Totals:\n");
+    if (err == EOF) return RVM_EIO;
+    for (i=0;i<range_elims_len;i++)
+        elims_totals_vec[i] = stats->tot_range_elims[i]
+                              + stats->tot_trans_elims[i];
+    err = pr_histogram(out_stream,elims_totals_vec,
+                       range_elims_vec,range_elims_len,10,2,rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+
+    err=fprintf(out_stream,"\n  %s\n",
+                "Cummulative Number of Transactions Coalesced per flush Cycle");
+    if (err == EOF) return RVM_EIO;
+    err = pr_histogram(out_stream,stats->tot_trans_coalesces,
+                       trans_coalesces_vec,trans_coalesces_len,10,2,
+		       rvm_true,rvm_true);
+    if (err == EOF) return RVM_EIO;
+    err = putc('\n',out_stream);
+    if (err == EOF) return RVM_EIO;
+
+    return RVM_SUCCESS;
+    }
+/* rvm_print_stats */
+rvm_return_t rvm_print_statistics(stats,out_stream)
+    rvm_statistics_t    *stats;         /* ptr top statistics record */
+    FILE                *out_stream;    /* output stream */
+    {
+    rvm_length_t    n_trans;
+    rvm_length_t    tot_trans;
+    rvm_return_t    retval;
+
+    /* initial checks */
+    if (bad_init()) return RVM_EINIT;
+    if (default_log == NULL) return RVM_ELOG;
+    if (stats == NULL) return RVM_ESTATISTICS;
+    if ((retval=bad_statistics(stats)) != RVM_SUCCESS)
+        return retval;
+
+    /* global totals */
+    n_trans = stats->n_flush_commit+stats->n_no_flush_commit
+                +stats->n_split;
+    tot_trans = stats->tot_flush_commit+stats->tot_no_flush_commit
+                +stats->tot_split;
+
+    /* print transaction statistics */
+    if ((retval=pr_trans_stats(stats,out_stream,n_trans,tot_trans))
+        != RVM_SUCCESS) return retval;
+
+    /* print log statistics */
+    if ((retval=pr_log_stats(stats,out_stream,n_trans,tot_trans))
+        != RVM_SUCCESS) return retval;
+
+    /* timing histogram printing */
+    if ((retval=pr_time_histos(stats,out_stream,n_trans,tot_trans))
+        != RVM_SUCCESS) return retval;
+
+    /* transaction optimization statistics */
+    if ((retval=pr_opt_stats(stats,out_stream,n_trans,tot_trans))
+        != RVM_SUCCESS) return retval;
+
+    /* transaction optimization histograms */
+    if ((retval=pr_opt_histos(stats,out_stream,n_trans,tot_trans))
+        != RVM_SUCCESS) return retval;
+
+    return RVM_SUCCESS;
+    }
diff --git a/rvm/rvm_private.h b/rvm/rvm_private.h
new file mode 100644
index 0000000..5e533c8
--- /dev/null
+++ b/rvm/rvm_private.h
@@ -0,0 +1,1716 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                 Internal Definitions for RVM
+*
+*/
+
+#pragma once
+
+/* permit multiple includes */
+#ifndef __MERO_RVM_RVM_PRIVATE_H__
+#define __MERO_RVM_RVM_PRIVATE_H__
+
+/* turn on debuging for now */
+#ifndef DEBUG
+#define DEBUG 1
+#endif
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <sys/types.h>
+#include <stdlib.h>
+#include <string.h>
+#include <assert.h>
+#include "rvm/rvm.h"
+#include "rvm/rvm_statistics.h"
+
+#include <fcntl.h>
+#ifndef O_BINARY
+#define O_BINARY 0
+#endif
+
+/* note: Log Version must change if Statistics Version changed */
+#define RVM_LOG_VERSION     "RVM Log Version  1.4 Oct 17, 1997 "
+
+/* general purpose macros */
+
+/* make sure realloc knows what to do with null ptr */
+#define REALLOC(x,l)      (((x)==NULL) ? malloc(l) : realloc((x),(l)))
+
+/* bcopy <=> memcpy, defs of syscalls */
+#ifndef BZERO
+#ifdef __STDC__
+#define BCOPY(x,y,n)     memcpy((y),(x),(n))
+#define BZERO(x,n)       memset((x),0,(n))
+#else
+#define BCOPY(x,y,n)     bcopy((x),(y),(n))
+#define BZERO(x,n)       bzero((x),(n))
+#endif
+#endif
+
+/* loop terminated by explicit break */
+#define DO_FOREVER  for (;;)
+#define MACRO_BEGIN			do {
+#define MACRO_END			} while(0)
+
+#define FORWARD     rvm_true            /* record scan forward */
+#define REVERSE     rvm_false           /* record scan reverse */
+
+
+
+/* RVM Internal Error Messages */
+
+#define ERR_DATE_SKEW       "Current time before last recorded - check kernel date"
+/* timestamp arithmetic */
+
+/* comparison macros */
+#define TIME_LSS(x,y)       (((x).tv_sec < (y).tv_sec) || \
+                             ((((x).tv_sec == (y).tv_sec) && \
+                             ((x).tv_usec < (y).tv_usec))))
+#define TIME_GTR(x,y)       (((x).tv_sec > (y).tv_sec) || \
+                             ((((x).tv_sec == (y).tv_sec) && \
+                             ((x).tv_usec > (y).tv_usec))))
+#define TIME_LEQ(x,y)       (!TIME_GTR((x),(y)))
+#define TIME_GEQ(x,y)       (!TIME_LSS((x),(y)))
+#define TIME_EQL(x,y)       (((x).tv_sec == (y).tv_sec) && \
+                             ((x).tv_usec == (y).tv_usec))
+#define TIME_EQL_ZERO(x)    (((x).tv_sec == 0 && ((x).tv_usec == 0)))
+
+#define ZERO_TIME(x)        MACRO_BEGIN \
+                            (x).tv_sec = 0; (x).tv_usec = 0; \
+                            MACRO_END
+
+/* range monitoring vector */
+typedef struct
+    {
+    char            *vmaddr;            /* range vm address */
+    rvm_length_t    length;             /* range length */
+    unsigned long   format;             /* data print format switches */
+    int             radix;              /* print radix for vmaddr */
+    }
+chk_vec_t;
+
+/* signal handler function type (rvmutl only) */
+typedef rvm_bool_t rvm_signal_call_t();
+
+/* recovery monitor call-back function type */
+typedef void rvm_monitor_call_t();
+/*  rvm_length_t    vmaddr;
+    rvm_length_t    length;
+    char            *data_ptr;
+    rvm_offset_t    *data_offset;
+    rec_hdr_t       *rec_hdr;
+    rvm_length_t    index;
+    char            *msg;
+*/
+/*                    round up & down macros
+            **** all depend on sizes being a power of 2 ****
+*/
+#define LENGTH_MASK          ((rvm_length_t)(~(sizeof(rvm_length_t)-1)))
+#define ROUND_TO_LENGTH(len) (((rvm_length_t)((rvm_length_t)(len) \
+                                              +sizeof(rvm_length_t)-1)) \
+                              & LENGTH_MASK)
+#define CHOP_TO_LENGTH(len)  ((rvm_length_t)((rvm_length_t)(len) \
+                                             & LENGTH_MASK))
+#define ALIGNED_LEN(addr,len) (ROUND_TO_LENGTH((rvm_length_t)(addr) \
+                                               +(rvm_length_t)(len)) \
+                               - CHOP_TO_LENGTH(addr))
+#define BYTE_SKEW(len)       ((rvm_length_t)(len) & ~LENGTH_MASK)
+
+#define SECTOR_SIZE          512
+#define SECTOR_MASK          ((rvm_length_t)(~(SECTOR_SIZE-1)))
+#define ROUND_TO_SECTOR_SIZE(x) (((rvm_length_t)(x)+SECTOR_SIZE-1) \
+                                 &SECTOR_MASK)
+#define CHOP_TO_SECTOR_SIZE(x)  ((rvm_length_t)(x)&SECTOR_MASK)
+
+#define SECTOR_INDEX(x)      ((x) & (SECTOR_SIZE-1))
+
+#define ROUND_OFFSET_TO_SECTOR_SIZE(x) \
+    rvm_rnd_offset_to_sector(&(x))
+
+#define CHOP_OFFSET_TO_SECTOR_SIZE(x) \
+    (RVM_MK_OFFSET(RVM_OFFSET_HIGH_BITS_TO_LENGTH(x), \
+                   CHOP_TO_SECTOR_SIZE(RVM_OFFSET_TO_LENGTH(x))))
+
+#define OFFSET_TO_SECTOR_INDEX(x) \
+    (SECTOR_INDEX(RVM_OFFSET_TO_LENGTH((x))))
+
+#define CHOP_OFFSET_TO_LENGTH_SIZE(x) \
+    (RVM_MK_OFFSET(RVM_OFFSET_HIGH_BITS_TO_LENGTH(x), \
+                   CHOP_TO_LENGTH(RVM_OFFSET_TO_LENGTH(x))))
+
+#define ROUND_TO_PAGE_SIZE(x) (((rvm_length_t)(x)+page_size-1) \
+                               &page_mask)
+#define CHOP_TO_PAGE_SIZE(x)  ((rvm_length_t)(x)&page_mask)
+
+/* other stuff... */
+
+#define OFFSET_TO_FLOAT(x) \
+    ((4.294967e+9)*((float)(RVM_OFFSET_HIGH_BITS_TO_LENGTH(x))) \
+     + (float)(RVM_OFFSET_TO_LENGTH(x)))
+/* internal structure id's */
+typedef enum
+    {
+    struct_first_id = 9,                /* base for free list array length */
+
+                                        /* free list allocated structures */
+    log_id,                             /* log device descriptor */
+    int_tid_id,                         /* internal transaction descriptor */
+    tid_rvm_id,                         /* external tid while on free list */
+    range_id,                           /* range descriptor */
+    seg_id,                             /* segment descriptor */
+    region_id,                          /* internal region descriptor */
+    region_rvm_id,                      /* external region while on free list */
+    options_rvm_id,                     /* external options while on free list */
+    statistics_rvm_id,                  /* rvm_statistics record while on free list */
+    mem_region_id,                      /* vm region tree node */
+    dev_region_id,                      /* device region tree node */
+    log_special_id,                     /* special log record */
+
+    struct_last_cache_id,               /* marker for free lists array length */
+
+                                        /* non-free list allocated structures */
+
+    log_status_id,                      /* log status descriptor */
+    log_dev_status_id,                  /* log device status area (on disk) */
+    log_wrap_id,                        /* log wrap-around marker */
+    log_seg_id,                         /* segment mapping marker in log */
+    seg_dict_id,                        /* recovery dictionary segment desc. */
+    trans_hdr_id,                       /* transaction header in log */
+    rec_end_id,                         /* log record end marker */
+    nv_range_id,                        /* new value range header */
+    nv_buf_id,                          /* new value vm buffer */
+    free_page_id,                       /* free page header descriptor */
+    rw_qentry_id,                       /* rw_lock queue entry */
+    tree_root_id,                       /* tree root */
+    /* mmapped_list_id,*/                    /* BSD/mmap systems only */
+    struct_last_id                      /* marker for last structure id */
+   }
+struct_id_t;
+/* macros to use struct_id's as int's & vice versa
+   for free list allocated structures only */
+#define ID_INDEX(id)    ((rvm_length_t)(id)-(rvm_length_t)struct_first_id-1)
+#define INDEX_ID(i)     ((struct_id_t)((i)+1+(long)struct_first_id))
+
+/* number of free list allocated structures */
+#define NUM_CACHE_TYPES ((rvm_length_t)struct_last_cache_id \
+                         -(rvm_length_t)struct_first_id-1)
+#define NUM_TYPES       ((rvm_length_t)struct_last_id \
+                         -(rvm_length_t)struct_first_id-1)
+
+/* preallocation sizes for internal structure free lists
+   must be in same order as above free list allocted enum's
+*/
+#define NUM_PRE_ALLOCATED \
+    0,                                  /* log's */ \
+    20,                                 /* tid's */ \
+    20,                                 /* rvm_tid's */ \
+    50,                                 /* range's */ \
+    0,                                  /* seg's */ \
+    10,                                 /* region's */ \
+    0,                                  /* rvm_region's */ \
+    0,                                  /* rvm_options */ \
+    2,                                  /* rvm_statistics */ \
+    10,                                 /* mem_region nodes */ \
+    1,                                  /* dev_region nodes */ \
+    1                                   /* special log markers */
+
+/* maximum sizes for internal structure free lists
+   must be in same order as above free list allocted enum's
+*/
+#define MAX_ALLOCATED \
+    0,                                  /* log's */ \
+    20,                                 /* tid's */ \
+    20,                                 /* rvm_tid's */ \
+    50,                                 /* range's */ \
+    0,                                  /* seg's */ \
+    10,                                 /* region's */ \
+    0,                                  /* rvm_region's */ \
+    0,                                  /* rvm_options */ \
+    2,                                  /* rvm_statistics */ \
+    10,                                 /* mem_region nodes */ \
+    2000,                               /* dev_region nodes */ \
+    1                                   /* special log markers */
+/* sizes and names of internal types
+   must be in same order as above enum's
+*/
+#define CACHE_TYPE_SIZES \
+    sizeof(log_t), \
+    sizeof(int_tid_t), \
+    sizeof(rvm_tid_t), \
+    sizeof(range_t), \
+    sizeof(seg_t), \
+    sizeof(region_t), \
+    sizeof(rvm_region_t), \
+    sizeof(rvm_options_t), \
+    sizeof(rvm_statistics_t), \
+    sizeof(mem_region_t), \
+    sizeof(dev_region_t), \
+    sizeof(log_special_t)
+
+#define OTHER_TYPE_SIZES \
+    0, \
+    sizeof(log_status_t), \
+    sizeof(log_dev_status_t), \
+    sizeof(log_wrap_t), \
+    sizeof(log_seg_t), \
+    sizeof(seg_dict_t), \
+    sizeof(trans_hdr_t), \
+    sizeof(rec_end_t), \
+    sizeof(nv_range_t), \
+    sizeof(nv_buf_t), \
+    sizeof(free_page_t), \
+    sizeof(rw_qentry_t), \
+    sizeof(tree_root_t)/*, \
+    sizeof(mmapped_list_t)*/
+
+#define TYPE_NAMES \
+    "log_id", \
+    "int_tid_id", \
+    "tid_rvm_id", \
+    "range_id", \
+    "seg_id", \
+    "region_id", \
+    "region_rvm_id", \
+    "options_rvm_id", \
+    "statistics_rvm_id", \
+    "mem_region_id", \
+    "dev_region_id", \
+    "log_special_id", \
+    "struct_last_cache_id", \
+    "log_status_id", \
+    "log_dev_status_id", \
+    "log_wrap_id", \
+    "log_seg_id", \
+    "seg_dict_id", \
+    "trans_hdr_id", \
+    "rec_end_id", \
+    "nv_range_id", \
+    "nv_buf_id", \
+    "free_page_id", \
+    "rw_qentry_id", \
+    "tree_root_id"/*, \
+    "mmapped_list_id"*/
+/* doubly-linked list cell header
+   this structure serves as the link and struct_id carrier for larger
+   structures when declared as the 1st field of the structure.
+   it is also used as the root, or header, of a list when statically allocated,
+   or embedded in another structure as other than the 1st field,
+   in which case its struct_id is that of the type of elements on the list.
+*/
+typedef struct list_entry_s
+    {
+    struct list_entry_s *nextentry;	/* in accordance with insque(3) */
+    struct list_entry_s *preventry;
+    union
+        {
+        struct list_entry_s  *name;     /* back pointer to head of list */
+        long                 length;    /* length of list if header */
+        }               list;
+    struct_id_t         struct_id;	/* self identifier; NEVER altered */
+    rvm_bool_t          is_hdr;         /* true if list header */
+    }
+list_entry_t;
+
+/* list macros, lst: address of list header */
+#define LIST_EMPTY(lst)     ((lst).list.length == 0)
+#define LIST_NOT_EMPTY(lst) ((lst).list.length != 0)
+
+/* list iterators for simple list traversals, no unlinking */
+#define FOR_ENTRIES_OF(lst,type,ptr)    /* list iterator, FIFO order */ \
+    for ( \
+         (ptr) = (type *)((lst).nextentry); \
+         !((ptr)->links.is_hdr); \
+         (ptr) = (type *)((ptr)->links.nextentry) \
+         )
+
+#define FOR_REVERSE_ENTRIES_OF(lst,type,ptr) /* list iterator, LIFO order */ \
+    for ( \
+         (ptr) = (type *)((lst).preventry); \
+         !((ptr)->links.is_hdr); \
+         (ptr) = (type *)((ptr)->links.preventry) \
+         )
+
+/* list iterators for traversals that unlink the entries */
+#define UNLINK_ENTRIES_OF(lst,type,ptr) /* list generator, FIFO order */ \
+    for ( \
+         (ptr) = (type *)((lst).nextentry); \
+         !((ptr)->links.is_hdr); \
+         (ptr) = (type *)((lst).nextentry) \
+         )
+
+#define UNLINK_REVERSE_ENTRIES_OF(lst,type,ptr) /* list generator, LIFO order */ \
+    for ( \
+         (ptr) = (type *)((lst).preventry); \
+         !((ptr)->links.is_hdr); \
+         (ptr) = (type *)((lst).preventry) \
+         )
+
+/* free page list entry */
+typedef struct
+    {
+    list_entry_t        links;          /* list links */
+    rvm_length_t        len;            /* length of free pages in bytes */
+    }
+free_page_t;
+/* Synchronization and Threads support */
+
+/*
+ * We can have one of three thread models:
+ *          cthreads:         Mach threads (kernel or coroutine)
+ *          lwp:              Coda's lightweight process package
+ *          pthreads:         POSIX threads
+ *
+ * If RVM_USELWP is defined, then lwp support is compiled in.
+ * If RVM_USEPT  is defined, then pthreads support is compiled in.
+ * If niether of these is defined, then cthreads support is compiled in.
+ *
+ * It is assumed in the rds package that cthreads and pthreads use
+ * preemptive scheduling, and they are synchronized appropriately.
+ *
+ * You must define only one of the above targets, and it must be defined
+ * consistently across the following packages: RVM, RDS, and URT
+ */
+
+#ifdef RVM_USELWP	 /* special thread support for Coda */
+#include "rvm/rvm_lwp.h"
+#elif defined(RVM_USEPT) /* special support for pthreads */
+#include "rvm/rvm_pthread.h"
+#else			 /* normal: use Cthreads */
+#include "rvm/cthreads.h"
+
+/* define types symbolically to permit use of non-Cthread thread support */
+#define RVM_MUTEX       struct mutex
+#define RVM_CONDITION	struct condition
+
+/* macro for testing if a lock is free */
+#define LOCK_FREE(lck) \
+    (mutex_try_lock(&(lck)) ? (mutex_unlock(&(lck)), rvm_true) : rvm_false)
+#endif
+
+/* protect a critical section
+   lck:  address of mutex
+   body: the critical section code
+*/
+#define CRITICAL(lck,body) \
+    MACRO_BEGIN \
+    mutex_lock(&(lck)); \
+    body; \
+    mutex_unlock(&(lck)); \
+    MACRO_END
+
+/*  rw_lock (read/write) support
+    An rw_lock permits many readers of a structure, but only
+    if there is no writer pending.  Only a single writer is permitted,
+    and to get the write lock, there must be no readers.
+    If a write is requested, no additional readers will be permitted
+    until the write is satisfied.  Blocked threads are processed in
+    FIFO order.
+*/
+typedef enum                            /* rw_lock access modes */
+    {
+    r = 32,                             /* get lock for read-only */
+    w,                                  /* get lock for read/write */
+    f                                   /* lock free, (internal use only) */
+    }
+rw_lock_mode_t;
+
+typedef struct                          /* rw_lock structure */
+    {
+    RVM_MUTEX           mutex;          /* mutex to protect rw_lock innards */
+    long                read_cnt;       /* read lock count, 0 ==> free */
+    long                write_cnt;      /* write lock count, 0 ==> free */
+    list_entry_t        queue;          /* blocked thread queue */
+    rw_lock_mode_t      lock_mode;      /* current lock mode */
+    }
+rw_lock_t;
+
+typedef struct                          /* rw_lock queue entry */
+    {
+    list_entry_t        links;          /* queue links & struct_id */
+    RVM_CONDITION       wait;           /* condition code for blocking */
+    rw_lock_mode_t      mode;           /* access mode */
+    }
+rw_qentry_t;
+
+/* protect a rw_lock critical section
+   lck:  address of rw_lock
+   mode: r or w
+   body: the critical section code
+*/
+#define RW_CRITICAL(rwl,mode,body) \
+    MACRO_BEGIN \
+    rw_lock(&(rwl),(mode)); \
+    body; \
+    rw_unlock(&(rwl),(mode)); \
+    MACRO_END
+
+/* macro for testing if an rw_lock is free */
+#define RW_LOCK_FREE(rwl) \
+    (((rwl).read_cnt+(rwl).write_cnt) == 0) && ((rwl).lock_mode == f)
+/* tree node structures */
+
+typedef struct tree_node_s              /* basic tree node */
+    {
+    struct tree_node_s  *lss;           /* ptr to less than entry */
+    struct tree_node_s  *gtr;           /* ptr to greater than entry */
+    long                bf;             /* balance factor */
+    struct_id_t         struct_id;      /* self identifier */
+    }
+tree_node_t;
+
+typedef union
+    {
+    tree_node_t         node;           /* links for trees */
+    list_entry_t        entry;          /* links for allocation cache */
+    }
+tree_links_t;
+
+typedef enum                            /* traversal states */
+    {
+    lss = 50,
+    self,
+    gtr,
+    init
+    }
+traverse_state_t;
+
+typedef struct                          /* tree traversal position entry */
+    {
+    tree_node_t         *ptr;           /* node pointer */
+    traverse_state_t     state;          /* state of traversal {lss,self,gtr} */
+    }
+tree_pos_t;
+
+typedef struct                          /* tree root structure */
+    {
+    struct_id_t         struct_id;      /* self identifier */
+    tree_node_t         *root;          /* ptr to root node */
+    tree_pos_t          *traverse;      /* traversal position vector */
+    rvm_length_t        traverse_len;   /* max length of traverse vector */
+    long                level;          /* current position in traversal
+                                           vector */
+    rvm_length_t        n_nodes;        /* number of nodes in tree */
+    rvm_length_t        max_depth;      /* length of deepest path in tree */
+    rvm_bool_t          unlink;         /* unlink nodes as traversed */
+    }
+tree_root_t;
+
+#define TRAVERSE_LEN_INCR  15           /* allocate 15 slots at a time */
+/* tree structure iterators
+     -- nodes are delinked as traversed
+     -- do not use tree_insert or tree_delete or otherwise change
+        tree shape in body of iterators if iteration is to be continued
+     -- iterators may not be nested for same tree
+*/
+#define FOR_NODES_OF(tree,type,ptr)     /* tree iterator, lss -> gtr order */ \
+    for ( \
+         (ptr) = (type *)init_tree_generator(&(tree),FORWARD,rvm_false); \
+         (ptr) != NULL; \
+         (ptr) = (type *)tree_successor(&(tree)) \
+         )
+
+#define FOR_REVERSE_NODES_OF(tree,type,ptr) /* tree iterator, gtr -> lss order */ \
+    for ( \
+         (ptr) = (type *)init_tree_generator(&(tree),REVERSE,rvm_false); \
+         (ptr) != NULL; \
+         (ptr) = (type *)tree_predecessor(&(tree)) \
+         )
+
+/* insertion test and iterate from existing nodes with equivalent key */
+#define FROM_EXISTING_NODE_OF(tree,type,ptr,node,cmp) \
+    for ( \
+         (ptr) = (type *)tree_iterate_insert(&(tree),(node),(cmp)); \
+         (ptr) != NULL; \
+         (ptr) = (type *)tree_successor(&(tree)) \
+         )
+
+#define UNLINK_NODES_OF(tree,type,ptr)  /* tree iterator, lss -> gtr order */ \
+    for ( \
+         (ptr) = (type *)init_tree_generator(&(tree),FORWARD,rvm_true); \
+         (ptr) != NULL; \
+         (ptr) = (type *)tree_successor(&(tree)) \
+         )
+
+#define UNLINK_REVERSE_NODES_OF(tree,type,ptr) /* tree iterator, gtr -> lss order */ \
+    for ( \
+         (ptr) = (type *)init_tree_generator(&(tree),REVERSE,rvm_true); \
+         (ptr) != NULL; \
+         (ptr) = (type *)tree_predecessor(&(tree)) \
+         )
+/* Structure to remember where we have/have not mmapped */
+
+/* vm buffers for dev_region_t nodes */
+typedef struct
+    {
+    struct_id_t         struct_id;      /* self identifier */
+    rvm_length_t        ref_cnt;        /* references to buffer */
+    rvm_length_t        chk_sum;        /* data buffer checksum */
+    rvm_length_t        alloc_len;      /* allocated length of buffer */
+    rvm_length_t        data_len;       /* length of log data */
+    char                *buf;           /* start of data area */
+    }
+nv_buf_t;
+
+#define NV_BUF_SIZE(len)  (ROUND_TO_LENGTH((len)) + sizeof(nv_buf_t))
+
+/* storage device region node */
+typedef struct
+    {
+    tree_links_t        links;          /* ptr structure */
+    rvm_offset_t        offset;         /* segment start offset of changes */
+    rvm_offset_t        end_offset;     /* end offset (offset + length) */
+    rvm_length_t        length;         /* length of region */
+    char                *nv_ptr;        /* ptr into nv_buf */
+    nv_buf_t            *nv_buf;        /* buffer for new values if allocated */
+    rvm_offset_t        log_offset;     /* location of new values in log */
+    char                *vmaddr;        /* original vm addr (debug use only) */
+    }
+dev_region_t;
+
+/* virtual memory region node */
+typedef struct
+    {
+    tree_links_t    links;              /* ptr structure */
+    struct region_s *region;            /* region descriptor */
+    char            *vmaddr;            /* base address */
+    rvm_length_t    length;             /* length of vm region */
+    }
+mem_region_t;
+
+/* node comparator function type */
+typedef long cmp_func_t();
+/*  tree_node_t     *node1;
+    tree_node_t     *node2;
+*/
+/* log records written by commit, and associated with new value records */
+/* generic record header; not actually allocated, but any record header
+   can be cast to this to get its type & length for detailed analysis
+*/
+typedef struct
+    {
+    struct_id_t     struct_id;          /* type of entry */
+    rvm_length_t    rec_length;         /* record length */
+    struct timeval  timestamp;          /* timestamp of record entry */
+    rvm_length_t    rec_num;            /* record number of entry */
+    }
+rec_hdr_t;
+
+/* transaction record header: trans_hdr_t -- a single copy in the log descriptor
+*/
+typedef struct
+    {
+    rec_hdr_t	    rec_hdr;		/* common log record header */
+    rvm_length_t    num_ranges;         /* number of ranges in record */
+    struct timeval  uname;              /* uname of transaction */
+    struct timeval  commit_stamp;       /* timestamp of commit */
+    rvm_length_t    n_coalesced;        /* count of coalesced transactions */
+    rvm_length_t    flags;              /* mode and optimization flags */
+    }
+trans_hdr_t;
+
+/* new value record range header: nv_range_t */
+typedef struct
+    {
+    rec_hdr_t	    rec_hdr;		/* common log record header */
+    rvm_length_t    sub_rec_len;        /* back displacement to previous hdr */
+    rvm_length_t    range_num;          /* range number in record */
+    rvm_length_t    length;             /* actual modification length */
+    rvm_offset_t    offset;             /* offset of changes in segment */
+    char            *vmaddr;            /* modification vm address */
+    rvm_length_t    chk_sum;            /* data checksum */
+    long            seg_code;           /* segment short name */
+    rvm_bool_t      is_split;           /* is a range split for log wrap */
+    }
+nv_range_t;
+/* special log types -- these records are inserted into the log to
+   record events not related to transaction commit and new value
+   recording.
+   These are generally used by the recovery algorithm to reconstruct the
+   committed images of segments at the time of a crash.
+*/
+/* segment mapping descriptor -- inserted by map when a segment
+   is mapped the first time; used to relate the short names to
+   an actual device or file name */
+typedef struct
+    {
+    long            seg_code;           /* segment short name */
+    rvm_offset_t    num_bytes;          /* maximum usable length of seg dev */
+    long            name_len;           /* length of segment name */
+    char            *name;              /* full path name */
+    }
+log_seg_t;
+
+/* log_special_t: the carrier for all special log types
+   free list allocated; additional type-dependent data can be placed
+   after this structure; all records end with rec_end_t record
+*/
+typedef struct
+    {
+    list_entry_t    links;              /* list links and free list struct id */
+                                        /* following fields are written in log */
+    rec_hdr_t	    rec_hdr;		/* common log record header */
+    union
+        {
+        log_seg_t   log_seg;            /* segment mapping marker */
+        }           special;
+    }
+log_special_t;
+/* generic log entry types */
+
+/* log record end marker: rec_end_t -- a single copy in the log descriptor */
+typedef struct
+    {
+    rec_hdr_t	    rec_hdr;		/* common log record header */
+    struct_id_t     rec_type;           /* type of recorded ended */
+    rvm_length_t    sub_rec_len;        /* back displacement to previous sub-
+                                           record; same as rec_length if none */
+    }
+rec_end_t;
+
+/* log wrap-around marker -- a single copy in the log descriptor */
+typedef struct
+    {
+    rec_hdr_t	    rec_hdr;		/* common log record header */
+    struct_id_t     struct_id2;         /* for scan_wrap_reverse()! */
+    }
+log_wrap_t;
+
+/* device descriptor -- included in log and segment descriptors */
+typedef struct
+    {
+    char            *name;              /* print name of device */
+    long            name_len;           /* allocation length */
+    long            handle;             /* device handle */
+    rvm_offset_t    num_bytes;          /* length of device */
+    rvm_bool_t      raw_io;             /* true if using raw i/o */
+    unsigned long   type;               /* to store device type */
+    rvm_bool_t      read_only;          /* true if opened read-only */
+
+    struct iovec   *iov;                /* gather write io vector */
+    long            iov_length;         /* length of iov array */
+    long            iov_cnt;            /* count of entries used in iov */
+    rvm_length_t    io_length;          /* accumulated length of i/o */
+    rvm_offset_t    last_position;      /* last location seeked or transfered */
+                                        /* the following fields are used for
+                                           log devices only */
+    char            *wrt_buf;           /* working raw io write buffer base */
+    rvm_length_t    wrt_buf_len;        /* usable wrt_buf length */
+    char            *ptr;               /* write buffer fill ptr */
+    char            *buf_start;         /* start of buffer flush region */
+    char            *buf_end;           /* end of buffer */
+    rvm_offset_t    sync_offset;        /* end offset after last sync */
+
+    char            *pad_buf;           /* padding buffer */
+    long            pad_buf_len;        /* length of current pad buf */
+    }
+device_t;
+/* log structure macros */
+
+#define RANGE_LEN(range)    (ALIGNED_LEN((range)->nv.vmaddr, \
+                                         (range)->nv.length))
+
+#define RANGE_SIZE(range)   ((rvm_length_t)(NV_RANGE_OVERHEAD \
+                            + RANGE_LEN(range)))
+
+#define TRANS_SIZE          (ROUND_TO_LENGTH((sizeof(trans_hdr_t) \
+                                              + sizeof(rec_end_t))))
+
+#define NV_RANGE_OVERHEAD   (ROUND_TO_LENGTH(sizeof(nv_range_t)))
+
+#define MIN_NV_RANGE_SIZE   (NV_RANGE_OVERHEAD+64)
+
+#define MIN_TRANS_SIZE      (TRANS_SIZE + MIN_NV_RANGE_SIZE \
+                             + ROUND_TO_LENGTH(sizeof(log_wrap_t)))
+
+#define LOG_SPECIAL_SIZE    (ROUND_TO_LENGTH(sizeof(log_special_t) \
+                                             - sizeof(list_entry_t)))
+
+#define LOG_SPECIAL_IOV_MAX 3
+
+/* largest log type header on disc */
+#define MAX_HDR_SIZE        (ROUND_TO_LENGTH((sizeof(log_special_t) \
+                                             + MAXPATHLEN)))
+/* other constants */
+
+/* maximum size nv's kept in vm during recovery */
+#define NV_LOCAL_MAX        (8*1024 - ROUND_TO_LENGTH(NV_BUF_SIZE( \
+                                              sizeof(rvm_length_t)+1)))
+
+/* size of status area i/o buffer */
+#define LOG_DEV_STATUS_SIZE \
+                ROUND_TO_SECTOR_SIZE(sizeof(log_dev_status_t))
+
+/* offsets for log status structures in files and partitions */
+#define RAW_STATUS_OFFSET   16*SECTOR_SIZE
+#define FILE_STATUS_OFFSET  0
+
+#define UPDATE_STATUS       100         /* flushes before updating log status area */
+/* log status descriptor -- included in the log descriptor */
+#ifdef RVM_LOG_TAIL_SHADOW
+extern rvm_offset_t log_tail_shadow;
+extern rvm_bool_t   has_wrapped;
+#define RVM_ASSIGN_OFFSET(x,y)  (x) = (y)
+#endif /* RVM_LOG_TAIL_SHADOW */
+
+typedef struct
+    {
+                                        /* status area control fields */
+    long            update_cnt;         /* number of updates before write */
+    rvm_bool_t      valid;              /* data in status area valid */
+    rvm_bool_t      log_empty;          /* true if log device & buffer empty */
+
+                                        /* log pointers & limits */
+    rvm_offset_t    log_start;          /* first offset for records */
+    rvm_offset_t    log_size;           /* dev.num_bytes - log_start:
+                                           space for records */
+    rvm_offset_t    log_head;           /* current log head */
+    rvm_offset_t    log_tail;           /* current log tail */
+    rvm_offset_t    prev_log_head;      /* previous head (truncation only) */
+    rvm_offset_t    prev_log_tail;      /* previous tail (truncation only) */
+
+                                        /* consistency check fields */
+    struct timeval  status_init;        /* timestamp log creation */
+    struct timeval  status_write;       /* timestamp for last status write*/
+    struct timeval  last_trunc;         /* timestamp for last truncation */
+    struct timeval  prev_trunc;         /* timestamp for previous truncation */
+    struct timeval  first_write;        /* timestamp of first record in log */
+    struct timeval  last_write;         /* timestamp of last record in log */
+    struct timeval  first_uname;        /* first transaction uname in log */
+    struct timeval  last_uname;         /* last transaction uname in log */
+    struct timeval  last_commit;        /* last transaction commit timestamp */
+    struct timeval  wrap_time;          /* wrap timestamp if log wrapped */
+    rvm_length_t    first_rec_num;      /* 1st rec num of truncation epoch */
+    rvm_length_t    last_rec_num;       /* last rec num of truncation epoch */
+    rvm_length_t    next_rec_num;       /* assignment counter for rec_nums */
+
+                                        /* transaction statistics */
+    rvm_length_t    n_abort;            /* number of transactions aborted */
+    rvm_length_t    n_flush_commit;     /* number of flush mode commits */
+    rvm_length_t    n_no_flush_commit;  /* number of no_flush mode commits */
+    rvm_length_t    n_split;            /* number trans split for log wrap */
+    rvm_length_t    n_truncation_wait;  /* transactions delayed by truncation */
+
+                                        /* log statistics */
+    rvm_length_t    n_flush;            /* number of internal flushes */
+    rvm_length_t    n_rvm_flush;        /* number of explicit flush calls */
+    rvm_length_t    n_special;          /* number of special log records */
+    rvm_offset_t    range_overlap;      /* current overlap eliminated by range coalesce */
+    rvm_offset_t    trans_overlap;      /* current overlap eliminated by trans coalesce */
+    rvm_length_t    n_range_elim;       /* number of ranges eliminated by
+                                           range coalesce/flush */
+    rvm_length_t    n_trans_elim;       /* number of ranges eliminated by
+                                           trans coalesce/flush */
+    rvm_length_t    n_trans_coalesced;  /* number of transactions coalesced in
+                                           this flush cycle */
+    struct timeval  flush_time;         /* time spent in flushes */
+    rvm_length_t    last_flush_time;    /* duration of last flush (msec) */
+    rvm_length_t    last_truncation_time; /* duration of last truncation (sec) */
+    rvm_length_t    last_tree_build_time; /* duration of tree build (sec) */
+    rvm_length_t    last_tree_apply_time; /* duration of tree apply phase
+                                             (sec) */
+
+                                        /* histogram vectors */
+
+    rvm_length_t    flush_times[flush_times_len]; /* flush timings */
+    rvm_length_t    range_lengths[range_lengths_len]; /* range lengths flushed */
+    rvm_length_t    range_elims[range_elims_len]; /* num ranges eliminated by
+                                                     range coalesce/flush */
+    rvm_length_t    trans_elims[trans_elims_len]; /* num ranges eliminated by
+                                                     trans coalesce/flush */
+    rvm_length_t    range_overlaps[range_overlaps_len]; /* space saved by
+                                                           range coalesce/flush */
+    rvm_length_t    trans_overlaps[range_overlaps_len]; /* space saved by
+                                                           trans coalesce/flush */
+
+                                        /* cummulative transaction stats */
+    rvm_length_t    tot_abort;          /* total aborted transactions */
+    rvm_length_t    tot_flush_commit;   /* total flush commits */
+    rvm_length_t    tot_no_flush_commit; /* total no_flush commits */
+    rvm_length_t    tot_split;          /* total transactions split for log
+                                           wrap-around */
+
+                                        /* cummulative log statistics */
+    rvm_length_t    tot_flush;          /* total internal flush calls  */
+    rvm_length_t    tot_rvm_flush;      /* total explicit rvm_flush calls  */
+    rvm_length_t    tot_special;        /* total special log records */
+    rvm_length_t    tot_wrap;           /* total log wrap-arounds */
+    rvm_length_t    log_dev_max;        /* maximum % log device used so far */
+    rvm_offset_t    tot_log_written;    /* total length of all writes to log */
+    rvm_offset_t    tot_range_overlap;  /* total overlap eliminated by range coalesce */
+    rvm_offset_t    tot_trans_overlap;  /* total overlap eliminated by trans coalesce */
+    rvm_length_t    tot_range_elim;     /* total number of ranges eliminated by
+                                           range coalesce */
+    rvm_length_t    tot_trans_elim;     /* total number of ranges eliminated by
+                                           trans coalesce */
+    rvm_length_t    tot_trans_coalesced; /* total number of transactions coalesced */
+
+                                        /* truncation statistics */
+    rvm_length_t    tot_rvm_truncate;   /* total explicit rvm_truncate calls */
+    rvm_length_t    tot_async_truncation; /* total asynchronous truncations */
+    rvm_length_t    tot_sync_truncation; /* total forced synchronous truncations */
+    rvm_length_t    tot_truncation_wait; /* total transactions delayed by truncation */
+    rvm_length_t    tot_recovery;       /* total recovery truncations */
+    struct timeval  tot_flush_time;     /* total time spent in flush */
+    struct timeval  tot_truncation_time; /* cumulative truncation time */
+
+                                        /* histogram vectors */
+
+    rvm_length_t    tot_tree_build_times[truncation_times_len]; /* truncation timings */
+    rvm_length_t    tot_tree_apply_times[truncation_times_len];
+    rvm_length_t    tot_truncation_times[truncation_times_len];
+    rvm_length_t    tot_flush_times[flush_times_len]; /* cummulative flush timings */
+    rvm_length_t    tot_range_lengths[range_lengths_len]; /* cummulative range lengths flushed */
+    rvm_length_t    tot_range_elims[range_elims_len]; /* total num ranges eliminated by
+                                                         range coalesce/flush */
+    rvm_length_t    tot_trans_elims[trans_elims_len]; /* total num ranges eliminated by                                                 trans coalesce/flush */
+    rvm_length_t    tot_range_overlaps[range_overlaps_len]; /* space saved by
+                                                           range coalesce/flush */
+    rvm_length_t    tot_trans_overlaps[range_overlaps_len]; /* space saved by
+                                                           trans coalesce/flush */
+    rvm_length_t    tot_trans_coalesces[trans_coalesces_len]; /* transactions coalesced
+                                                                 per flush  */
+    rvm_length_t    flush_state;        /* flush status */
+    rvm_length_t    trunc_state;        /* truncation status */
+    }
+log_status_t;
+/* log status descriptor on log device: log_dev_status_t */
+typedef struct
+    {
+    struct_id_t     struct_id;          /* self identifier */
+    rvm_length_t    chk_sum;            /* check sum */
+    char            version[RVM_VERSION_MAX]; /* RVM interface version string */
+    char            log_version[RVM_VERSION_MAX]; /* RVM log version string */
+    char            statistics_version[RVM_VERSION_MAX]; /* RVM statistics version string */
+    log_status_t    status;             /* log status info */
+    }
+log_dev_status_t;
+
+/* Flush and Truncation states */
+                                        /* log flush initiated by rvm_flush */
+#define RVM_FLUSH_CALL      (1)
+                                        /* log flush initated by commit */
+#define RVM_FLUSH_COMMIT    (2)
+                                        /* truncation initiated by rvm_truncate */
+#define RVM_RECOVERY        (4)
+#define RVM_TRUNCATE_CALL   (010)
+                                        /* truncation initiated by rvm daemon */
+#define RVM_ASYNC_TRUNCATE  (020)
+                                        /* truncation forced by flush */
+#define RVM_SYNC_TRUNCATE   (040)
+                                        /* truncation phase 1: find current log tail */
+#define RVM_TRUNC_FIND_TAIL (0100)
+                                        /* phase 2: build modification trees */
+#define RVM_TRUNC_BUILD_TREE (0200)
+                                        /* phase 3: apply modifications */
+#define RVM_TRUNC_APPLY     (0400)
+                                        /* phase 4: update log status */
+#define RVM_TRUNC_UPDATE    (01000)
+
+#define RVM_TRUNC_PHASES    (RVM_TRUNC_FIND_TAIL | RVM_TRUNC_BUILD_TREE \
+                             | RVM_TRUNC_APPLY | RVM_TRUNC_UPDATE)
+
+/* log recovery buffer descriptor -- single copy in log descriptor */
+typedef struct
+    {
+    char            *buf;               /* working recovery buffer base */
+    char            *shadow_buf;
+    long            length;             /* length of allocated buffer */
+    rvm_offset_t    buf_len;            /* log buffer length as offset */
+    long            r_length;           /* length of data read into buffer */
+    rvm_offset_t    offset;             /* offset of buffer start in segment */
+    long            ptr;                /* index of present buffer position */
+    struct timeval  timestamp;          /* timestamp of transaction in buffer */
+
+    char            *aux_buf;           /* working auxillary buffer base */
+    long            aux_length;         /* length of aux_buf */
+    rvm_offset_t    aux_offset;         /* offset of data in buffer */
+    long            aux_rlength;        /* length of data read into buffer */
+
+    struct timeval  prev_timestamp;     /* timestamp of previous record */
+    rvm_length_t    prev_rec_num;       /* previous record number */
+    rvm_bool_t      prev_direction;     /* last scanning direction */
+    rvm_bool_t      split_ok;           /* ok to process split records */
+    }
+log_buf_t;
+
+/* log buffer management defs */
+
+#define SYNCH       rvm_true            /* synchronization required */
+#define NO_SYNCH    rvm_false           /* synchronization not required */
+/* log truncation daemon control structures */
+
+typedef enum
+    {
+    rvm_idle = 1000,                    /* daemon idle */
+    init_truncate,                      /* initiate truncation */
+    truncating,                         /* truncation in progress */
+    terminate,                          /* shutdown */
+    error                               /* terminated due to error */
+    }
+daemon_state_t;
+
+typedef struct
+    {
+    cthread_t       thread;             /* daemon thread handle */
+    RVM_MUTEX       lock;               /* daemon lock -- protects following
+                                           fields */
+    RVM_CONDITION   code;               /* condition code to signal daemon */
+    RVM_CONDITION   flush_flag;         /* condition code to signal flush */
+    RVM_CONDITION   wake_up;            /* conditon code to signal threads
+                                           waiting for truncation completion */
+    daemon_state_t  state;              /* control state */
+    long            truncate;           /* truncation threshold, as % of log */
+    }
+log_daemon_t;
+/* log descriptor */
+typedef struct
+    {
+    list_entry_t    links;              /* list links and struct id -- points
+                                           to log list root */
+    long            ref_cnt;            /* count seg's using this log device */
+
+    RVM_MUTEX       dev_lock;           /* log device lock, protects device and
+                                           following i/o related fields: */
+    device_t        dev;                /* log device descriptor */
+    log_status_t    status;             /* log status area descriptor */
+    trans_hdr_t     trans_hdr;          /* i/o header for transaction log entry */
+    rec_end_t       rec_end;            /* i/o end marker for log entry */
+    log_wrap_t      log_wrap;           /* i/o log wrap-around marker */
+    log_buf_t       log_buf;            /* log recovery buffer */
+                                        /* end of log_dev_lock protected fields */
+
+    RVM_MUTEX       tid_list_lock;      /* lock for tid list header & links
+                                           used when adding/deleting a tid */
+    list_entry_t    tid_list;           /* root of active transaction list */
+
+    RVM_MUTEX       flush_list_lock;    /* lock for flush list header & links
+                                           used to add/delete a no_flush tid */
+    list_entry_t    flush_list;         /* list of no_flush committed tid's */
+
+    RVM_MUTEX       special_list_lock;  /* lock for special list header & links
+                                           used to add/delete a special entry */
+    list_entry_t    special_list;       /* list of special log entries */
+
+    rw_lock_t       flush_lock;         /* log flush synchronization */
+    log_daemon_t    daemon;             /* truncation daemon control */
+    RVM_MUTEX       truncation_lock;    /* truncation synchronization */
+    cthread_t       trunc_thread;
+    rvm_bool_t      in_recovery;        /* true if in recovery */
+
+    struct seg_dict_s
+                    *seg_dict_vec;      /* recovery segment dictionary */
+    long            seg_dict_len;       /* length of seg_dict_vec */
+    device_t        *cur_seg_dev;       /* current segment device in truncation */
+    }
+log_t;
+/* segment descriptor: seg_t */
+typedef struct
+    {
+    list_entry_t    links;              /* list links and struct id */
+
+    RVM_MUTEX       dev_lock;           /* device lock */
+    device_t        dev;                /* segment device descriptor */
+    long            seg_code;           /* short name for log entries */
+    log_t           *log;               /* log descriptor ptr */
+
+    RVM_MUTEX       seg_lock;           /* lock for seg lists: protects header
+                                           and links -- used when mapping or
+                                           unmapping a region */
+    list_entry_t    map_list;           /* mapped region list header */
+    list_entry_t    unmap_list;         /* unmapped region list header */
+
+    rvm_bool_t      threads_waiting;    /* at least one thread is waiting to
+                                           map a previously unmapped region */
+    }
+seg_t;
+
+/* recovery dictionary segment descriptor: seg_dict_t */
+struct seg_dict_s
+    {
+    struct_id_t     struct_id;          /* self-identifier */
+    seg_t           *seg;               /* ptr to real segment */
+    device_t        dev;                /* used in recovery only */
+    long            seg_code;           /* short segment id */
+    tree_root_t     mod_tree;           /* modification tree for recovery */
+    };
+
+typedef struct seg_dict_s seg_dict_t;
+
+#define SEG_DICT_INDEX(x)   ((x)-1)     /* index of segemnt in seg_dict_vec */
+/* region descriptor: region_t */
+typedef struct region_s
+    {
+    list_entry_t    links;              /* list links and struct id
+                                           -- protected by seg.map_lock
+                                              or seg.unmap_lock */
+    rw_lock_t       region_lock;        /* rw lock for following fields */
+    seg_t           *seg;               /* back ptr to segment */
+    mem_region_t    *mem_region;        /* back ptr to region tree node */
+    rvm_offset_t    offset;             /* offset of region base in segment */
+    rvm_offset_t    end_offset;         /* offset of region end in segment */
+    char            *vmaddr;            /* virtual memory base address */
+    rvm_length_t    length;             /* length of region */
+    rvm_bool_t      no_copy;            /* data not copied on map */
+
+    RVM_MUTEX       count_lock;         /* accounting lock for next 2 fields */
+    long            n_uncommit;         /* # uncommitted modifications in region */
+    rvm_bool_t      dirty;              /* dirty bit; set by end_transaction */
+
+    struct timeval  unmap_ts;           /* unmap timestamp for truncation chk */
+    }
+region_t;
+
+/* modification range descriptor: range_t */
+typedef struct
+    {
+    tree_links_t    links;              /* tree links and struct id */
+    char            *data;              /* old/new values, when used */
+    rvm_length_t    data_len;           /* allocation length of data buffer */
+    char            *nvaddr;            /* address of saved new values */
+    region_t        *region;            /* back ptr to affected region */
+    rvm_offset_t    end_offset;         /* end byte of range */
+    nv_range_t      nv;                 /* nv range record header for i/o */
+    }
+range_t;
+/* transaction id descriptor: int_tid_t */
+typedef struct
+    {
+    list_entry_t    links;              /* list links and struct id; protected
+                                           by log tid_list_lock */
+    rw_lock_t       tid_lock;           /* remaining fields protected by
+                                           tid_lock until on flush list*/
+    struct timeval  uname;              /* unique identifier */
+    struct timeval  commit_stamp;       /* timestamp of commit */
+    log_t           *log;               /* back link to log descriptor */
+    rvm_offset_t    log_size;           /* log space required */
+    tree_root_t     range_tree;         /* range tree root */
+    range_t         **x_ranges;         /* vector of overlaping ranges */
+    long            x_ranges_alloc;     /* allocated length of x_ranges */
+    long            x_ranges_len;       /* current length of x_ranges */
+    long            range_elim;         /* ranges eliminated by range coalesce */
+    long            trans_elim;         /* ranges eliminated by trans coalesce */
+    rvm_offset_t    range_overlap;      /* overlap eliminated by range coalesce */
+    rvm_offset_t    trans_overlap;      /* overlap eliminated by trans coalesce */
+    rvm_length_t    n_coalesced;        /* count of coalesced transactions */
+    range_t         split_range;        /* extra range for flush */
+    rvm_length_t    flags;              /* mode and optimization flags */
+    rvm_length_t    back_link;          /* displacement to previous header */
+    }
+int_tid_t;
+
+/* definitions for tid flags field (also used in trans_hdr flags field */
+#define RESTORE_FLAG        (2*RVM_COALESCE_TRANS)
+#define FLUSH_FLAG          (2*RESTORE_FLAG)
+#define FIRST_ENTRY_FLAG    (2*FLUSH_FLAG)
+#define LAST_ENTRY_FLAG     (2*FIRST_ENTRY_FLAG)
+#define FLUSH_MARK          (2*LAST_ENTRY_FLAG)
+
+#define TID(x)              ((tid->flags & (x)) != 0)
+#define TRANS_HDR(x)        ((trans_hdr->flags & (x)) != 0)
+/* functions and structures for managing list of RVM-allocated
+     regions of memory (added by tilt, Nov 19 1996) */
+
+typedef struct rvm_page_entry {
+    char                   *start;
+    char                   *end;
+    struct rvm_page_entry  *prev;
+    struct rvm_page_entry  *next;
+} rvm_page_entry_t;
+
+rvm_bool_t rvm_register_page(char *vmaddr, rvm_length_t length);
+rvm_bool_t rvm_unregister_page(char *vmaddr, rvm_length_t length);
+rvm_bool_t mem_chk(char *vmaddr, rvm_length_t length);
+rvm_page_entry_t *find_page_entry(char *vmaddr);
+/* list management functions */
+
+extern
+void init_list_header();                /* [rvm_utils.c] */
+/*  list_entry_t    *whichlist;
+    struct_id_t     struct_id;
+*/
+extern
+list_entry_t *move_list_entry();        /* [rvm_utils.c] */
+/*  register list_entry_t *fromptr;
+    register list_entry_t *toptr;
+    register list_entry_t *cell;
+*/
+extern
+void insert_list_entry();               /* [rvm_utils.c] */
+/*  register list_entry_t *entry;
+    register list_entry_t *new_entry;
+*/
+extern
+list_entry_t *alloc_list_entry();        /* [rvm_utils.c] */
+/*  struct_id_t     id; */
+
+/* internal type allocators/deallocators */
+
+extern
+void clear_free_list();                 /* [rvm_utils.c] */
+/*  struct_id_t     id; */
+
+extern
+region_t *make_region();                /* [rvm_utils.c] */
+
+extern
+void free_region();                     /* [rvm_utils.c] */
+/*  region_t        *region; */
+
+extern
+seg_t *make_seg();                      /* [rvm_utils.c] */
+/*  char            *seg_dev_name;
+    rvm_return_t    *retval
+*/
+extern
+void free_seg();                        /* [rvm_utils.c] */
+/*  seg_t           *seg; */
+
+extern
+void free_seg_dict_vec();               /* [rvm_utils.c] */
+/*  log_t           *log; */
+
+extern
+log_t *make_log();                      /* [rvm_utils.c] */
+/*  char            *dev_name;
+    rvm_return_t    *retval
+*/
+extern
+void free_log();                        /* [rvm_utils.c] */
+/*  log_t           *log; */
+
+extern
+char *make_full_name();                /* [rvm_utils.c] */
+/*  char            *dev_str;
+    char            *dev_name;
+    rvm_return_t    *retval;
+*/
+extern
+void free_log();                        /* [rvm_utils.c] */
+/*  log_t           *log; */
+
+extern
+log_special_t *make_log_special();      /* [rvm_utils.c] */
+/*  struct_id_t     special_id;
+    rvm_length_t    length;
+*/
+extern
+void free_log_special();                /* [rvm_utils.c] */
+/*  log_special_t   *special; */
+extern
+rvm_return_t dev_init();                /* [rvm_utils.c] */
+/*  device_t        *dev;
+    char            *dev_str;
+*/
+extern
+range_t *make_range();                  /* [rvm_utils.c] */
+
+extern
+void free_range();                      /* [rvm_utils.c] */
+/*  range_t         *range; */
+
+extern
+int_tid_t *make_tid();                  /* [rvm_utils.c] */
+/*  rvm_mode_t      mode; */
+
+extern
+void free_tid();                        /* [rvm_utils.c] */
+/*  register int_tid_t  *tid; */
+
+extern
+mem_region_t *make_mem_region();        /* [rvm_utils.c] */
+
+extern
+void free_mem_region();                 /* [rvm_utils.c] */
+/*  mem_region_t   *node; */
+
+extern
+dev_region_t *make_dev_region();        /* [rvm_utils.c] */
+
+extern
+void free_dev_region();                 /* [rvm_utils.c] */
+/*  dev_region_t   *node; */
+/* log management functions */
+
+extern
+void init_log_list();                   /* [rvm_logstatus.c] */
+
+extern
+void enter_log();                       /* [rvm_logstatus.c] */
+/*  log_t           *log; */
+
+extern
+log_t *find_log();                      /* [rvm_logstatus.c] */
+/*  char            *log_dev; */
+
+extern
+rvm_return_t open_log();                /* [rvm_logstatus.c] */
+/*  char            *dev_name;
+    log_t           **log_ptr;
+    char            *status_buf;
+    rvm_options_t   *rvm_options;
+*/
+extern
+rvm_return_t create_log();              /* [rvm_logstatus.c] */
+/*  log_t           **log_ptr;
+    rvm_options_t   *rvm_options;
+*/
+extern
+rvm_return_t do_log_options();          /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    rvm_options_t   *rvm_options;
+*/
+extern
+rvm_return_t close_log();               /* [rvm_logstatus.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t close_all_logs();          /* [rvm_logstatus.c] */
+
+extern
+void copy_log_stats();                  /* [rvm_logstatus.c] */
+/*  log_t           *log; */
+
+extern
+void clear_log_status();                /* [rvm_logstatus.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t init_log_status();         /* [rvm_logstatus.c] */
+/*  log_t           *log; */
+extern
+rvm_return_t read_log_status();         /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    char            *status_buf;
+*/
+extern
+rvm_return_t write_log_status();        /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    device_t        *dev;
+*/
+extern
+rvm_return_t update_log_tail();         /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    rec_hdr_t       *rec_hdr;
+*/
+extern
+void log_tail_length();                 /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    rvm_offset_t    *tail_length;
+*/
+extern
+void log_tail_sngl_w();                 /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    rvm_offset_t    *tail_length;
+*/
+extern
+long cur_log_percent();                 /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    rvm_offset_t    *space_nneded;
+*/
+extern
+void cur_log_length();                  /* [rvm_logstatus.c] */
+/*  log_t           *log;
+    rvm_offset_t    *length;
+*/
+extern
+rvm_return_t queue_special();           /* [rvm_logflush.c] */
+/*  log_t           *log;
+    log_special_t   *special;
+*/
+extern
+rvm_return_t flush_log_special();       /* [rvm_logflush.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t flush_log();               /* [rvm_logflush.c] */
+/*  log_t           *log;
+    long            *count;
+*/
+extern
+rvm_return_t locate_tail();             /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t init_buffer();             /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_offset_t    *offset;
+    rvm_bool_t      direction;
+    rvm_bool_t      synch;
+*/
+extern
+void clear_aux_buf();                   /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t load_aux_buf();            /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_offset_t    *offset;
+    rvm_length_t    length;
+    rvm_length_t    *aux_ptr;
+    rvm_length_t    *data_len;
+    rvm_bool_t      direction;
+    rvm_bool_t      synch;
+*/
+extern
+void reset_hdr_chks();                  /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+extern
+rvm_bool_t chk_hdr_type();              /* [rvm_logrecovr.c] */
+/*  rec_hdr_t       *rec_hdr; */
+
+extern
+rvm_bool_t chk_hdr_currency();          /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rec_hdr_t       *rec_hdr;
+*/
+extern
+rvm_bool_t chk_hdr_sequence();          /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rec_hdr_t       *rec_hdr;
+    rvm_bool_t      direction;
+*/
+extern
+rvm_bool_t chk_hdr();                   /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rec_hdr_t       *rec_hdr;
+    rec_end_t       *rec_end;
+    rvm_bool_t      direction;
+*/
+extern
+rvm_bool_t validate_hdr();              /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rec_hdr_t       *rec_hdr;
+    rec_end_t       *rec_end;
+    rvm_bool_t      direction;
+*/
+extern
+rvm_return_t validate_rec_forward();    /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_return_t validate_rec_reverse();    /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_return_t scan_forward();            /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_return_t scan_reverse();            /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_return_t scan_nv_forward();         /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_return_t scan_nv_reverse();         /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_return_t scan_wrap_reverse();       /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rvm_bool_t      synch;
+*/
+extern
+rvm_bool_t initiate_truncation();       /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    long            threshold;
+*/
+extern
+rvm_return_t wait_for_truncation();     /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    struct timeval  *time_stamp;
+*/
+extern
+rvm_return_t log_recover();             /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    long            *count;
+    rvm_bool_t      is_daemon;
+    rvm_length_t    flag;
+*/
+extern
+void log_daemon();                      /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t change_tree_insert();      /* [rvm_logrecovr.c] */
+/*  seg_dict_t      *seg_dict;
+    dev_region_t    *node;
+*/
+extern
+rvm_return_t apply_mods();              /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+extern
+rvm_return_t alloc_log_buf();           /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+extern
+void free_log_buf();                    /* [rvm_logrecovr.c] */
+/*  log_t           *log; */
+
+/* Segment & region management functions */
+/* [rvm_map.c] */
+void init_map_roots(void);
+rvm_return_t bad_region(rvm_region_t *rvm_region);
+char *page_alloc(rvm_length_t len);
+void page_free(char *vmaddr, rvm_length_t length);
+long open_seg_dev(seg_t *seg, rvm_offset_t *dev_length);
+long close_seg_dev(seg_t *seg);
+rvm_return_t close_all_segs(void);
+seg_t *seg_lookup(char *dev_name, rvm_return_t *retval);
+rvm_return_t define_seg(log_t *log, seg_t *seg);
+rvm_return_t define_all_segs(log_t *log);
+long dev_partial_include(rvm_offset_t *base1, rvm_offset_t *end1, rvm_offset_t *base2, rvm_offset_t *end2);
+long dev_total_include(rvm_offset_t *base1, rvm_offset_t *end1, rvm_offset_t *base2, rvm_offset_t *end2);
+long mem_partial_include(tree_node_t *tnode1, tree_node_t *tnode2);
+long mem_total_include(tree_node_t *tnode1, tree_node_t *tnode2);
+region_t *find_whole_range(char *dest, rvm_length_t length, rw_lock_mode_t mode);
+region_t *find_partial_range(char *dest, rvm_length_t length, long *code);
+rvm_return_t rvm_map(rvm_region_t *rvm_region, rvm_options_t *rvm_options);
+
+
+/* segment dictionary functions */
+extern
+rvm_return_t enter_seg_dict();          /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    long            seg_code;
+*/
+extern
+rvm_return_t def_seg_dict();            /* [rvm_logrecovr.c] */
+/*  log_t           *log;
+    rec_hdr_t       *rec_hdr;
+*/
+/* I/O functions */
+
+extern
+long open_dev();                        /* [rvm_io.c] */
+/*  device_t        *dev;
+    long            flags;
+    long            mode;
+*/
+extern
+long close_dev();                       /* [rvm_io.c] */
+/*  device_t        *dev; */
+
+extern
+long read_dev();                        /* [rvm_io.c] */
+/*  device_t        *dev;
+    rvm_offset_t    *offset;
+    char            *dest;
+    rvm_length_t    length;
+*/
+extern
+long write_dev();                       /* [rvm_io.c] */
+/*  device_t        *dev;
+    rvm_offset_t    *offset;
+    char            *src;
+    rvm_length_t    length;
+    rvm_bool_t      no_sync;
+*/
+extern
+long sync_dev();                        /* [rvm_io.c] */
+/*  device_t        *dev; */
+
+extern
+long gather_write_dev();                /* [rvm_io.c] */
+/*  device_t        *dev;
+    rvm_offset_t    *offset;
+    struct iovec    *iov;
+    rvm_length_t    iovcnt;
+*/
+
+/* length is optional [rvm_io.c] */
+extern long set_dev_char(device_t *dev,rvm_offset_t *dev_length);
+
+/* read/write lock */
+extern                                  /* [rvm_utils.c] */
+void rw_lock();
+/*  rw_lock_t       *rwl;
+    rw_lock_mode_t  mode;
+*/
+extern                                  /* [rvm_utils.c] */
+void rw_unlock();
+/*  rw_lock_t       *rwl;
+    rw_lock_mode_t  mode;
+*/
+extern                                  /* [rvm_utils.c] */
+void init_rw_lock();
+/*  rw_lock_t       *rwl; */
+
+extern
+void init_tree_root();                  /* [rvm_utils.c] */
+/*  tree_root_t     *root; */
+
+extern
+void clear_tree_root();                 /* [rvm_utils.c] */
+/*  tree_root_t     *root; */
+
+extern                                  /* [rvm_utils.c] */
+void rw_lock_clear();
+/*  rw_lock_t       *rwl; */
+/* Binary Tree Functions */
+
+extern
+tree_node_t *tree_lookup();             /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    tree_node_t     *node;
+    cmp_func_t      *cmp;
+*/
+extern
+rvm_bool_t tree_insert();               /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    tree_node_t     *node;
+    cmp_func_t      *cmp;
+*/
+extern
+rvm_bool_t tree_delete();               /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    tree_node_t     *node;
+    cmp_func_t      *cmp;
+*/
+extern
+tree_node_t *init_tree_generator();     /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    rvm_bool_t      direction;
+    rvm_bool_t      unlink;
+*/
+extern
+tree_node_t *tree_iterate_insert();     /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    tree_node_t     *node;
+    cmp_func_t      *cmp;
+*/
+extern
+tree_node_t *tree_successor();          /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    rvm_bool_t      direction;
+*/
+extern
+tree_node_t *tree_predecessor();        /* [rvm_utils.c] */
+/*  tree_root_t     *tree;
+    rvm_bool_t      direction;
+*/
+/* initialization, query, and structure checkers */
+
+extern rvm_bool_t bad_init(void);        /* [rvm_init.c] */
+
+/* [rvm_status.c] */
+rvm_return_t bad_options(rvm_options_t *rvm_options, rvm_bool_t chk_log_dev);
+rvm_return_t bad_statistics(rvm_statistics_t *rvm_statistics);
+
+extern
+rvm_return_t bad_region();              /* [rvm_map.c] */
+/*   rvm_region_t   *rvm_region; */
+
+extern
+rvm_return_t bad_tid();                 /* [rvm_trans.c] */
+/*   rvm_tid_t      *rvm_tid; */
+
+extern
+rvm_return_t do_rvm_options();          /* [rvm_status.c] */
+/*  rvm_options_t   *rvm_options; */
+/* make unique name */
+extern                                  /* [rvm_utils.c] */
+void make_uname();
+/*  struct timeval  *time; */
+extern                                  /* [rvm_utils.c] */
+long init_unames();
+
+/* time value arithmetic */
+extern
+struct timeval add_times();             /* [rvm_utils.c] */
+/*  struct timeval  *x;
+    struct timeval  *y;
+*/
+extern
+struct timeval sub_times();             /* [rvm_utils.c] */
+/*  struct timeval  *x;
+    struct timeval  *y;
+*/
+extern
+long round_time();                      /* [rvm_utils.c] */
+/*  struct timeval  *x; */
+
+/* statistics gathering functions */
+extern
+void enter_histogram();                 /* [rvm_utils] */
+/*  long            val;
+    long            *histo;
+    long            *histo_def;
+    long            length;
+*/
+
+/* various initializers */
+extern
+void init_map_roots();                  /* [rvm_map.c] */
+
+extern
+long init_utils();                      /* [rvm_utils.c] */
+
+/* check summing and byte-aligned copy and pad functions */
+extern
+rvm_length_t chk_sum();                 /* rvm_utils.c */
+/*  char            *nvaddr;
+    rvm_length_t    len;
+*/
+extern
+rvm_length_t zero_pad_word();           /* rvm_utils.c */
+/*  rvm_length_t    word;
+    char            *addr;
+    rvm_bool_t      leading;
+*/
+extern
+void src_aligned_bcopy();               /* rvm_utils.c */
+/*  char            *src;
+    char            *dest;
+    rvm_length_t    len;
+*/
+extern
+void dest_aligned_bcopy();               /* rvm_utils.c */
+/*  char            *src;
+    char            *dest;
+    rvm_length_t    len;
+*/
+
+/*  offset arithmetic */
+extern
+rvm_offset_t rvm_rnd_offset_to_sector(); /* [rvm_utils.c] */
+/*  rvm_offset_t    *x; */
+
+/* debug support */
+extern
+void rvm_debug();                       /* [rvm_debug] */
+/*  rvm_length_t    val; */
+
+
+#endif /* __MERO_RVM_RVM_PRIVATE_H__ */
diff --git a/rvm/rvm_pthread.c b/rvm/rvm_pthread.c
new file mode 100644
index 0000000..fdef818
--- /dev/null
+++ b/rvm/rvm_pthread.c
@@ -0,0 +1,32 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#include <pthread.h>
+#include "rvm/rvm.h"
+
+pthread_t       rvm_pthreadid;
+void		*rvm_ptstat;
+int             rvm_join_res;
+
+int rvm_lock_free (pthread_mutex_t *m)
+{
+    int result = pthread_mutex_trylock(m);
+    if (result == 0) pthread_mutex_unlock(m);
+    return (result == 0) ? rvm_true : rvm_false;
+}
+
diff --git a/rvm/rvm_pthread.h b/rvm/rvm_pthread.h
new file mode 100644
index 0000000..412b87f
--- /dev/null
+++ b/rvm/rvm_pthread.h
@@ -0,0 +1,157 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+#pragma once
+
+#ifndef __MERO_RVM_RVM_PTHREAD_H__
+#define __MERO_RVM_RVM_PTHREAD_H__
+
+/* pthread compatibility for RVM */
+
+#include <pthread.h>
+
+/* used in pthread_create */
+extern pthread_t rvm_pthreadid;
+
+/* used in pthread_join */
+extern void *rvm_ptstat;
+extern int       rvm_join_res;
+
+#ifndef MACRO_BEGIN
+#define MACRO_BEGIN                 do {
+#define MACRO_END                   } while (0)
+#endif  /* MACRO_BEGIN */
+
+#define BOGUSCODE     (BOGUS_USE_OF_PTHREADS)   /* force compilation error */
+
+#define RVM_MUTEX          pthread_mutex_t
+#define RVM_CONDITION      pthread_cond_t
+
+/*
+ * Unfortunately, pthread mutexes cannot be initialized statically: they
+ * must be initialized by a call to pthread_mutex_init.  Oh well.
+ * This means that some locking situations won't work properly.
+ * I'll define MUTEX_INITIALIZER to be BOGUSCODE to make this more
+ * explicit to pthreads clients.
+ */
+
+/* That's nonsense, the following is from pthread_mutex(3):
+ *
+ *    Variables of type pthread_mutex_t can also be initialized statically
+ *    using the constants PTHREAD_MUTEX_INITIALIZER (for fast mutexes),
+ *    PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP (for recursive mutexes), and
+ *    PTHREADS_ERRORCHECK_MUTEX_INITIALIZER_MP (for error checking mutexes).
+ *
+ * --JH
+ */
+
+#define MUTEX_INITIALIZER  PTHREAD_MUTEX_INITIALIZER
+
+/* Supported cthread definitions: */
+
+#define cthread_t                    pthread_t *
+
+#define cthread_fork(fname, arg)     (pthread_create(&rvm_pthreadid, NULL, \
+						     (void *(*)(void*))(fname),\
+						     (arg)), \
+				      &rvm_pthreadid)
+
+/*
+ * Returns either NULL or the address of the pthread_status block.
+ * Unfortunately, it appears that chtread_join didn't have a way of
+ * saying "badthread, can't do it," so I'm not sure of the best way to
+ * do this.
+ */
+#define cthread_join(t)              (rvm_join_res =                      \
+                                         pthread_join(*(t),&rvm_ptstat),  \
+                                      (rvm_join_res) ? NULL : rvm_ptstat)
+
+#define cthread_init()               do {} while(0)
+
+#define cthread_exit(retval)         (pthread_exit((void *)(retval)))
+
+#define cthread_yield()              do {} while(0)
+
+#define condition_wait(c,m)          (pthread_cond_wait((c),(m)))
+
+#define condition_signal(c)          (pthread_cond_signal((c)))
+
+#define condition_broadcast(c)       (pthread_cond_broadcast((c)))
+
+/* This is defined just as in rvm_lwp.h, but is almost surely a bug */
+#define condition_clear(c)           /* nop */
+
+#define condition_init(c)            (pthread_cond_init((c), NULL))
+
+#define mutex_init(m)                (pthread_mutex_init((m), NULL))
+#define mutex_clear(m)               (pthread_mutex_destroy(m), NULL)
+
+/* This doesn't work for some reason... */
+/*
+#define LOCK_FREE(m)         (rvm_ptlocked = pthread_mutex_trylock(&(m)),     \
+                              if (rvm_ptlocked) {pthread_mutex_unlock(&(m))}, \
+			      rvm_ptlocked)
+*/
+/* defined in rvm_pthread.c */
+extern int rvm_lock_free(pthread_mutex_t *m);
+#define LOCK_FREE(m)         (rvm_lock_free(&(m)))
+
+#define cthread_self()       (rvm_pthreadid = pthread_self(), &rvm_pthreadid)
+
+#ifdef  DEBUGRVM
+#define mutex_lock(m)        MACRO_BEGIN                             \
+                               printf("mutex_lock OL(0x%x)%s:%d...", \
+                                      (m), __FILE__, __LINE__);      \
+                               pthread_mutex_lock((m));              \
+                               printf("done\n");                     \
+                             MACRO_END
+#define mutex_unlock(m)      MACRO_BEGIN                               \
+                               printf("mutex_unlock RL(0x%x)%s:%d...", \
+                                       (m), __FILE__, __LINE__);       \
+                               pthread_mutex_unlock((m));              \
+                               printf("done\n");                       \
+                             MACRO_END
+#else   /* DEBUGRVM */
+#define mutex_lock(m)        (pthread_mutex_lock((m)))
+#define mutex_unlock(m)      (pthread_mutex_unlock((m)))
+#endif  /* DEBUGRVM */
+
+
+/* Unsupported cthread calls */
+
+#define	mutex_alloc()			BOGUSCODE
+#define	mutex_set_name(m, x)		BOGUSCODE
+#define	mutex_name(m)			BOGUSCODE
+#define	mutex_free(m)			BOGUSCODE
+
+#define	condition_alloc()		BOGUSCODE
+#define	condition_set_name(c, x)	BOGUSCODE
+#define	condition_name(c)		BOGUSCODE
+#define	condition_free(c)		BOGUSCODE
+
+#define cthread_detach()		BOGUSCODE
+#define cthread_sp()			BOGUSCODE
+#define	cthread_assoc(id, t)		BOGUSCODE
+#define cthread_set_name		BOGUSCODE
+#define cthread_name			BOGUSCODE
+#define cthread_count()			BOGUSCODE
+#define cthread_set_limit		BOGUSCODE
+#define cthread_limit()			BOGUSCODE
+#define	cthread_set_data(t, x)		BOGUSCODE
+#define	cthread_data(t)			BOGUSCODE
+
+#endif /*__MERO_RVM_RVM_PTHREAD_H__ */
diff --git a/rvm/rvm_releaseseg.c b/rvm/rvm_releaseseg.c
new file mode 100644
index 0000000..2d5c580
--- /dev/null
+++ b/rvm/rvm_releaseseg.c
@@ -0,0 +1,51 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#include <stdio.h>
+#include <stdlib.h>
+
+#include "rvm/rvm.h"
+#include "rvm/rvm_segment.h"
+#include "rvm/rvm_segment_private.h"
+
+/* release regions of a segment */
+rvm_return_t
+rvm_release_segment (
+    unsigned long       nregions,     /* number of regions mapped */
+    rvm_region_def_t    **regions)    /* array of region descriptors */
+{
+    rvm_region_t *region = rvm_malloc_region();
+    rvm_return_t err = RVM_SUCCESS;
+    int i;
+
+    for (i = 0; i < nregions; i++) {
+        region->offset = (*regions)[i].offset;
+        region->length = (*regions)[i].length;
+        region->vmaddr = (*regions)[i].vmaddr;
+
+        err = rvm_unmap(region);
+        if (err != RVM_SUCCESS)
+            printf("release_segment unmap failed %s\n", rvm_return(err));
+
+        deallocate_vm(region->vmaddr, region->length);
+    }
+    rvm_free_region(region);
+    free(*regions);
+    return err;
+}
+
diff --git a/rvm/rvm_segment.h b/rvm/rvm_segment.h
new file mode 100644
index 0000000..ea2723d
--- /dev/null
+++ b/rvm/rvm_segment.h
@@ -0,0 +1,74 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+ * Segment Loader public definitions
+ */
+#pragma once
+
+#ifndef __MERO_RVM_RVM_SEGMENT_H__
+#define __MERO_RVM_RVM_SEGMENT_H__
+
+#include "rvm/rvm.h"
+
+/* taken from rvm_segment_private.h */
+#define RVM_SEGMENT_HDR_SIZE RVM_PAGE_SIZE /* length of segment header */
+
+/* region definition descriptor */
+typedef struct
+    {
+    rvm_offset_t        offset;         /* region's offset in segment */
+    rvm_length_t        length;         /* region length */
+    char                *vmaddr;        /* mapping address for region */
+    }
+rvm_region_def_t;
+
+/* initializer for region definition descriptor */
+#define RVM_INIT_REGION(region,off,len,addr) \
+    (region).length = (len); \
+    (region).vmaddr = (addr); \
+    (region).offset = (off);
+
+/* error code for damaged segment header */
+#define RVM_ESEGMENT_HDR 2000
+
+/* define regions within a segment for segement loader */
+extern rvm_return_t rvm_create_segment (
+    char                *DevName,       /* pointer to data device name */
+    rvm_offset_t        DevLength,      /* Length of dataDev if really a device */
+    rvm_options_t       *options,       /* options record for RVM */
+    rvm_length_t        nregions,       /* number of regions defined for segment*/
+    rvm_region_def_t    *region_defs    /* array of region defs for segment */
+    );
+
+/* load regions of a segment */
+extern rvm_return_t rvm_load_segment (
+    char                *DevName,       /* pointer to data device name */
+    rvm_offset_t        DevLength,      /* Length of dataDev if really a device */
+    rvm_options_t       *options,       /* options record for RVM */
+    unsigned long       *nregions,      /* returned -- number of regions mapped */
+    rvm_region_def_t    *regions[]      /* returned array of region descriptors */
+    );
+
+/* release regions of a segment */
+extern rvm_return_t rvm_release_segment (
+    unsigned long       nregions,      /* number of regions mapped */
+    rvm_region_def_t    **regions      /* array of region descriptors */
+    );
+
+#endif /* __MERO_RVM_RVM_SEGMENT_H__ */
diff --git a/rvm/rvm_segment_private.h b/rvm/rvm_segment_private.h
new file mode 100644
index 0000000..5daa005
--- /dev/null
+++ b/rvm/rvm_segment_private.h
@@ -0,0 +1,58 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+/* segment loader private declarations */
+
+#pragma once
+
+#ifndef __MERO_RVM_RVM_SEGMENT_PRIVATE_H__
+#define __MERO_RVM_RVM_SEGMENT_PRIVATE_H__
+/* Worker definitions */
+
+extern rvm_return_t allocate_vm();
+extern rvm_return_t deallocate_vm();
+
+extern int overlap();
+
+/* Macro definitions for the segment header */
+
+/* The follwing version string must be changed if any change is made in
+   the segment header (rvm_segment_hdr_t)
+*/
+#define RVM_SEGMENT_VERSION "RVM Segment Loader Release 0.1  15 Nov. 1990"
+
+/* Moved this to rvm_segment.h, as rdsinit needs it. */
+/* #define RVM_SEGMENT_HDR_SIZE RVM_PAGE_SIZE   * length of segment header */
+#define RVM_MAX_REGIONS                 /* maximum regions in seg  hdr */ \
+        ((RVM_SEGMENT_HDR_SIZE/sizeof(rvm_region_def_t))-1)
+
+typedef enum { rvm_segment_hdr_id = 1 } rvm_seg_struct_id_t;
+
+/* segment header: rvm_segment_hdr_t */
+typedef struct
+    {
+    rvm_seg_struct_id_t     struct_id;      /* self-identifier, do not change */
+
+    char                version[RVM_VERSION_MAX]; /* version string */
+    rvm_length_t        nregions;                 /* number of regions defined */
+    rvm_region_def_t    regions[1];     /* region definition array  -- length
+                                           actually determined by
+                                           RVM_MAX_REGIONS */
+    }
+rvm_segment_hdr_t;
+
+#endif /* __MERO_RVM_RVM_SEGMENT_PRIVATE_H__ */
diff --git a/rvm/rvm_segutil.c b/rvm/rvm_segutil.c
new file mode 100644
index 0000000..a021a8e
--- /dev/null
+++ b/rvm/rvm_segutil.c
@@ -0,0 +1,170 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <unistd.h>
+#include <stdlib.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
+#include "rvm/rvm.h"
+#include "rvm/rvm_segment.h"
+
+/* from rvm_private.h */
+rvm_bool_t rvm_register_page(char *vmaddr, rvm_length_t length);
+rvm_bool_t rvm_unregister_page(char *vmaddr, rvm_length_t length);
+
+#ifdef __CYGWIN32__
+#include <windows.h>
+#endif
+
+/* Routine to check if regions will overlap in memory. */
+
+int overlap(nregions, regionDefs)
+     unsigned long      nregions;
+     rvm_region_def_t   regionDefs[];
+{
+    int i,j;
+    rvm_region_def_t temp;
+
+    /* sort array */
+    for (i = 0; i < (nregions - 1); i++) {
+	for (j = i + 1; j < nregions; j++) {
+	    if (regionDefs[j].vmaddr < regionDefs[i].vmaddr) {
+		temp.vmaddr = regionDefs[i].vmaddr;
+		temp.length = regionDefs[i].length;
+		temp.offset = regionDefs[i].offset;
+
+		regionDefs[i].vmaddr = regionDefs[j].vmaddr;
+		regionDefs[i].length = regionDefs[j].length;
+		regionDefs[i].offset = regionDefs[j].offset;
+
+		regionDefs[j].vmaddr = temp.vmaddr;
+		regionDefs[j].length = temp.length;
+		regionDefs[j].offset = temp.offset;
+	    }
+	}
+    }
+
+    for (i = 0; i < (nregions - 1); i++) {
+	if (regionDefs[i].vmaddr + regionDefs[i].length > regionDefs[i+1].vmaddr)
+	    return(TRUE);
+    }
+
+    return FALSE;
+}
+
+
+/* BSD44 memory allocation; uses mmap as an allocator.  Any mmap-aware
+   system should be able to use this code */
+
+#include <sys/types.h>
+#include <sys/mman.h>
+#include "rvm/coda_mmap_anon.h"
+#include <errno.h>
+#define ALLOCATE_VM_DEFINED
+
+
+rvm_return_t
+allocate_vm(addr, length)
+     char **addr;
+     unsigned long length;
+{
+    rvm_return_t  ret = RVM_SUCCESS;
+    char *requested_addr = *addr; /* save this so that we can check it
+				     against the address location
+				     returned by mmap. this is
+				     important because if it isn't 0,
+				     it's a location that we HAVE to
+				     be able to map to. */
+
+#ifdef HAVE_MMAP
+    mmap_anon(*addr, *addr, length, (PROT_READ | PROT_WRITE));
+#else
+    {
+      HANDLE hMap = CreateFileMapping((HANDLE)0xFFFFFFFF, NULL,
+                                      PAGE_READWRITE, 0, length, NULL);
+      if (hMap == NULL)
+          return(RVM_EINTERNAL);
+      *addr = MapViewOfFileEx(hMap, FILE_MAP_READ | FILE_MAP_WRITE, 0, 0, 0, *addr);
+      if (*addr == NULL) {
+#if 0
+	  DWORD errnum;
+	  errnum = GetLastError();
+	  printf ("allocate_vm: errnum = %d\n", errnum);
+#endif
+          *addr = (char *)-1;
+      }
+      CloseHandle(hMap);
+    }
+#endif
+
+    if (*addr == (char*)-1) {
+	if (errno == ENOMEM) {
+	    ret = RVM_ENO_MEMORY;
+	} else {
+	    ret = RVM_EINTERNAL;
+	}
+    }
+
+    if (requested_addr != 0 && *addr != requested_addr) {
+	ret = RVM_EINTERNAL;	/* couldn't allocated requested memory. */
+    }
+
+    /* modified by tilt, Nov 19 1996.
+       When we allocate a page (or range of pages) we register
+       it in an internal table we're keeping around to keep
+       track of pages. (The previous solution was to try to
+       re-allocate the page, and see if it fails, which is
+       not only wrong [since we don't if it's allocated, or
+       actually allocated in the RVM heap!!], but doesn't
+       work with mmap()). */
+    if (rvm_register_page(*addr, length) == rvm_false) {
+	ret = RVM_EINTERNAL;
+    }
+
+    return ret;
+}
+
+rvm_return_t
+deallocate_vm(addr, length)
+     char *addr;
+     unsigned long length;
+{
+    rvm_return_t   ret = RVM_SUCCESS;
+
+#ifdef HAVE_MMAP
+    if (munmap(addr, length)) {
+	ret = RVM_EINTERNAL;
+    }
+#else
+    UnmapViewOfFile(addr);
+#endif
+
+    if (rvm_unregister_page(addr, length) == rvm_false) {
+	ret = RVM_EINTERNAL;
+    }
+
+    return ret;
+}
+
+
diff --git a/rvm/rvm_statistics.h b/rvm/rvm_statistics.h
new file mode 100644
index 0000000..39f0bd5
--- /dev/null
+++ b/rvm/rvm_statistics.h
@@ -0,0 +1,201 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                 Definitions for RVM Statistics Structures
+*
+*/
+/*LINTLIBRARY*/
+#pragma once
+
+/* permit multiple includes */
+#ifndef __MERO_RVM_RVM_STATISTICS_H__
+#define __MERO_RVM_RVM_STATISTICS_H__
+
+#define RVM_STATISTICS_VERSION  "RVM Statistics Version 1.1 8 Dec 1992"
+
+#include <stdio.h>
+
+/* histgram definitions */
+
+#define flush_times_len         10      /* length of flush timing vectors */
+#define flush_times_dist                /* timing distribution in millisecs */ \
+    25,50,100,250,500,1000,2500,5000,10000 /* use as array initializer */
+
+#define truncation_times_len    5       /* length of truncation timing vectors */
+#define truncation_times_dist           /* timing distribution in seconds */ \
+    1,10,100,500
+
+#define range_lengths_len       13      /* length of range length histogram */
+#define range_lengths_dist              /* range lengths in bytes */ \
+    0,4,8,16,32,64,128,256,512,1024,2048,4096
+
+#define range_overlaps_len      13      /* length of overlaps eliminated by
+                                           range coalesce */
+#define range_overlaps_dist             /* range lengths in bytes */ \
+    0,4,8,16,32,64,128,256,512,1024,2048,4096
+
+#define trans_overlaps_len      13      /* length of overlaps eliminated by
+                                           transaction coalesce */
+#define trans_overlaps_dist             /* range lengths in bytes */ \
+    0,4,8,16,32,64,128,256,512,1024,2048,4096
+
+#define range_elims_len          6      /* ranges eliminated from log by
+                                           range coalesce */
+#define range_elims_dist                /* number of ranges */ \
+    0,5,10,50,100
+
+#define trans_elims_len          6      /* ranges eliminated from log by
+                                           trans coalesce */
+#define trans_elims_dist                /* number of ranges */ \
+    0,5,10,50,100
+
+#define trans_coalesces_len     6       /* transactions coalesced by
+                                           trans coalesce */
+#define trans_coalesces_dist            /* number of transactions */ \
+    0,5,10,50,100
+/* RVM statistics record */
+typedef struct
+    {
+    rvm_struct_id_t struct_id;          /* self-identifier, do not change */
+    rvm_bool_t      from_heap;          /* true if heap allocated;
+					   do not change */
+
+                                        /* transaction statistics --
+					   current epoch */
+
+    rvm_length_t    n_abort;            /* number of transactions aborted */
+    rvm_length_t    n_flush_commit;     /* number of flush mode commits */
+    rvm_length_t    n_no_flush_commit;  /* number of no_flush mode commits */
+    rvm_length_t    n_uncommit;         /* number of uncommited transactions */
+    rvm_length_t    n_no_flush;         /* number of queued no_flush transactions */
+    rvm_length_t    n_truncation_wait;  /* total transactions delayed by truncation */
+    rvm_offset_t    no_flush_length;    /* length of queued no_flush transactions */
+                                        /* log statistics -- current epoch */
+
+    rvm_length_t    n_split;            /* number trans split for log wrap */
+    rvm_length_t    n_flush;            /* number of internal flushes */
+    rvm_length_t    n_rvm_flush;        /* number of explicit flush calls */
+    rvm_length_t    n_special;          /* number of special log records */
+    rvm_length_t    n_wrap;             /* number of log wrap-arounds (0 or 1) */
+    rvm_length_t    log_dev_cur;        /* current % log device in use */
+    rvm_offset_t    log_written;        /* current length of writes to log */
+    rvm_offset_t    range_overlap;      /* current overlap eliminated by range coalesce */
+    rvm_offset_t    trans_overlap;      /* current overlap eliminated by trans coalesce */
+    rvm_length_t    n_range_elim;       /* current number of ranges eliminated by
+                                           range coalesce/flush */
+    rvm_length_t    n_trans_elim;       /* current number of ranges eliminated by
+                                           trans coalesce/flush */
+    rvm_length_t    n_trans_coalesced;  /* number of transactions coalesced in
+                                           this flush cycle */
+    struct timeval  flush_time;         /* time spent in flushes */
+    rvm_length_t    last_flush_time;    /* duration of last flush (msec) */
+    rvm_length_t    last_truncation_time; /* duration of last truncation (sec) */
+    rvm_length_t    last_tree_build_time; /* duration of tree build (sec) */
+    rvm_length_t    last_tree_apply_time; /* duration of tree apply phase
+                                             (sec) */
+                                        /* histogram vectors */
+
+    rvm_length_t    flush_times[flush_times_len]; /* flush timings (msec) */
+    rvm_length_t    range_lengths[range_lengths_len]; /* range lengths flushed */
+    rvm_length_t    range_elims[range_elims_len]; /* num ranges eliminated by
+                                                     range coalesce/flush */
+    rvm_length_t    trans_elims[trans_elims_len]; /* num ranges eliminated by
+                                                     trans coalesce/flush */
+    rvm_length_t    range_overlaps[range_overlaps_len]; /* space saved by
+                                                           range coalesce/flush */
+    rvm_length_t    trans_overlaps[range_overlaps_len]; /* space saved by
+                                                           trans coalesce/flush */
+                                        /* transaction stats -- cumulative since log init */
+
+    rvm_length_t    tot_abort;          /* total aborted transactions */
+    rvm_length_t    tot_flush_commit;   /* total flush commits */
+    rvm_length_t    tot_no_flush_commit; /* total no_flush commits */
+
+                                        /* log stats -- cumulative */
+
+    rvm_length_t    tot_split;          /* total transactions split for log wrap-around */
+    rvm_length_t    tot_flush;          /* total internal flush calls  */
+    rvm_length_t    tot_rvm_flush;      /* total explicit rvm_flush calls  */
+    rvm_length_t    tot_special;        /* total special log records */
+    rvm_length_t    tot_wrap;           /* total log wrap-arounds */
+    rvm_length_t    log_dev_max;        /* maximum % log device used so far */
+    rvm_offset_t    tot_log_written;    /* total length of all writes to log */
+    rvm_offset_t    tot_range_overlap;  /* total overlap eliminated by range coalesce */
+    rvm_offset_t    tot_trans_overlap;  /* total overlap eliminated by trans coalesce */
+    rvm_length_t    tot_range_elim;     /* total number of ranges eliminated by
+                                           range coalesce */
+    rvm_length_t    tot_trans_elim;     /* total number of ranges eliminated by
+                                           trans coalesce */
+    rvm_length_t    tot_trans_coalesced; /* total number of transactions coalesced */
+
+                                        /* truncation stats -- cummulative */
+
+    rvm_length_t    tot_rvm_truncate;   /* total explicit rvm_truncate calls */
+    rvm_length_t    tot_async_truncation; /* total asynchronous truncations */
+    rvm_length_t    tot_sync_truncation; /* total forced synchronous truncations */
+    rvm_length_t    tot_truncation_wait; /* total transactions delayed by truncation */
+    rvm_length_t    tot_recovery;       /* total recovery truncations */
+    struct timeval  tot_flush_time;     /* total time spent in flush */
+    struct timeval  tot_truncation_time; /* cumulative truncation time */
+
+                                        /* histogram vectors */
+                                        /* truncation timings (sec) */
+    rvm_length_t    tot_tree_build_times[truncation_times_len];
+    rvm_length_t    tot_tree_apply_times[truncation_times_len];
+    rvm_length_t    tot_truncation_times[truncation_times_len];
+                                        /* cummulative flush timings (msec) */
+    rvm_length_t    tot_flush_times[flush_times_len];
+                                        /* cummulative range lengths */
+    rvm_length_t    tot_range_lengths[range_lengths_len];
+                                        /* total num ranges eliminated by
+                                           range coalesce/flush */
+    rvm_length_t    tot_range_elims[range_elims_len];
+                                        /* total num ranges eliminated by
+                                           trans coalesce/flush */
+    rvm_length_t    tot_trans_elims[trans_elims_len];
+                                        /* space saved by range coalesce/flush */
+    rvm_length_t    tot_range_overlaps[range_overlaps_len];
+                                        /* space saved by trans coalesce/flush */
+    rvm_length_t    tot_trans_overlaps[range_overlaps_len];
+                                        /* transactions coalesced per flush  */
+    rvm_length_t    tot_trans_coalesces[trans_coalesces_len];
+    }
+rvm_statistics_t;
+/* get RVM statistics */
+rvm_return_t rvm_statistics(const char *version, rvm_statistics_t *statistics);
+#define RVM_STATISTICS(statistics) \
+    rvm_statistics(RVM_STATISTICS_VERSION,(statistics))
+
+/* rvm_statistics_t initializer, copier & finalizer */
+
+extern rvm_statistics_t *rvm_malloc_statistics ();
+
+extern void rvm_init_statistics(rvm_statistics_t *statistics);
+extern rvm_statistics_t *rvm_copy_statistics(rvm_statistics_t *statistics);
+extern void rvm_free_statistics(rvm_statistics_t *statistics);
+
+/* rvm_statistics_t printer */
+extern rvm_return_t rvm_print_statistics(
+    rvm_statistics_t    *statistics,    /* pointer to record to be printed */
+    FILE                *out_stream     /* output stream */
+    );
+
+
+#endif /* __MERO_RVM_RVM_STATISTICS_H__ */
diff --git a/rvm/rvm_status.c b/rvm/rvm_status.c
new file mode 100644
index 0000000..757d565
--- /dev/null
+++ b/rvm/rvm_status.c
@@ -0,0 +1,403 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+#*/
+
+/*
+*
+*                   RVM Global Options
+*
+*/
+
+#include "rvm_private.h"
+
+/* global variables */
+extern log_t        *default_log;       /* default log descriptor ptr */
+extern rvm_bool_t   rvm_utlsw;          /* true if call by rvmutl */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+extern rvm_length_t rvm_max_read_len;   /* maximum Mach read length */
+extern rvm_length_t flush_times_vec[flush_times_len]; /* flush timing histogram defs */
+extern rvm_length_t truncation_times_vec[truncation_times_len]; /* truncation timing 
+                                                                   histogram defs */
+extern rvm_length_t range_lengths_vec[range_lengths_len]; /* range length
+                                                             histogram defs */
+extern rvm_length_t range_overlaps_vec[range_overlaps_len]; /* range coalesce
+                                                             histogram defs */
+extern rvm_length_t trans_overlaps_vec[trans_overlaps_len]; /* trans coalesce
+                                                             histogram defs */
+extern rvm_length_t range_elims_vec[range_elims_len]; /* ranges eliminated by range
+                                                         coalesce histogram defs */
+extern rvm_length_t trans_elims_vec[trans_elims_len]; /* ranges eliminated by trans
+                                                         coalesce histogram defs */
+extern rvm_length_t trans_coalesces_vec[trans_coalesces_len]; /* transactions
+                                                                 coalesed per flush */
+
+rvm_length_t        rvm_optimizations = 0;  /* optimizations switches */
+
+rvm_bool_t	    rvm_map_private = 0;  /* Do we map private or not. */
+
+/* version strings */
+char                rvm_version[RVM_VERSION_MAX] =
+                        {RVM_VERSION};
+char                rvm_log_version[RVM_VERSION_MAX] =
+                        {RVM_LOG_VERSION};
+char                rvm_statistics_version[RVM_VERSION_MAX] =
+                        {RVM_STATISTICS_VERSION};
+char                rvm_release[RVM_VERSION_MAX] =
+                        {"RVM Release 11 Jan 1993"};
+
+/* local macros */
+#define TID_ARRAY_REALLOC_INCR  5       /* allocate tid_array 5 elements at a
+                                           time */
+/* rvm_set_options */
+rvm_return_t rvm_set_options(rvm_options)
+    rvm_options_t   *rvm_options;
+    {
+    rvm_return_t    retval;
+
+    /* be sure RVM is initialized */
+    if (bad_init()) return RVM_EINIT;
+
+    /* must have an options record here */
+    if ((retval=bad_options(rvm_options, rvm_true)) != RVM_SUCCESS)
+        return retval;                  /* bad options ptr or record */
+    if (rvm_options == NULL)
+        return RVM_EOPTIONS;
+
+    /* check validity of options record & ptr */
+    return do_rvm_options(rvm_options);
+
+    }
+/* structure validation */
+rvm_return_t bad_statistics(rvm_statistics)
+    rvm_statistics_t   *rvm_statistics;
+    {
+    if (rvm_statistics == NULL)
+        return RVM_SUCCESS;
+    if (rvm_statistics->struct_id != rvm_statistics_id)
+        return RVM_ESTATISTICS;
+
+    return RVM_SUCCESS;
+    }
+
+rvm_return_t bad_options(rvm_options,chk_log_dev)
+    rvm_options_t   *rvm_options;
+    rvm_bool_t      chk_log_dev;
+    {
+    if (rvm_options == NULL)
+        return RVM_SUCCESS;
+    if (rvm_options->struct_id != rvm_options_id)
+        return RVM_EOPTIONS;
+
+    if (chk_log_dev && (rvm_options->log_dev != NULL))
+        if (strlen(rvm_options->log_dev) > (MAXPATHLEN-1))
+            return RVM_ENAME_TOO_LONG;
+
+    return RVM_SUCCESS;
+    }
+/* rvm options processing */
+rvm_return_t do_rvm_options(rvm_options)
+    rvm_options_t   *rvm_options;
+    {
+    log_t           *log;               /* log descriptor */
+    rvm_return_t    retval;
+
+    if (rvm_options != NULL)
+        {
+        /* set up maximum read length for large transafers */
+        rvm_options->max_read_len =
+            CHOP_TO_SECTOR_SIZE(rvm_options->max_read_len);
+        if (rvm_options->max_read_len < SECTOR_SIZE)
+            rvm_options->max_read_len = MAX_READ_LEN;
+        rvm_max_read_len = rvm_options->max_read_len;
+
+        /* do log - modifying options */
+        if ((retval=do_log_options(&log,rvm_options)) != RVM_SUCCESS)
+            return retval;
+
+        /* set optimizations */
+        rvm_optimizations = rvm_options->flags & (RVM_ALL_OPTIMIZATIONS);
+        if (rvm_optimizations & RVM_COALESCE_TRANS)
+            rvm_optimizations |= RVM_COALESCE_RANGES;
+	
+	/* set mapping kind */
+	rvm_map_private = rvm_options->flags & RVM_MAP_PRIVATE;
+        }
+
+
+    return RVM_SUCCESS;
+    }
+/* rvm_query */
+rvm_return_t rvm_query(rvm_options,rvm_region)
+    rvm_options_t       *rvm_options;
+    rvm_region_t        *rvm_region;
+    {
+    log_t               *log;           /* log descriptor */
+    log_status_t        *status;        /* log status area descriptor */
+    int_tid_t           *tid;           /* transaction descriptor */
+    region_t            *region=NULL;   /* mapped region descriptor */
+    range_t             *range;         /* tid modification range */
+    rvm_length_t        n_tids = 0;     /* number of tids found */
+    rvm_bool_t          copy_tid = rvm_false; /* copy tid if true */
+    rvm_return_t        retval;
+
+    /* be sure RVM is initialized */
+    if (bad_init()) return RVM_EINIT;
+
+    /* check validity of region record & ptr */
+    if (rvm_region != NULL)
+        if (bad_region(rvm_region))
+            return RVM_EREGION;
+
+    /* check validity of options record */
+    if (rvm_options == NULL) return RVM_EOPTIONS;
+    if ((retval=bad_options(rvm_options,rvm_false)) != RVM_SUCCESS)
+        return retval;
+
+    /* set fields for log options */
+    if (default_log != NULL)
+        {
+        log = default_log;
+        status = &default_log->status;
+
+        /* set log device name if buffer supplied */
+        if (rvm_options->log_dev != NULL)
+            (void)strcpy(rvm_options->log_dev,log->dev.name);
+        rvm_options->truncate = log->daemon.truncate;
+        rvm_options->recovery_buf_len = log->log_buf.length;
+        rvm_options->flush_buf_len = log->dev.wrt_buf_len;
+
+        /* log truncation fields */
+        CRITICAL(log->dev_lock,         /* begin dev_lock crit sec */
+            {
+            rvm_options->log_empty = rvm_false;
+            if (RVM_OFFSET_EQL_ZERO(status->prev_log_head))
+                if (RVM_OFFSET_EQL(status->log_head,status->log_tail))
+                    rvm_options->log_empty = rvm_true;
+            });                         /* end dev_lock crit sec */
+        /* if region specified, look it up */
+        if (rvm_region != NULL)
+            {                           /* begin region_lock crit sect */
+            region = find_whole_range(rvm_region->vmaddr,
+                                      rvm_region->length,r);
+            if (region == NULL)
+                return RVM_ENOT_MAPPED; /* not locked if not found
+                                                       */
+            }
+
+        /* count uncommitted transactions */
+        CRITICAL(log->tid_list_lock,    /* begin log tid list crit sec */
+            {
+            FOR_ENTRIES_OF(log->tid_list,int_tid_t,tid)
+                {
+                if (rvm_region == NULL)
+                    copy_tid = rvm_true;
+                else
+                    {
+                    /* see if tid modifies specified region */
+                    copy_tid = rvm_false;
+                    RW_CRITICAL(tid->tid_lock,r, /* begin tid lock crit sec */
+                        {
+                        FOR_NODES_OF(tid->range_tree,range_t,range)
+                            if (range->region == region)
+                                {
+                                copy_tid = rvm_true;
+                                break;
+                                }
+                        });             /* end tid lock crit sec */
+                    }
+                /* copy uncommitted tid descriptions to uncommitted tid array */
+                if (copy_tid)
+                    {
+                    rvm_options->n_uncommit++;
+                    if (n_tids < rvm_options->n_uncommit)
+                        {
+                        /* reallocate tid_array */
+                        n_tids += TID_ARRAY_REALLOC_INCR;
+                        rvm_options->tid_array = (rvm_tid_t *)
+                            REALLOC(rvm_options->tid_array,
+                                    n_tids*sizeof(rvm_tid_t));
+                        if (rvm_options->tid_array == NULL)
+                            {
+                            retval = RVM_ENO_MEMORY;
+                            goto err_exit;
+                            }
+                        }
+
+                    /* copy tid uname */
+                    rvm_init_tid(&rvm_options->tid_array[
+                                   rvm_options->n_uncommit-1]);
+                    rvm_options->tid_array[rvm_options->
+                                           n_uncommit-1].uname
+                                               = tid->uname;
+                    rvm_options->tid_array[rvm_options->
+                                           n_uncommit-1].tid
+                                               = tid;
+                    }
+                }
+err_exit:;
+            });                         /* end log tid list crit sec */
+
+        if (rvm_region != NULL)
+            rw_unlock(&region->region_lock,r); /* end region_lock crit sect */
+        }
+
+    /* return non-log options */
+    rvm_options->flags = rvm_optimizations | rvm_map_private;
+    rvm_options->max_read_len = rvm_max_read_len;
+
+    return retval;
+    }
+/* rvm_statistics */
+rvm_return_t rvm_statistics(const char *version, rvm_statistics_t *rvm_statistics)
+{
+    log_t               *log;           /* log descriptor */
+    log_status_t        *status;        /* log status area descriptor */
+    int_tid_t           *tid;           /* transaction ptr */
+    rvm_length_t        i;
+    rvm_return_t        retval;
+
+    /* be sure RVM is initialized */
+    if (bad_init()) return RVM_EINIT;
+
+    /* check validity of statistics version and record */
+    if (strcmp(version, RVM_STATISTICS_VERSION))
+        return RVM_ESTAT_VERSION_SKEW;
+    if (rvm_statistics == NULL)
+        return RVM_ESTATISTICS;
+    if ((retval=bad_statistics(rvm_statistics)) != RVM_SUCCESS)
+        return retval;
+
+    /* check log */
+    if (default_log == NULL)
+        return RVM_ELOG;
+    log = default_log;
+    status = &log->status;
+    /* copy log and transaction statistics from log status area */
+    rvm_statistics->log_dev_cur = cur_log_percent(log,NULL);
+    CRITICAL(log->dev_lock,             /* begin dev_lock crit sec */
+        {
+        rvm_statistics->n_abort = status->n_abort;
+        rvm_statistics->n_flush_commit = status->n_flush_commit;
+        rvm_statistics->n_no_flush_commit = 
+            status->n_no_flush_commit;
+        rvm_statistics->n_split = status->n_split;
+        rvm_statistics->n_flush = status->n_flush;
+        rvm_statistics->n_rvm_flush = status->n_rvm_flush;
+        rvm_statistics->n_special = status->n_special;
+        rvm_statistics->n_wrap = 0;
+        if (RVM_OFFSET_GTR(status->log_head,status->log_tail))
+            rvm_statistics->n_wrap = 1;
+        rvm_statistics->tot_abort = status->tot_abort;
+        rvm_statistics->tot_flush_commit = status->tot_flush_commit;
+        rvm_statistics->tot_no_flush_commit =
+            status->tot_no_flush_commit;
+        rvm_statistics->tot_split = status->tot_split;
+        rvm_statistics->tot_rvm_truncate =
+            status->tot_rvm_truncate;
+        rvm_statistics->tot_async_truncation =
+            status->tot_async_truncation;
+        rvm_statistics->tot_sync_truncation =
+            status->tot_sync_truncation;
+        rvm_statistics->tot_truncation_wait = 
+            status->tot_truncation_wait;
+        rvm_statistics->tot_recovery = status->tot_recovery;
+        rvm_statistics->tot_flush = status->tot_flush;
+        rvm_statistics->tot_rvm_flush = status->tot_rvm_flush;
+        rvm_statistics->tot_special = status->tot_special;
+        rvm_statistics->tot_wrap = status->tot_wrap;
+        rvm_statistics->log_dev_max = status->log_dev_max;
+        cur_log_length(log,&rvm_statistics->log_written);
+        rvm_statistics->tot_log_written = status->tot_log_written;
+        rvm_statistics->range_overlap = status->range_overlap;
+        rvm_statistics->tot_range_overlap = status->tot_range_overlap;
+        rvm_statistics->trans_overlap = status->trans_overlap;
+        rvm_statistics->tot_trans_overlap = status->tot_trans_overlap;
+        rvm_statistics->n_range_elim = status->n_range_elim;
+        rvm_statistics->n_trans_elim = status->n_trans_elim;
+        rvm_statistics->n_trans_coalesced = status->n_trans_coalesced;
+        rvm_statistics->tot_range_elim = status->tot_range_elim;
+        rvm_statistics->tot_trans_elim = status->tot_trans_elim;
+        rvm_statistics->tot_trans_coalesced = status->tot_trans_coalesced;
+        rvm_statistics->last_flush_time = status->last_flush_time;
+        rvm_statistics->last_truncation_time = status->last_truncation_time;
+        rvm_statistics->last_tree_build_time = status->last_tree_build_time;
+        rvm_statistics->last_tree_apply_time = status->last_tree_apply_time;
+        /* copy histograms and timings */
+        for (i=0; i < flush_times_len; i++)
+            {
+            rvm_statistics->flush_times[i] = status->flush_times[i];
+            rvm_statistics->tot_flush_times[i] =
+                status->tot_flush_times[i];
+            }
+        rvm_statistics->flush_time = status->flush_time;
+        rvm_statistics->tot_flush_time = status->tot_flush_time;
+        rvm_statistics->tot_truncation_time = status->tot_truncation_time;
+        for (i=0; i < range_lengths_len; i++)
+            {
+            rvm_statistics->range_lengths[i] =
+                status->range_lengths[i];
+            rvm_statistics->tot_range_lengths[i] =
+                status->tot_range_lengths[i];
+            rvm_statistics->range_overlaps[i] =
+                status->range_overlaps[i];
+            rvm_statistics->tot_range_overlaps[i] =
+                status->tot_range_overlaps[i];
+            rvm_statistics->trans_overlaps[i] =
+                status->trans_overlaps[i];
+            rvm_statistics->tot_trans_overlaps[i] =
+                status->tot_trans_overlaps[i];
+            }
+        for (i=0; i < range_elims_len; i++)
+            {
+            rvm_statistics->range_elims[i] = status->range_elims[i];
+            rvm_statistics->tot_range_elims[i] =
+                status->tot_range_elims[i];
+            rvm_statistics->trans_elims[i] = status->trans_elims[i];
+            rvm_statistics->tot_trans_elims[i] =
+                status->tot_trans_elims[i];
+            rvm_statistics->tot_trans_coalesces[i] =
+                status->tot_trans_coalesces[i];
+            }
+        for (i=0; i < truncation_times_len; i++)
+            {
+            rvm_statistics->tot_tree_build_times[i] =
+                status->tot_tree_build_times[i];
+            rvm_statistics->tot_tree_apply_times[i] =
+                status->tot_tree_apply_times[i];
+            rvm_statistics->tot_truncation_times[i] =
+                status->tot_truncation_times[i];
+            }
+        });                             /* end dev_lock crit sec */
+    /* get non-status area statistics */
+    CRITICAL(log->tid_list_lock,
+        rvm_statistics->n_uncommit = log->tid_list.list.length);
+    CRITICAL(log->flush_list_lock,
+        {
+        rvm_statistics->n_no_flush = 0;
+        RVM_ZERO_OFFSET(rvm_statistics->no_flush_length);
+        FOR_ENTRIES_OF(log->flush_list,int_tid_t,tid)
+            {
+            if (!TID(FLUSH_FLAG))
+                {
+                rvm_statistics->n_no_flush++;
+                rvm_statistics->no_flush_length = RVM_ADD_OFFSETS(
+                       rvm_statistics->no_flush_length,tid->log_size);
+                }
+            }
+        });
+
+    return RVM_SUCCESS;
+}
diff --git a/rvm/rvm_trans.c b/rvm/rvm_trans.c
new file mode 100644
index 0000000..304f49d
--- /dev/null
+++ b/rvm/rvm_trans.c
@@ -0,0 +1,1010 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-2010 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                   RVM Transaction & Range Functions
+*
+*/
+
+#include <errno.h>
+#include "rvm_private.h"
+
+/* global variables */
+
+extern log_t        *default_log;       /* default log descriptor ptr */
+extern char         *rvm_errmsg;        /* internal error message buffer */
+extern rvm_length_t rvm_optimizations;  /* optimization switches */
+
+/* local constants, macros */
+
+#define X_RANGES_INCR   5               /* increment for x_ranges allocation */
+
+/* special search function signature type */
+typedef rvm_bool_t bool_func_t();
+/* rvm_tid checker */
+rvm_return_t bad_tid(rvm_tid)
+    rvm_tid_t           *rvm_tid;
+    {
+    if (rvm_tid == NULL)
+        return RVM_ETID;
+
+    if (rvm_tid->struct_id != rvm_tid_id)
+        return RVM_ETID;
+
+    return RVM_SUCCESS;
+    }
+
+/* tid lookup and checker */
+static int_tid_t *get_tid(rvm_tid_t *rvm_tid)
+{
+    int_tid_t           *tid;
+
+    /* sanity checks */
+    if (bad_tid(rvm_tid) != RVM_SUCCESS) return NULL;
+    if (rvm_tid->uname.tv_sec == 0) return NULL;
+    tid = (int_tid_t *)rvm_tid->tid;
+    if (tid == NULL) return NULL;
+    if (tid->links.struct_id != int_tid_id) return NULL;
+
+    /* begin tid_lock crit sec (normally ended by caller) */
+    RW_CRITICAL(tid->tid_lock,w,
+        {
+        if (TIME_EQL(rvm_tid->uname,tid->uname)
+            && TIME_EQL_ZERO(tid->commit_stamp))
+           return tid;
+
+        });
+
+    return NULL;
+}
+
+/* reallocate tid x_ranges vector if necessary */
+static rvm_bool_t alloc_x_ranges(tid,len)
+    int_tid_t       *tid;
+    int             len;
+    {
+    if ((tid->x_ranges_len+len) > tid->x_ranges_alloc)
+        {
+        if (len <= X_RANGES_INCR)
+            tid->x_ranges_alloc += X_RANGES_INCR;
+        else
+            tid->x_ranges_alloc += len;
+        tid->x_ranges = (range_t **)REALLOC(tid->x_ranges,
+                             tid->x_ranges_alloc*sizeof(range_t *));
+        if (tid->x_ranges == NULL) return rvm_false;
+        }
+
+    return rvm_true;
+    }
+
+/* construct range descriptor and fill in region parameters */
+static range_t *build_range(region,dest,length)
+    region_t        *region;
+    char            *dest;
+    rvm_length_t    length;
+    {
+    range_t         *new_range;         /* ptr for new range descriptor */
+
+    /* create range descriptor and initialize */
+    if ((new_range = make_range()) == NULL) return NULL;
+    new_range->region = region;
+    new_range->nv.seg_code = region->seg->seg_code;
+
+    /* set basic size fields */
+    new_range->nv.length = length;
+    new_range->nv.vmaddr = dest;
+
+    /* calculate offsets */
+    new_range->nv.offset = RVM_ADD_LENGTH_TO_OFFSET(region->offset,
+             ((rvm_length_t)dest-(rvm_length_t)region->vmaddr));
+    new_range->end_offset =
+        RVM_ADD_LENGTH_TO_OFFSET(new_range->nv.offset,length);
+
+    return new_range;
+    }
+/* save old values for a range */
+static char *save_ov(range)
+    range_t         *range;
+    {
+
+    range->data_len = RANGE_LEN(range);
+    range->data = malloc(range->data_len);
+    if (range->data != NULL)
+        BCOPY(range->nv.vmaddr,range->data,range->nv.length);
+
+    return range->data;
+    }
+
+/* restore old values for a transaction */
+static void restore_ov(tid)
+    int_tid_t       *tid;
+    {
+    range_t         *range;
+
+    /* ranges must be restored in reverse order of modification
+       to exactly restore memory when optimizations are not done;
+       order irrelevant if optimizing since ranges will be disjoint */
+    UNLINK_REVERSE_NODES_OF(tid->range_tree,range_t,range)
+        {
+        assert(range->links.node.struct_id == range_id);
+
+        /* restore old values if necessary */
+        if (range->nv.length != 0)
+            {
+            if (TID(RESTORE_FLAG))
+                BCOPY(range->data,range->nv.vmaddr,range->nv.length);
+
+            /* decr uncommited mods cnt */
+            assert(range->region->links.struct_id == region_id);
+            CRITICAL(range->region->count_lock,
+                     range->region->n_uncommit--);
+            }
+
+        free_range(range);
+        }
+    }
+/* range tree comparator function for chronological order insertion */
+static long cmp_range_num(range1,range2)
+    range_t         *range1;
+    range_t         *range2;
+    {
+
+    /* compare range numbers */
+    if (range1->nv.range_num > range2->nv.range_num)
+        return 1;
+    if (range1->nv.range_num < range2->nv.range_num)
+        return -1;
+    return 0;
+    }
+
+/* unconditionally add new range descriptor to tid's range tree
+   used for non-optimized transactions only */
+static rvm_return_t add_new_range(tid,new_range)
+    int_tid_t       *tid;
+    range_t         *new_range;         /* ptr for new range descriptor */
+    {
+
+    /* build old value record if necessary */
+    assert(new_range->links.node.struct_id == range_id);
+    if (new_range->nv.length != 0)
+        {
+        if (TID(RESTORE_FLAG))
+            if (save_ov(new_range) == NULL)
+                {
+                free_range(new_range);
+                return RVM_ENO_MEMORY;
+                }
+
+        /* count uncommitted mods */
+        assert(new_range->region->links.struct_id == region_id);
+        CRITICAL(new_range->region->count_lock,
+                 new_range->region->n_uncommit++);
+        }
+
+    /* insert range to transaction's range tree in order of arrival */
+    new_range->nv.range_num = tid->range_tree.n_nodes+1;
+    if (!tree_insert(&tid->range_tree,(tree_node_t *)new_range,
+                     cmp_range_num))
+        assert(rvm_false);
+
+    return RVM_SUCCESS;
+    }
+/* range tree comparator for tree_insert by compound key:
+   {region, segment displacement}
+   adjacent nodes within region are considered equivalent */
+static long region_partial_include(range1,range2)
+    range_t         *range1;
+    range_t         *range2;
+    {
+
+    /* compare displacements if in same region */
+    if (range1->region == range2->region)
+        {
+        /* compare displacement within segment */
+        if (RVM_OFFSET_GTR(range1->nv.offset,range2->end_offset))
+            return 1;
+        if (RVM_OFFSET_LSS(range1->end_offset,range2->nv.offset))
+            return -1;
+        return 0;
+        }
+
+    /* compare region descriptor address */
+    if (range1->region > range2->region)
+        return 1;
+    return -1;
+    }
+
+/* range tree comparator for tree_insert by compound key:
+   {segment, segment displacement}
+   adjacent nodes within segment are considered equivalent */
+static long segment_partial_include(range1,range2)
+    range_t         *range1;
+    range_t         *range2;
+    {
+
+    /* compare displacements if in same segment */
+    if (range1->nv.seg_code == range2->nv.seg_code)
+        {
+        /* compare displacement within segment */
+        if (RVM_OFFSET_GTR(range1->nv.offset,range2->end_offset))
+            return 1;
+        if (RVM_OFFSET_LSS(range1->end_offset,range2->nv.offset))
+            return -1;
+        return 0;
+        }
+    
+    /* compare segment codes */
+    if (range1->nv.seg_code > range2->nv.seg_code)
+        return 1;
+    return -1;
+    }
+/* detect and form list of overlapping or adjacent ranges */
+static rvm_bool_t find_overlap(tid,new_range,cmp_func,elim,overlap,retval)
+    int_tid_t       *tid;               /* transaction descriptor to search */
+    range_t         *new_range;         /* descriptor for new range */
+    cmp_func_t      *cmp_func;          /* tree comparator function */
+    rvm_length_t    *elim;              /* number of eliminated ranges */
+    rvm_offset_t    *overlap;           /* length of eliminated ranges */
+    rvm_return_t    *retval;            /* error code return */
+    {
+    range_t         *range;             /* ptr to existing range descriptor */
+    rvm_offset_t    off_tmp;            /* arithmetic temporary */
+
+    tid->x_ranges_len = 0;
+    *retval = RVM_SUCCESS;
+
+    /* try to insert, iterate on collision */
+    FROM_EXISTING_NODE_OF(tid->range_tree,range_t,range,
+                         new_range,*cmp_func)
+        {
+        /* see if out of region/segment or new_range */
+        if ((*cmp_func)(new_range,range) != 0)
+            break;
+
+        /* save overlaping/adjacent ranges */
+        if (!alloc_x_ranges(tid,1))
+            {
+            *retval = RVM_ENO_MEMORY;
+            return rvm_true;
+            }
+        tid->x_ranges[tid->x_ranges_len++] = range;
+        (*elim)++;
+
+        /* special case: new range completly contained in existing range */
+        if ((tid->x_ranges_len == 1)
+            && (RVM_OFFSET_GEQ(new_range->nv.offset,range->nv.offset)
+                && RVM_OFFSET_LEQ(new_range->end_offset,
+                                  range->end_offset)))
+            {
+            *overlap = RVM_ADD_LENGTH_TO_OFFSET(*overlap,
+                                                new_range->nv.length);
+            return rvm_true;
+            }
+        /* general case: merge all overlaping/adjacent ranges
+           tally savings and save new bounds of composite range */
+
+        /* test if non-overlaping values preceed new range */
+        if (RVM_OFFSET_LSS(range->nv.offset,new_range->nv.offset))
+            {
+            off_tmp = RVM_SUB_OFFSETS(range->end_offset,
+                                      new_range->nv.offset);
+            *overlap = RVM_ADD_OFFSETS(*overlap,off_tmp);
+            new_range->nv.offset = range->nv.offset;
+            }
+        else
+            /* test if non-overlapping values follow new range */
+            if (RVM_OFFSET_GTR(range->end_offset,new_range->end_offset))
+                {
+                off_tmp = RVM_SUB_OFFSETS(new_range->end_offset,
+                                          range->nv.offset);
+                *overlap = RVM_ADD_OFFSETS(*overlap,off_tmp);
+                new_range->end_offset = range->end_offset;
+                }
+        else
+            /* range completely contained in new range */
+            *overlap = RVM_ADD_LENGTH_TO_OFFSET(*overlap,
+                                                range->nv.length);
+
+        /* update length of composite range */
+        new_range->nv.length =
+            RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(new_range->end_offset,
+                                                new_range->nv.offset));
+        }
+
+
+    return rvm_false;
+    }
+/* merge new range with existing range(s) */
+static rvm_return_t merge_range(tid,region,new_range)
+    int_tid_t       *tid;
+    region_t        *region;
+    range_t         *new_range;         /* ptr to new range descriptor */
+    {
+    range_t         *range;             /* ptr range descriptor */
+    char            *vmaddr;            /* working vm address */
+    char            *data_addr;         /* working ov buffer address */
+    rvm_length_t    len;                /* length temp */
+    int             i;
+    rvm_return_t    retval;
+
+    /* do simple insertion if not optimizing */
+    if (!TID(RVM_COALESCE_RANGES))
+        return add_new_range(tid,new_range);
+
+    /* search for overlap/adjacency with existing ranges in same region */
+    if (find_overlap(tid,new_range,region_partial_include,
+                     &tid->range_elim,&tid->range_overlap,&retval))
+        {
+        /* totally optimized away or error */
+        free_range(new_range);
+        return retval;
+        }
+
+    /* if new range was inserted, update region ref count and exit */
+    if (tid->x_ranges_len == 0)
+        {
+        if (TID(RESTORE_FLAG))
+            if (save_ov(new_range) == NULL)
+                {
+                if (!tree_delete(&tid->range_tree,new_range,
+                                 region_partial_include))
+                    assert(rvm_false);
+                free_range(new_range);
+                return RVM_ENO_MEMORY;
+                }
+        CRITICAL(region->count_lock,region->n_uncommit++);
+        }
+    else
+        {
+        /* update vmaddr and allocate new old value buffer */
+        range = tid->x_ranges[0];
+        if (range->nv.vmaddr < new_range->nv.vmaddr)
+            new_range->nv.vmaddr = range->nv.vmaddr;
+        if (TID(RESTORE_FLAG))
+            {
+            new_range->data_len = RANGE_LEN(new_range);
+            new_range->data = malloc(new_range->data_len);
+            if (range->data == NULL) return RVM_ENO_MEMORY;
+            }
+        /* otherwise, merge existing old values into new node and put
+           its data in 1st existing node in the tree; kill any others */
+        vmaddr = new_range->nv.vmaddr;
+        data_addr = new_range->data;
+        for (i = 0; i < tid->x_ranges_len; i++)
+            {
+            range = tid->x_ranges[i];
+            if (TID(RESTORE_FLAG))
+                {
+                /* copy old values preceeding existing range to new buffer */
+                if (vmaddr < range->nv.vmaddr)
+                    {
+                    len = (rvm_length_t)RVM_SUB_LENGTH_FROM_ADDR(
+                                            range->nv.vmaddr,vmaddr);
+                    BCOPY(vmaddr,data_addr,len);
+                    vmaddr = RVM_ADD_LENGTH_TO_ADDR(vmaddr,len);
+                    data_addr = RVM_ADD_LENGTH_TO_ADDR(data_addr,len);
+                    }
+
+                /* copy old values of existing range to new buffer */
+                BCOPY(range->data,data_addr,range->nv.length);
+                vmaddr =
+                    RVM_ADD_LENGTH_TO_ADDR(vmaddr,range->nv.length);
+                data_addr =
+                    RVM_ADD_LENGTH_TO_ADDR(data_addr,range->nv.length);
+
+                /* copy trailing old values if necessary */
+                if (i == (tid->x_ranges_len-1))
+                    {
+                    len = (rvm_length_t)
+                        RVM_ADD_LENGTH_TO_ADDR(new_range->nv.vmaddr,
+                                               new_range->nv.length);
+                    if ((char *)len > vmaddr)
+                        {
+                        len = (rvm_length_t)
+                            RVM_SUB_LENGTH_FROM_ADDR(len,vmaddr);
+                        BCOPY(vmaddr,data_addr,len);
+                        assert(RVM_ADD_LENGTH_TO_ADDR(vmaddr,len) ==
+                           RVM_ADD_LENGTH_TO_ADDR(new_range->nv.vmaddr,
+                                                  new_range->nv.length));
+                        }
+                    }
+                }
+
+            /* kill replaced nodes in tree */
+            if (i != 0)
+                {
+                if (!tree_delete(&tid->range_tree,range,
+                                 region_partial_include))
+                    assert(rvm_false);
+                free_range(range);
+                }
+            }
+        /* update ov buffer, merged range size */
+        range = tid->x_ranges[0];
+        if (TID(RESTORE_FLAG))
+            {
+            free(range->data);
+            range->data = new_range->data;
+            range->data_len = new_range->data_len;
+            new_range->data = NULL;
+            }
+        range->nv.vmaddr = new_range->nv.vmaddr;
+        range->nv.length = new_range->nv.length;
+        range->nv.offset = new_range->nv.offset;
+        range->end_offset = new_range->end_offset;
+        free_range(new_range); 
+
+        /* update region uncommitted reference count */
+        CRITICAL(region->count_lock,
+            region->n_uncommit -= (tid->x_ranges_len-1));
+        }    
+
+    return RVM_SUCCESS;
+    }
+/* rvm_set_range */
+rvm_return_t rvm_set_range(rvm_tid,dest,length)
+    rvm_tid_t       *rvm_tid;           /* transaction affected */
+    void            *dest;              /* base vm address of range */
+    rvm_length_t    length;             /* length of range */
+    {
+    int_tid_t       *tid;               /* internal tid ptr */
+    region_t        *region;
+    range_t         *new_range;         /* ptr to new range descriptor */
+    rvm_return_t    retval = RVM_SUCCESS;
+
+    /* basic entry checks */
+    if (bad_init()) return RVM_EINIT;
+
+    /* ignore null ranges */
+    if ((rvm_optimizations != 0) && (length == 0))
+        return RVM_SUCCESS;
+
+    /* lookup and lock tid */
+    if ((tid = get_tid(rvm_tid)) == NULL) /* begin tid_lock critical section */
+        return RVM_ETID;
+
+    /* lookup and lock region */
+    region = find_whole_range(dest,length,r); /* begin region_lock crit sect */
+    if (region == NULL)
+        retval = RVM_ENOT_MAPPED;
+    else
+        {
+        /* build new range descriptor and do optimizations */
+        new_range = build_range(region,dest,length);
+        retval = merge_range(tid,region,new_range);
+        rw_unlock(&region->region_lock,r);   /* end region_lock crit sect */
+        }
+
+    rw_unlock(&tid->tid_lock,w);        /* end tid_lock critical section */
+    return retval;
+    }
+/* rvm_modify_bytes */
+rvm_return_t rvm_modify_bytes(rvm_tid,dest,src,length)
+    rvm_tid_t           *rvm_tid;       /* transaction affected */
+    void                *dest;          /* base vm address of range */
+    const void                *src;           /* source of nv's */
+    rvm_length_t        length;         /* length of range */
+    {
+    rvm_return_t        retval;
+
+    /* call rvm_set_range to do most of the work */
+    if ((retval = rvm_set_range(rvm_tid,dest,length)) != RVM_SUCCESS)
+        return retval;
+
+    /* must memmove since there is no guarantee
+       that src and dest don't overlap */
+/*    (void)memmove(dest,src,length); <-- not available on RT's */
+    (void)BCOPY(src,dest,(int)length);
+
+    return RVM_SUCCESS;
+    }
+/* calculate transaction's new value log entry size */
+static void nv_size(tid,size)
+    int_tid_t       *tid;               /* transaction to size */
+    rvm_offset_t    *size;              /* compute length; double [out] */
+    {
+    range_t         *range;             /* current range */
+
+    /* sum sizes of ranges */
+    RVM_ZERO_OFFSET(*size);
+    FOR_NODES_OF(tid->range_tree,range_t,range)
+        *size = RVM_ADD_LENGTH_TO_OFFSET(*size,RANGE_SIZE(range));
+    }
+
+/* compute total i/o size */
+static rvm_return_t nv_io_size(tid,size)
+    int_tid_t       *tid;               /* transaction to size */
+    rvm_offset_t    *size;              /* compute length; double [out] */
+    {
+    log_t           *log = tid->log;    /* log descriptor */
+
+    /* compute length of log entries; with overhead,
+       including possible wrap & split (too small to overflow) */
+    nv_size(tid,size);
+    *size = RVM_ADD_LENGTH_TO_OFFSET(*size,TRANS_SIZE+MIN_TRANS_SIZE);
+
+    /* test max size */
+    if (RVM_OFFSET_GTR(*size,log->status.log_size))
+        return RVM_ETOO_BIG;            /* bigger than log device */
+    if (RVM_OFFSET_HIGH_BITS_TO_LENGTH(*size) != 0)
+        return RVM_ETOO_BIG;            /* 32 bit max */
+
+    return RVM_SUCCESS;
+    }
+/* save new values for a range */
+static rvm_return_t save_nv(range)
+    range_t         *range;             /* modification range descriptor */
+    {
+    rvm_length_t    range_len;          /* save size of range */
+
+    if (range->nv.length != 0)
+        {
+        range_len = RANGE_LEN(range);
+        if (range->data == NULL)
+            {
+            range->data = calloc(1, range_len);
+            if (range->data == NULL) return RVM_ENO_MEMORY;
+            range->nvaddr = range->data;
+            range->data_len = range_len;
+            }
+        assert(range->data_len >= range_len);
+    
+        /* copy to old value space */
+        src_aligned_bcopy(range->nv.vmaddr,range->data,
+                          range->nv.length);
+        }
+
+    return RVM_SUCCESS;
+    }
+
+/* save all new values for no_flush commit */
+static rvm_return_t save_all_nvs(tid)
+    int_tid_t       *tid;               /* committing transaction */
+    {
+    range_t         *range;             /* modification range descriptor */
+    rvm_return_t    retval;
+
+    /* cache nv's for future flush */
+    FOR_NODES_OF(tid->range_tree,range_t,range)
+        {
+        /* allocate buffer and save new values for range */
+        if ((retval=save_nv(range)) != RVM_SUCCESS)
+            return retval;
+
+        /* update uncommited count */
+        assert(range->region->links.struct_id == region_id);
+        CRITICAL(range->region->count_lock,
+                 range->region->n_uncommit--);
+        }
+
+    return RVM_SUCCESS;
+    }
+/* merge range into queued tid */
+static rvm_return_t merge_tid(q_tid,tid,new_range)
+    int_tid_t       *q_tid;             /* ptr to last queued tid */
+    int_tid_t       *tid;               /* ptr to new tid */
+    range_t         *new_range;         /* new range ptr */
+    {
+    region_t        *region = new_range->region;
+    range_t         *range;             /* existing range ptr */
+    rvm_length_t    range_len;          /* save size of range */
+    char            *nv_ptr;            /* ptr into new value buffers */
+    char            *new_nv_ptr;        /* 2nd ptr into new value buffers */
+    rvm_offset_t    old_offset;         /* original offset of new range */
+    rvm_length_t    old_length;         /* original length of new range */
+    rvm_offset_t    old_end_offset;     /* original end offset of new range */
+    rvm_length_t    data_off;           /* low order bytes of data offset */
+    int             i;
+    rvm_return_t    retval;
+
+    /* search for overlap/adjacency with existing ranges in same segment */
+    old_offset = new_range->nv.offset;
+    old_length = new_range->nv.length;
+    old_end_offset = new_range->end_offset;
+    if (find_overlap(q_tid,new_range,segment_partial_include,
+                     &tid->trans_elim,&tid->trans_overlap,&retval))
+        {
+        if (retval != RVM_SUCCESS) return retval;
+
+        /* new range totally contained in existing range, update new values */
+        range = q_tid->x_ranges[0];
+        nv_ptr = (char *)RVM_OFFSET_TO_LENGTH(
+                   RVM_SUB_OFFSETS(old_offset,range->nv.offset));
+        nv_ptr = RVM_ADD_LENGTH_TO_ADDR(nv_ptr,range->data
+             + BYTE_SKEW(RVM_OFFSET_TO_LENGTH(range->nv.offset)));
+        assert(range->data != NULL);
+        assert((nv_ptr+old_length-range->data) <= range->data_len);
+        BCOPY(new_range->nv.vmaddr,nv_ptr,old_length);
+        goto exit;
+        }
+    /* see if simply inserted */
+    if (q_tid->x_ranges_len == 0)
+        {
+        q_tid->log_size =               /* update length of queued tid */
+            RVM_ADD_LENGTH_TO_OFFSET(q_tid->log_size,
+                                     RANGE_SIZE(new_range));
+        /* save new values if no_flush commit */
+        if (TID(FLUSH_FLAG))
+            {
+            if (new_range->data != NULL)
+                {
+                free(new_range->data);
+                new_range->data = NULL;
+                new_range->data_len = 0;
+                new_range->nvaddr = NULL;
+                }
+            return RVM_SUCCESS;
+            }
+        if ((retval=save_nv(new_range)) != RVM_SUCCESS)
+            return retval;
+        range = new_range;
+        goto update;
+        }
+
+    /* see if existing totally contained in new and no other overlap */
+    range = q_tid->x_ranges[0];
+    if ((q_tid->x_ranges_len == 1)
+            && (RVM_OFFSET_GEQ(range->nv.offset,old_offset)
+                && RVM_OFFSET_LEQ(range->end_offset,old_end_offset)))
+        {
+        /* copy new range data */
+        if (TID(FLUSH_FLAG))
+            {
+            if (new_range->data != NULL)
+                {
+                free(new_range->data);
+                new_range->data = NULL;
+                new_range->data_len = 0;
+                new_range->nvaddr = NULL;
+                }
+            }
+        else
+            if ((retval=save_nv(new_range)) != RVM_SUCCESS)
+                return retval;
+        goto replace;
+        }
+    /* do general merge: reallocate new value save buffer */
+    if (new_range->data != NULL)
+        free(new_range->data);
+    data_off = RVM_OFFSET_TO_LENGTH(new_range->nv.offset);
+    new_range->data_len = ALIGNED_LEN(data_off,new_range->nv.length);
+    if ((new_range->data=calloc(1, new_range->data_len)) == NULL)
+        return RVM_ENO_MEMORY;
+    new_range->nvaddr = new_range->data;
+
+    /* save new values from new range */
+    nv_ptr =
+        RVM_ADD_LENGTH_TO_ADDR(new_range->data,
+            RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(old_offset,
+                new_range->nv.offset))+BYTE_SKEW(data_off));
+    assert((nv_ptr+old_length-new_range->data) <= new_range->data_len);
+    BCOPY(new_range->nv.vmaddr,nv_ptr,old_length);
+
+    /* update vmaddr -- only valid if no remappings have been done
+       since last commit; from now on, used only by rvmutl */
+    if (range->nv.vmaddr < new_range->nv.vmaddr)
+        new_range->nv.vmaddr = range->nv.vmaddr;
+    /* merge new values from existing ranges
+       copy leading bytes from 1st existing range */
+    if (RVM_OFFSET_GTR(old_offset,range->nv.offset))
+        {
+        range_len =
+            RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(old_offset,
+                                                 range->nv.offset));
+        assert(range->data != NULL);
+        nv_ptr = RVM_ADD_LENGTH_TO_ADDR(range->data,BYTE_SKEW(
+                         RVM_OFFSET_TO_LENGTH(range->nv.offset)));
+        new_nv_ptr = RVM_ADD_LENGTH_TO_ADDR(new_range->data,
+                                            BYTE_SKEW(data_off));
+        assert((new_nv_ptr-new_range->data+range_len)
+               <= new_range->data_len);
+        BCOPY(nv_ptr,new_nv_ptr,range_len);
+        }
+
+    /* copy trailing bytes from last existing range */
+    range = q_tid->x_ranges[q_tid->x_ranges_len-1];
+    if (RVM_OFFSET_GTR(range->end_offset,old_end_offset))
+        {
+        range_len =
+            RVM_OFFSET_TO_LENGTH(RVM_SUB_OFFSETS(range->end_offset,
+                                                 old_end_offset));
+        assert(range->data != NULL);
+        nv_ptr = RVM_ADD_LENGTH_TO_ADDR(range->data,BYTE_SKEW(
+                         RVM_OFFSET_TO_LENGTH(range->nv.offset)));
+        nv_ptr = RVM_ADD_LENGTH_TO_ADDR(nv_ptr,range->nv.length);
+        nv_ptr = RVM_SUB_LENGTH_FROM_ADDR(nv_ptr,range_len);
+        new_nv_ptr = RVM_ADD_LENGTH_TO_ADDR(new_range->data,
+                         BYTE_SKEW(data_off)+new_range->nv.length);
+        new_nv_ptr = RVM_SUB_LENGTH_FROM_ADDR(new_nv_ptr,range_len);
+        assert((nv_ptr-range->data+range_len) <= range->data_len);
+        assert((new_nv_ptr-new_range->data+range_len)
+               <= new_range->data_len);
+        BCOPY(nv_ptr,new_nv_ptr,range_len);
+        }
+    /* delete nodes now obsolete */
+    for (i = 1; i < q_tid->x_ranges_len; i++)
+        {
+        range = q_tid->x_ranges[i];
+        q_tid->log_size =
+            RVM_SUB_LENGTH_FROM_OFFSET(q_tid->log_size,
+                                       RANGE_SIZE(range));
+        if (!tree_delete(&q_tid->range_tree,range,
+                             segment_partial_include))
+            assert(rvm_false);
+        free_range(range);
+        }
+    range = q_tid->x_ranges[0];
+
+    /* replace existing range data with new range data*/
+  replace:
+    q_tid->log_size =
+        RVM_SUB_LENGTH_FROM_OFFSET(q_tid->log_size,
+                                   RANGE_SIZE(range));
+    q_tid->log_size =
+        RVM_ADD_LENGTH_TO_OFFSET(q_tid->log_size,
+                                 RANGE_SIZE(new_range));
+    range->nv.vmaddr = new_range->nv.vmaddr;
+    range->nv.length = new_range->nv.length;
+    range->nv.offset = new_range->nv.offset;
+    range->end_offset = new_range->end_offset;
+    free(range->data);
+    range->data = new_range->data;
+    range->data_len = new_range->data_len;
+    range->nvaddr = new_range->nvaddr;
+    new_range->data = NULL;
+
+  exit:
+    free_range(new_range);
+  update:    /* update region's uncommitted reference count */
+    if (range->data != NULL)
+        CRITICAL(region->count_lock,region->n_uncommit--);
+    return RVM_SUCCESS;
+    }
+/* merge ranges of transaction with previous transactions */
+static rvm_return_t coalesce_trans(tid,q_tid)
+    int_tid_t       *tid;               /* tid to log */
+    int_tid_t       *q_tid;             /* ptr to last queued tid */
+    {
+    range_t         *range;             /* range ptr */
+    rvm_return_t    retval;
+
+    /* merge ranges of new tid with queued tid */
+    UNLINK_NODES_OF(tid->range_tree,range_t,range)
+        {
+        /* merge the range into queued tid */
+        if ((retval=merge_tid(q_tid,tid,range)) != RVM_SUCCESS)
+            return retval;
+        }
+
+    /* completely merged, update commit stamp and statistics
+       scrap descriptor */
+    q_tid->commit_stamp = tid->commit_stamp;
+    q_tid->range_elim += tid->range_elim;
+    q_tid->trans_elim += tid->trans_elim;
+    q_tid->range_overlap = RVM_ADD_OFFSETS(q_tid->range_overlap,
+                                           tid->range_overlap);
+    q_tid->trans_overlap = RVM_ADD_OFFSETS(q_tid->trans_overlap,
+                                           tid->trans_overlap);
+    q_tid->n_coalesced++;
+    free_tid(tid);
+
+    return RVM_SUCCESS;
+    }
+/* get address of queued tid that current tid can merge with */
+static int_tid_t *get_queued_tid(tid)
+    int_tid_t       *tid;               /* tid to log */
+    {
+    log_t           *log = tid->log;    /* log descriptor */
+    int_tid_t       *q_tid;             /* ptr to last queued tid */
+    rvm_offset_t    size_temp;
+
+    if (log->flush_list.list.length == 0)
+	return NULL;                   /* no, force re-init of merge */
+
+    /* get last queued tid in flush list */
+    q_tid = (int_tid_t *)log->flush_list.preventry;
+
+    /* see if can legitimately merge */
+    if ((q_tid->flags & FLUSH_MARK) != 0 ||
+	(q_tid->flags & RVM_COALESCE_TRANS) == 0)
+	return NULL;                   /* no, force re-init of merge */
+
+    size_temp = RVM_ADD_OFFSETS(q_tid->log_size,tid->log_size);
+    if (RVM_OFFSET_GTR(size_temp,log->status.log_size))
+        return NULL;                   /* no, force re-init of merge */
+
+    return q_tid;
+    }
+/* establish log entry for committing tid */
+static rvm_return_t queue_tid(tid)
+    int_tid_t       *tid;               /* tid to log */
+    {
+    log_t           *log = tid->log;    /* log descriptor */
+    int_tid_t       *q_tid;             /* ptr to last queued tid */
+    rvm_bool_t      flush_flag;
+    rvm_return_t    retval;
+
+    /* make sure transaction not too large for log */
+    if ((retval=nv_io_size(tid,&tid->log_size)) != RVM_SUCCESS)
+        return retval;                  /* transaction too big to log */
+
+    if (init_unames() != 0)             /* update uname generator */
+        return RVM_EIO;
+    flush_flag = (rvm_bool_t)TID(FLUSH_FLAG); /* save flush flag */
+
+    /* queue tid for flush */
+    CRITICAL(log->flush_list_lock,      /* begin flush_list_lock crit sec */
+        {
+	make_uname(&tid->commit_stamp);     /* record commit timestamp */
+        /* test for transaction coalesce */
+        if (TID(RVM_COALESCE_TRANS))
+            {
+            /* see if must initialize coalescing */
+            if ((q_tid=get_queued_tid(tid)) == NULL)
+                {
+                if (flush_flag) goto enqueue; /* nothing to coalesce! */
+
+                /* initialize transaction merger by inserting copy of tid
+                   with empty tree so that ranges get reordered by merge */
+                if ((q_tid=(int_tid_t *)alloc_list_entry(int_tid_id)) == NULL)
+                    {
+                    retval = RVM_ENO_MEMORY;
+                    goto exit;
+                    }
+                BCOPY(tid,q_tid,sizeof(int_tid_t));
+                init_tree_root(&q_tid->range_tree);
+                RVM_ZERO_OFFSET(q_tid->log_size);
+                tid->x_ranges = NULL; /* array now owned by q_tid */
+                (void)move_list_entry(NULL,&log->flush_list,
+                                      &q_tid->links);
+                }
+
+            /* merge ranges of tid with previously queued tid(s) */
+            retval = coalesce_trans(tid,q_tid);
+            goto exit;
+            }
+        /* save new values if necessary and queue */
+        if (!flush_flag)
+            if (tid->range_tree.n_nodes != 0)
+                if ((retval = save_all_nvs(tid)) != RVM_SUCCESS)
+                    goto exit;  /* too big for heap */
+
+        /* enqueue new tid */
+enqueue:
+        (void)move_list_entry(NULL,&log->flush_list,&tid->links);
+
+exit:;
+        });                             /* end flush_list_lock crit sec */
+    if (retval != RVM_SUCCESS) return retval;
+
+    /* flush log if commit requires */
+    if (flush_flag)
+        retval = flush_log(log,&log->status.n_flush);
+
+    return retval;
+    }
+/* rvm_begin_transaction */
+rvm_return_t rvm_begin_transaction(rvm_tid,mode)
+    rvm_tid_t           *rvm_tid;       /* ptr to rvm_tid */
+    rvm_mode_t          mode;           /* transaction's mode */
+    {
+    int_tid_t           *tid;           /* internal tid */
+    log_t               *log;           /* log device  */
+    rvm_return_t        retval;
+
+    /* basic entry checks */
+    if (bad_init()) return RVM_EINIT;
+    if ((retval=bad_tid(rvm_tid)) != RVM_SUCCESS)
+        return retval;
+    if ((default_log == NULL) || (default_log->dev.handle == 0))
+        return RVM_ELOG;
+    if ((mode != restore) && (mode != no_restore))
+        return RVM_EMODE;
+
+    /* allocate tid descriptor */
+    if ((tid = make_tid(mode)) == NULL) return RVM_ENO_MEMORY;
+    rvm_tid->uname = tid->uname;
+
+    /* queue the tid on log tid_list */
+    log = default_log;                  /* this must change if > 1 log */
+    tid->log = log;
+    CRITICAL(log->tid_list_lock,        /* begin tid_list_lock critical section */
+        {
+        (void) move_list_entry(NULL,&log->tid_list,&tid->links);
+        });                             /* end tid_list_lock critical section */
+    rvm_tid->tid = tid;
+    return RVM_SUCCESS;
+    }
+/* rvm_abort_transaction */
+rvm_return_t rvm_abort_transaction(rvm_tid)
+    rvm_tid_t       *rvm_tid;           /* ptr to transaction to abort */
+    {
+    int_tid_t       *tid;               /* internal tid */
+    log_t           *log;               /* log descriptor ptr */
+
+    /* basic entry checks */
+    if (bad_init()) return RVM_EINIT;
+    if ((tid = get_tid(rvm_tid)) == NULL) /* begin tid_lock crit section */
+        return RVM_ETID;
+
+    log = tid->log;
+    CRITICAL(log->tid_list_lock,        /* unlink from log's tid_list */
+             move_list_entry(&log->tid_list,NULL,&tid->links));
+    tid->commit_stamp.tv_sec = 1;       /* temporary mark */
+    rw_unlock(&tid->tid_lock,w);        /* end tid_lock critical section */
+
+    /* restore virtual memory */
+    restore_ov(tid);
+
+    CRITICAL(log->dev_lock, {
+	log->status.n_abort++;              /* count transactions aborted */
+    });
+
+    rvm_tid->tid = NULL;
+    free_tid(tid);                      /* free transaction descriptor */
+    return RVM_SUCCESS;
+    }
+/* rvm_end_transaction */
+rvm_return_t rvm_end_transaction(rvm_tid,mode)
+    rvm_tid_t       *rvm_tid;           /* ptr to transaction to commit */
+    rvm_mode_t      mode;               /* end mode */
+    {
+    int_tid_t       *tid;               /* internal tid */
+    log_t           *log;               /* log descriptor ptr */
+    rvm_return_t    retval;
+
+    /* basic entry checks */
+    if (bad_init()) return RVM_EINIT;
+    if ((mode != flush) && (mode != no_flush))
+        return RVM_EMODE;
+    if ((tid = get_tid(rvm_tid)) == NULL) /* begin tid_lock crit section */
+        return RVM_ETID;
+
+    /* remove tid from log's tid_list */
+    log = tid->log;
+    CRITICAL(log->dev_lock,		/* begin dev_lock crit section */
+    {                          
+	CRITICAL(log->tid_list_lock,	/* unlink tid from log's tid_list*/
+		 move_list_entry(&log->tid_list,NULL,&tid->links));
+        if (mode == flush)              /* record flush mode and count */
+	{
+            tid->flags |= FLUSH_FLAG;
+            log->status.n_flush_commit++;
+	}
+        else
+            log->status.n_no_flush_commit++;
+    });					/* end tid_list_lock crit section */
+    tid->commit_stamp.tv_sec = 1;       /* temporary mark */
+    rw_unlock(&tid->tid_lock,w);        /* end tid_lock crit section */
+
+    /* kill null tids */
+    if ((rvm_optimizations != 0) && (tid->range_tree.n_nodes == 0))
+        {
+        rvm_tid->tid = NULL;
+        free_tid(tid);
+        return RVM_SUCCESS;
+        }
+
+    /* build new value record(s) & flush if necessary */
+    if ((retval=queue_tid(tid)) != RVM_SUCCESS)
+        ZERO_TIME(tid->commit_stamp);
+    else
+        rvm_tid->tid = NULL;
+    return retval;
+    }
diff --git a/rvm/rvm_unmap.c b/rvm/rvm_unmap.c
new file mode 100644
index 0000000..3984232
--- /dev/null
+++ b/rvm/rvm_unmap.c
@@ -0,0 +1,100 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                   RVM  Unmapping
+*
+*/
+
+#include "rvm_private.h"
+
+/* global variables */
+
+extern log_t    *default_log;           /* default log descriptor ptr */
+extern char     *rvm_errmsg;            /* internal error message buffer */
+
+extern
+rw_lock_t       region_tree_lock;       /* lock for region tree */
+extern
+tree_root_t     region_tree;           /* root of mapped region tree */
+/* rvm_unmap */
+rvm_return_t rvm_unmap(rvm_region)
+    rvm_region_t        *rvm_region;    /* region to unmap */
+    {
+    rvm_return_t        retval;
+    region_t            *region;        /* internal region descriptor */
+    seg_t               *seg;           /* internal segment descriptor */
+
+    if (bad_init()) return RVM_EINIT;
+    if ((retval=bad_region(rvm_region)) != RVM_SUCCESS)
+        return retval;
+
+    /* find and lock region descriptor */
+    region = find_whole_range(rvm_region->vmaddr,    /* begin region_tree_lock, */
+                              rvm_region->length,w); /* region_lock crit sects */
+    if (region == NULL)
+        return RVM_ENOT_MAPPED;
+
+    /* check if has uncommitted transactions */
+    if (region->n_uncommit != 0)
+        {
+        retval = RVM_EUNCOMMIT;
+        goto err_exit;
+        }
+
+    /* be sure whole region specified */
+    if ((region->vmaddr != rvm_region->vmaddr) ||
+        (region->length != rvm_region->length))
+        {
+        retval = RVM_ERANGE;
+        goto err_exit;
+        }
+
+    /* remove from region tree and unlock tree */
+    if (!tree_delete(&region_tree,(tree_node_t *)region->mem_region,
+                      mem_total_include))
+        assert(rvm_false);              /* couldn't find node */
+    rw_unlock(&region_tree_lock,w);     /* end region_tree_lock crit sect */
+    rw_unlock(&region->region_lock,w);  /* end region_lock crit sect */
+
+    /* unlink from seg's map_list */
+    seg = region->seg;
+    CRITICAL(seg->seg_lock,             /* begin seg_lock critical section */
+        {
+        (void)move_list_entry(&seg->map_list,NULL,
+                              (list_entry_t *)region);
+
+        /* if dirty, put on unmapped list; otherwise scrap */
+        if (region->dirty)
+            {
+            make_uname(&region->unmap_ts); /* timestamp unmap */
+            (void)move_list_entry(NULL,&seg->unmap_list,
+                                  (list_entry_t *)region);
+            }
+        else
+            free_region(region);
+        });                             /* end seg_lock critical section */
+
+    return RVM_SUCCESS;
+
+err_exit:
+    rw_unlock(&region->region_lock,w);
+    rw_unlock(&region_tree_lock,w);
+    return retval;
+    }
diff --git a/rvm/rvm_utils.c b/rvm/rvm_utils.c
new file mode 100644
index 0000000..83f61c9
--- /dev/null
+++ b/rvm/rvm_utils.c
@@ -0,0 +1,2325 @@
+/* BLURB lgpl
+
+                           Coda File System
+                              Release 5
+
+          Copyright (c) 1987-1999 Carnegie Mellon University
+                  Additional copyrights listed below
+
+This  code  is  distributed "AS IS" without warranty of any kind under
+the  terms of the  GNU  Library General Public Licence  Version 2,  as
+shown in the file LICENSE. The technical and financial contributors to
+Coda are listed in the file CREDITS.
+
+                        Additional copyrights
+                           none currently
+
+#*/
+
+/*
+*
+*                   RVM internal structure support functions
+*
+*                   * Doubly linked list and free list functions
+*                   * RVM structures cache
+*                   * Initializers, Copiers, and Finalizers for RVM exported
+*                     structures.
+*                   * RW_lock functions
+*
+*/
+
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#if defined(hpux) || defined(__hpux)
+#include <hp_bsd.h>
+#endif /* hpux */
+#include <unistd.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <sys/time.h>
+#include "rvm_private.h"
+
+/* globals */
+
+extern rvm_length_t     page_size;
+extern rvm_length_t     page_mask;
+extern char             *rvm_errmsg;    /* internal error message buffer */
+extern rvm_length_t rvm_optimizations;  /* optimization switches */
+/* locals */
+
+/* locks for free lists */
+RVM_MUTEX free_lists_locks[NUM_CACHE_TYPES];
+
+/* creation count vector for instances of internal types */
+long type_counts[NUM_CACHE_TYPES];
+
+/* free list vector for internal types */
+list_entry_t free_lists[NUM_CACHE_TYPES];
+
+/* preallocation count vector for internal types free lists */
+long pre_alloc[NUM_CACHE_TYPES] = {NUM_PRE_ALLOCATED};
+
+/* allocation cache maximum counts */
+long max_alloc[NUM_CACHE_TYPES] = {MAX_ALLOCATED};
+
+/* internal structure size vector for generic list allocations */
+long cache_type_sizes[NUM_CACHE_TYPES] = {CACHE_TYPE_SIZES};
+
+#ifndef ZERO
+#define ZERO 0
+#endif
+
+/* initialization lock & flag */
+/* cannot be statically allocated if using pthreads */
+static RVM_MUTEX        free_lists_init_lock = MUTEX_INITIALIZER;
+static rvm_bool_t       free_lists_inited = rvm_false;
+/*  Routines to allocate and manipulate the doubly-linked circular lists
+    used in RVM (derived from rpc2 routines)
+
+    List headers always use the list_entry_t structure and maintain a count of
+    elements on the list.  Headers can be statically or dynamically allocated,
+    but must be initialized with init_list_header before use.
+*/
+/* routine to initialize a list header
+   can be statically or dynamically allocated
+*/
+void init_list_header(whichlist,struct_id)
+    list_entry_t    *whichlist;         /* pointer to list header */
+    struct_id_t     struct_id;          /* list type */
+    {
+    whichlist->nextentry = whichlist;   /* pointers are to self now */
+    whichlist->preventry = whichlist;
+    whichlist->struct_id = struct_id;   /* set the type */
+    whichlist->list.length = 0;         /* list is empty */
+    whichlist->is_hdr = rvm_true;       /* mark header */
+    }
+
+/*  routine to allocate typed list cells
+    creates 1 entry of id type & returns address of cell
+*/
+static list_entry_t *malloc_list_entry(id)
+    struct_id_t     id;
+    {
+    register list_entry_t    *cell;
+
+    /* allocate the element */
+    cell = (list_entry_t *) calloc(1, cache_type_sizes[ID_INDEX(id)]);
+    assert(cell != NULL);
+    type_counts[ID_INDEX(id)] ++;       /* count allocations */
+
+    cell->struct_id = id;               /* cell type */
+    cell->is_hdr = rvm_false;           /* is not a header */
+
+    return cell;
+    }
+/*  generic routine to move elements between lists
+    the types of lists must be the same if both from & to ptrs are not null.
+    if cell is NULL, the 1st entry in fromptr list is selected
+    if fromptr is NULL, victim must not be NULL.
+    if fromptr is not null, from->list.length is decremented.
+    if toptr is not null, victim is appended & to->list.length is incremented.
+    in all cases a pointer to the moved entry is returned.
+*/
+list_entry_t *move_list_entry(fromptr, toptr, victim)
+    register list_entry_t *fromptr;     /* from list header */
+    register list_entry_t *toptr;       /* to list header */
+    register list_entry_t *victim;      /* pointer to entry to be moved */
+    {
+
+    if (!fromptr && victim)
+	fromptr = victim->list.name;
+
+    if (fromptr)
+        {
+        assert(fromptr->is_hdr);
+        if ((victim == NULL) && LIST_EMPTY((*fromptr)))
+            victim = malloc_list_entry(fromptr->struct_id);
+        else
+            {
+            if (victim == NULL)         /* choose 1st if no victim */
+                victim = fromptr->nextentry;
+            assert(!victim->is_hdr);
+            assert(victim->list.name == fromptr);
+            /* remque((void *)victim); */ /* unlink from first list */
+            if (victim->nextentry)
+                victim->nextentry->preventry = victim->preventry;
+            if (victim->preventry)
+                victim->preventry->nextentry = victim->nextentry;
+            victim->nextentry = victim->preventry = NULL;
+
+            fromptr->list.length --;
+            }
+        }
+    else
+        {
+        assert(victim != NULL);
+        assert(!victim->is_hdr);
+        assert(toptr != NULL);
+        }
+
+    if (toptr != NULL)
+        {
+        assert(toptr->is_hdr);
+        assert(victim->struct_id == toptr->struct_id);
+        victim->list.name = toptr;
+        /* insque((void *)victim,(void *)toptr->preventry); */ /* insert at tail of second list */
+        victim->preventry = toptr->preventry;
+        victim->nextentry = toptr;
+        victim->preventry->nextentry = toptr->preventry = victim;
+
+        toptr->list.length ++;
+        }
+    else victim->list.name = NULL;
+
+    return victim;
+    }
+/* list insertion routine */
+void insert_list_entry(entry,new_entry)
+    register list_entry_t *entry;       /* existing list entry */
+    register list_entry_t *new_entry;   /* entry to insert after entry */
+    {
+    list_entry_t        *list_hdr;      /* header of target list */
+
+    /* basic sanity checks */
+    assert(!new_entry->is_hdr);
+    assert(new_entry->struct_id == entry->struct_id);
+
+    /* discover header of target list */
+    if (entry->is_hdr)
+        list_hdr = entry;
+    else
+        list_hdr = entry->list.name;
+
+    /* further sanity checks */
+    assert(list_hdr != NULL);
+    assert(list_hdr->is_hdr);
+    assert(new_entry->struct_id == list_hdr->struct_id);
+
+    /* remove list_entry from any list it might be on */
+    if (new_entry->list.name)
+        (void)move_list_entry(NULL, NULL, new_entry);
+
+    /* do insertion into target list */
+    new_entry->list.name = list_hdr;
+    new_entry->nextentry = entry->nextentry;
+    entry->nextentry = new_entry;
+    new_entry->preventry = entry;
+    new_entry->nextentry->preventry = new_entry;
+    list_hdr->list.length++;
+
+    }
+/* internal types free lists support */
+
+/* initialization -- call once at initialization
+   free lists will be initialized and be pre-allocated with the number of
+   elements specified in NUM_PRE_ALLOCATED (from rvm_private.h)
+*/
+static void init_free_lists()
+    {
+    list_entry_t    *cell;
+    int             i,j;
+
+    assert(!free_lists_inited);
+    for (i = 0; i < ID_INDEX(struct_last_cache_id); i++)
+        {
+        init_list_header(&free_lists[i],INDEX_ID(i));
+        mutex_init(&free_lists_locks[i]);
+        for (j = 0; j < pre_alloc[i]; j++)
+            {
+            cell = malloc_list_entry(INDEX_ID(i));
+            assert(cell != NULL);
+            (void)move_list_entry(NULL,&free_lists[i],cell);
+            }
+        }
+    }
+
+/* get a cell from free list */
+list_entry_t *alloc_list_entry(id)
+    struct_id_t     id;
+    {
+    list_entry_t    *cell;
+
+    assert((((long)id > (long)struct_first_id) &&
+           ((long)id < (long)struct_last_cache_id)));
+
+    CRITICAL(free_lists_locks[ID_INDEX(id)],
+        {
+        cell = move_list_entry(&free_lists[ID_INDEX(id)],
+                               NULL,NULL);
+        });
+
+    return cell;
+    }
+/* kill cell */
+static void kill_list_entry(cell)
+    list_entry_t    *cell;
+    {
+    assert(cell != NULL);
+
+    /* unlink from present list */
+    if (cell->list.name)
+        (void)move_list_entry(NULL, NULL, cell);
+
+    /* terminate with extreme prejudice */
+    type_counts[ID_INDEX(cell->struct_id)] --;
+    free((char *)cell);
+    }
+
+/* move cell to free list
+   will remove cell from any list that it is on before freeing
+*/
+static void free_list_entry(cell)
+    register list_entry_t    *cell;
+    {
+    int id_index;
+    assert(cell != NULL);
+    assert((((long)cell->struct_id>(long)struct_first_id) &&
+           ((long)cell->struct_id<(long)struct_last_cache_id)));
+
+
+    id_index = ID_INDEX(cell->struct_id);
+    CRITICAL(free_lists_locks[id_index],
+        {                               /* begin free_list_lock crit sec */
+        if (max_alloc[id_index] >
+            free_lists[id_index].list.length)
+            {
+             /* move to appropriate free list */
+            (void)move_list_entry(cell->list.name,
+                              &free_lists[id_index],
+                              cell);
+            }
+        else
+            /* kill if enough of this type cached */
+            kill_list_entry(cell);
+        });                             /* end free_list_lock crit sec */
+    }
+/* clear free lists */
+void clear_free_list(id)
+    struct_id_t     id;                 /* type of free list */
+    {
+    list_entry_t    *cell;
+
+    CRITICAL(free_lists_locks[ID_INDEX(id)],
+        {                               /* begin free_list_lock crit sec */
+        while (LIST_NOT_EMPTY(free_lists[ID_INDEX(id)]))
+            {
+            cell = free_lists[ID_INDEX(id)].nextentry;
+            kill_list_entry(cell);
+            }
+        });                             /* end free_list_lock crit sec */
+    }
+
+void clear_free_lists()
+    {
+    int             i;
+
+    for (i = 0; i < ID_INDEX(struct_last_cache_id); i++)
+        clear_free_list(INDEX_ID(i));
+    }
+/* unique name generator */
+/* Cannot be statically allocated in pthreads */
+static RVM_MUTEX     uname_lock = MUTEX_INITIALIZER;
+static struct timeval   uname = {0,0};
+
+void make_uname(new_uname)
+    struct timeval  *new_uname;
+    {
+    /* generate a uname */
+    CRITICAL(uname_lock,
+        {
+        new_uname->tv_sec = uname.tv_sec;
+        new_uname->tv_usec = uname.tv_usec++;
+        if (uname.tv_usec >= 1000000)
+            {
+            uname.tv_sec++;
+            uname.tv_usec = 0;
+            }
+        });
+    }
+
+/* uname initialization */
+long init_unames()
+    {
+    struct timeval  new_uname;
+    long            retval;
+
+    retval= gettimeofday(&new_uname,(struct timezone *)NULL);
+    if ( retval ) {
+	    printf("init_unames: retval %ld\n", retval);
+	    perror("init_names:");
+	    return retval;
+    }
+
+    CRITICAL(uname_lock,
+        {
+        if (TIME_GTR(new_uname,uname))
+            uname = new_uname;
+        });
+
+    return 0;
+    }
+
+/* module initialization */
+/* Locks cannot be statically allocated in pthreads. */
+long init_utils()
+    {
+    CRITICAL(free_lists_init_lock,
+        {
+        if (!free_lists_inited)
+            {
+            init_free_lists();
+            free_lists_inited = rvm_true;
+            }
+        });
+
+    return init_unames();
+    }
+/* time value arithmetic */
+struct timeval add_times(x,y)
+    struct timeval  *x;
+    struct timeval  *y;
+    {
+    struct timeval  tmp;
+
+    tmp.tv_sec = x->tv_sec + y->tv_sec;
+    tmp.tv_usec = x->tv_usec + y->tv_usec;
+    if (tmp.tv_usec >= 1000000)
+        {
+        tmp.tv_sec++;
+        tmp.tv_usec -= 1000000;
+        }
+    return tmp;
+    }
+
+struct timeval sub_times(x,y)
+    struct timeval  *x;
+    struct timeval  *y;
+    {
+    struct timeval  tmp;
+
+    tmp = *x;
+    if (tmp.tv_usec < y->tv_usec)
+        {
+        tmp.tv_sec--;
+        tmp.tv_usec += 1000000;
+        }
+    tmp.tv_usec -= y->tv_usec;
+    tmp.tv_sec -= y->tv_sec;
+    return tmp;
+    }
+
+/* round time to seconds */
+long round_time(x)
+    struct timeval  *x;
+    {
+    if (x->tv_usec >= 500000)
+        return x->tv_sec+1;
+
+    return x->tv_sec;
+    }
+/* region descriptor allocator/finalizer */
+region_t *make_region()
+    {
+    region_t    *region;
+
+    region = (region_t *)alloc_list_entry(region_id);
+    if (region != NULL)
+        {
+        init_rw_lock(&region->region_lock);
+        mutex_init(&region->count_lock);
+        }
+    return region;
+    }
+
+void free_region(region)
+    region_t    *region;
+    {
+    assert(region->links.struct_id == region_id);
+    assert(LOCK_FREE(region->count_lock));
+
+    rw_lock_clear(&region->region_lock);
+    mutex_clear(&region->count_lock);
+    free_list_entry((list_entry_t *) region);
+    }
+/* construct full path name for file names */
+char *make_full_name(dev_str,dev_name,retval)
+    char            *dev_str;           /* device name */
+    char            *dev_name;          /* device name buffer for descriptor */
+    rvm_return_t    *retval;            /* return code */
+    {
+    char            wd_name[MAXPATHLEN+1]; /* current working directory */
+    long            wd_len = 0;         /* working dir string length */
+    long            len;                /* length of total device path */
+
+    *retval = RVM_SUCCESS;
+    len = strlen(dev_str) + 1;          /* one extra for null terminator */
+
+    /* see if working directory must be added to device name */
+    if (*dev_str != '/')
+        {
+        if (getcwd(wd_name, sizeof(wd_name)) == 0)
+            assert(rvm_false);
+        wd_len = strlen(wd_name);
+        len += (wd_len+1);              /* one extra for '/' */
+        }
+    if (len > (MAXPATHLEN+1))
+        {
+        *retval = RVM_ENAME_TOO_LONG;
+        return NULL;
+        }
+
+    /* allocate buffer, if necessary, and copy full path name */
+    if (dev_name == NULL)
+        if ((dev_name=malloc(len)) == NULL)
+            {
+            *retval = RVM_ENO_MEMORY;
+            return NULL;
+            }
+
+    *dev_name = 0;
+    if (wd_len != 0)
+        {
+        (void)strcpy(dev_name,wd_name);
+        dev_name[wd_len] = '/';
+        dev_name[wd_len+1] = 0;
+        }
+    (void)strcat(dev_name,dev_str);
+
+    return dev_name;
+    }
+/* device descriptor initializer */
+rvm_return_t dev_init(dev,dev_str)
+    device_t        *dev;               /* device descriptor */
+    char            *dev_str;           /* device name */
+    {
+    rvm_return_t    retval;
+
+    /* set device name */
+    if (dev_str != NULL)
+        {
+        dev->name = make_full_name(dev_str,NULL,&retval);
+        if (retval != RVM_SUCCESS) return retval;
+        dev->name_len = strlen(dev->name)+1;
+        }
+
+    dev->iov = NULL;
+    dev->iov_length = 0;
+    dev->raw_io = rvm_false;
+    dev->read_only = rvm_false;
+    dev->wrt_buf = NULL;
+    dev->buf_start = NULL;
+    dev->buf_end = NULL;
+    dev->ptr = NULL;
+    RVM_ZERO_OFFSET(dev->sync_offset);
+    dev->pad_buf = NULL;
+    dev->pad_buf_len = 0;
+
+    return RVM_SUCCESS;
+    }
+/* segment descriptor allocator/finalizer */
+seg_t *make_seg(seg_dev_name,retval)
+    char            *seg_dev_name;      /* segment device name */
+    rvm_return_t    *retval;            /* return code */
+    {
+    seg_t           *seg;
+
+    /* initialize segment descriptor */
+    seg = (seg_t *)alloc_list_entry(seg_id);
+    if (seg != NULL)
+        {
+        /* initialize segment device */
+        if ((*retval=dev_init(&seg->dev,seg_dev_name)) != RVM_SUCCESS)
+            {
+            free(seg);
+            return NULL;                /* no space for device name */
+            }
+
+        /* initialize the lists & locks*/
+        mutex_init(&seg->seg_lock);
+        mutex_init(&seg->dev_lock);
+        init_list_header(&seg->map_list,region_id);
+        init_list_header(&seg->unmap_list,region_id);
+        }
+
+    return seg;
+    }
+
+void free_seg(seg)
+    seg_t       *seg;
+    {
+    assert(seg->links.struct_id == seg_id);
+
+    /* all lists should be empty and locks free */
+    assert(LIST_EMPTY(seg->map_list));
+    assert(LIST_EMPTY(seg->unmap_list));
+    assert(LOCK_FREE(seg->seg_lock));
+    assert(LOCK_FREE(seg->dev_lock));
+
+    mutex_clear(&seg->seg_lock);
+    mutex_clear(&seg->dev_lock);
+    if (seg->dev.name != NULL)
+        {
+        free(seg->dev.name);            /* free device name char array */
+        seg->dev.name = NULL;
+        }
+    free_list_entry(&seg->links);
+    }
+/* segemnt dictionary finalizer */
+void free_seg_dict_vec(log)
+    log_t           *log;
+    {
+    int             i;                  /* loop counter */
+
+    if (log->seg_dict_vec != NULL)
+        {
+        /* free tree roots */
+        for (i=0; i < log->seg_dict_len; i++)
+            clear_tree_root(&log->seg_dict_vec[i].mod_tree);
+
+        free((char *)log->seg_dict_vec);
+        log->seg_dict_vec = NULL;
+        log->seg_dict_len = 0;
+        }
+    }
+/* log descriptor finalizer */
+void free_log(log)
+    log_t           *log;
+    {
+    assert(log->links.struct_id == log_id);
+    assert(LIST_EMPTY(log->tid_list));     /* should not be any transactions
+                                              now */
+    assert(LIST_EMPTY(log->flush_list));   /* should not be any queued no_flush
+                                              transactions either */
+    assert(LIST_EMPTY(log->special_list)); /* no special log records should
+                                              be left */
+    assert(LOCK_FREE(log->dev_lock));      /* all locks should be free */
+    assert(LOCK_FREE(log->tid_list_lock));
+    assert(LOCK_FREE(log->flush_list_lock));
+    assert(LOCK_FREE(log->special_list_lock));
+    assert(RW_LOCK_FREE(log->flush_lock));
+    assert(LOCK_FREE(log->truncation_lock));
+    assert(LOCK_FREE(log->daemon.lock));
+    assert((log->daemon.thread != ZERO)
+           ? log->daemon.state == terminate : 1);
+
+    /* finalizer control structures */
+    mutex_clear(&log->dev_lock);
+    mutex_clear(&log->tid_list_lock);
+    mutex_clear(&log->flush_list_lock);
+    mutex_clear(&log->special_list_lock);
+    rw_lock_clear(&log->flush_lock);
+    mutex_clear(&log->truncation_lock);
+    mutex_clear(&log->daemon.lock);
+    condition_clear(&log->daemon.code);
+    condition_clear(&log->daemon.flush_flag);
+    condition_clear(&log->daemon.wake_up);
+
+    /* free malloc-ed buffers */
+    if (log->dev.name != NULL)
+        free(log->dev.name);            /* kill name string */
+    if (log->dev.iov != NULL)
+        free((char *)log->dev.iov);     /* kill io vector */
+    if (log->dev.wrt_buf != NULL)       /* kill raw io gather write buffer */
+        page_free(log->dev.wrt_buf,log->dev.wrt_buf_len);
+    log->dev.wrt_buf_len = 0;
+    log->dev.name = NULL;
+    log->dev.iov = NULL;
+    free_log_buf(log);                  /* kill recovery buffers */
+
+    free_list_entry(&log->links);       /* free descriptor */
+    }
+/* log descriptor allocation */
+log_t *make_log(log_dev_name,retval)
+    char            *log_dev_name;      /* device name */
+    rvm_return_t    *retval;            /* return code */
+    {
+    log_t           *log;
+    log_buf_t       *log_buf;
+
+    /* initialize log descriptor */
+    if ((log = (log_t *)alloc_list_entry(log_id)) != NULL)
+        {
+        /* init device and status */
+        if ((*retval=dev_init(&log->dev,log_dev_name)) != RVM_SUCCESS)
+            {
+            free(log);
+            return NULL;                /* no space for device name */
+            }
+	/* first and foremost */
+	log->trunc_thread = (cthread_t) 0;
+
+        log->status.valid = rvm_false;
+        log->status.log_empty = rvm_false;
+        log->status.trunc_state = 0;
+        log->status.flush_state = 0;
+
+        /* init log flush structures */
+        log->trans_hdr.rec_hdr.struct_id = trans_hdr_id;
+        log->rec_end.rec_hdr.struct_id = rec_end_id;
+        log->log_wrap.rec_hdr.struct_id = log_wrap_id;
+        log->log_wrap.struct_id2 = log_wrap_id;  /* for scan_wrap_reverse() */
+        log->log_wrap.rec_hdr.rec_length = sizeof(log_wrap_t);
+
+        /* init recovery buffer and dictionary */
+        log_buf = &log->log_buf;
+        log_buf->buf = NULL;
+        log_buf->length = 0;
+        RVM_ZERO_OFFSET(log_buf->buf_len);
+        log_buf->aux_buf = NULL;
+        log_buf->aux_length = 0;
+        log_buf->split_ok = rvm_false;
+        log->seg_dict_vec = NULL;
+        log->seg_dict_len = 0;
+        log->in_recovery = rvm_false;
+        mutex_init(&log->truncation_lock);
+        init_rw_lock(&log->flush_lock);
+        log_buf->prev_rec_num = 0;
+        ZERO_TIME(log_buf->prev_timestamp);
+        log_buf->prev_direction = rvm_false;
+
+        /* init lists and locks */
+        mutex_init(&log->dev_lock);
+        mutex_init(&log->tid_list_lock);
+        init_list_header(&log->tid_list,int_tid_id);
+        mutex_init(&log->flush_list_lock);
+        init_list_header(&log->flush_list,int_tid_id);
+        mutex_init(&log->special_list_lock);
+        init_list_header(&log->special_list,log_special_id);
+
+        /* init daemon control structure */
+        log->daemon.thread = ZERO;
+        mutex_init(&log->daemon.lock);
+        condition_init(&log->daemon.code);
+        condition_init(&log->daemon.flush_flag);
+        condition_init(&log->daemon.wake_up);
+        log->daemon.state = rvm_idle;
+        }
+
+    return log;
+    }
+/* log special types allocation/deallocation */
+log_special_t *make_log_special(special_id,length)
+    struct_id_t     special_id;         /* id of special type */
+    rvm_length_t    length;             /* length of type-specific allocation */
+    {
+    log_special_t   *special;           /* ptr to descriptor allocated */
+    char            *buf=NULL;          /* type-specific buffer */
+
+    if ((special=(log_special_t *)alloc_list_entry(log_special_id))
+        != NULL)
+        {
+        special->rec_hdr.struct_id = special_id; /* set "real" type */
+
+        /* buffer allocation */
+        if ((length=ROUND_TO_LENGTH(length)) != 0)
+            if ((buf = calloc(1, (unsigned)length)) == NULL)
+                {
+                free_list_entry((list_entry_t *)special);
+                return NULL;
+                }
+        special->rec_hdr.rec_length = LOG_SPECIAL_SIZE + length;
+
+        /* type specific initialization */
+        switch (special_id)
+            {
+          case log_seg_id:
+            special->special.log_seg.name=buf;
+            break;
+          default: assert(rvm_false);       /* should never happen */
+            }
+        }
+
+    return special;
+    }
+void free_log_special(special)
+    log_special_t   *special;           /* ptr to descriptor allocated */
+    {
+    assert(special->links.struct_id == log_special_id);
+
+    /* type specific finalization */
+    switch (special->rec_hdr.struct_id)
+        {
+      case log_seg_id:
+        if (special->special.log_seg.name != NULL)
+            {
+            free(special->special.log_seg.name);
+            special->special.log_seg.name = NULL;
+            }
+        break;
+      default: assert(rvm_false);       /* should not happen */
+        }
+
+    free_list_entry((list_entry_t *)special);
+
+    }
+/* range descriptor allocator/finalizer */
+range_t *make_range()
+    {
+    register range_t    *range;
+
+    if ((range = (range_t *)alloc_list_entry(range_id)) != NULL)
+        {
+        range->links.node.lss = NULL;
+        range->links.node.gtr = NULL;
+        range->links.node.bf = 0;
+        range->nvaddr = NULL;
+        range->data = NULL;
+        range->data_len = 0;
+        range->nv.rec_hdr.struct_id = nv_range_id;
+        range->nv.is_split = rvm_false;
+        }
+
+    return range;
+    }
+
+void free_range(range)
+    register range_t    *range;
+    {
+    assert(range->links.node.struct_id == range_id);
+
+    if (range->data != NULL)
+        {
+        free(range->data);
+        range->data = NULL;
+        range->nvaddr = NULL;
+        range->data_len = 0;
+        }
+
+    range->links.entry.list.name = NULL; /* not really on any list */
+    range->links.entry.is_hdr = rvm_false;
+    free_list_entry(&range->links.entry);
+    }
+/* internal transaction descriptor allocator/finalizer */
+
+int_tid_t *make_tid(mode)
+    rvm_mode_t      mode;               /* transaction begin mode */
+    {
+    register int_tid_t  *tid;
+
+    tid = (int_tid_t *)alloc_list_entry(int_tid_id);
+    if (tid != NULL)
+        {
+        make_uname(&tid->uname);
+        init_rw_lock(&tid->tid_lock);
+        init_tree_root(&tid->range_tree);
+        tid->x_ranges = NULL;
+        tid->x_ranges_alloc = 0;
+        tid->x_ranges_len = 0;
+        tid->n_coalesced = 0;
+        tid->range_elim = 0;
+        tid->trans_elim = 0;
+        RVM_ZERO_OFFSET(tid->range_overlap);
+        RVM_ZERO_OFFSET(tid->trans_overlap);
+        ZERO_TIME(tid->commit_stamp);
+        tid->flags = rvm_optimizations & RVM_ALL_OPTIMIZATIONS;
+        if (mode == restore) tid->flags |= RESTORE_FLAG;
+        tid->split_range.nv.rec_hdr.struct_id = nv_range_id;
+        tid->back_link = sizeof(trans_hdr_t);
+        }
+
+    return tid;
+    }
+
+void free_tid(tid)
+    register int_tid_t  *tid;
+    {
+    range_t           *range;
+
+    assert(tid->links.struct_id == int_tid_id);
+    rw_lock_clear(&tid->tid_lock);
+
+    /* free range list */
+    UNLINK_NODES_OF(tid->range_tree,range_t,range)
+        free_range((range_t *)range);
+    clear_tree_root(&tid->range_tree);
+
+    /* free search vector */
+    if (tid->x_ranges != NULL)
+        {
+        free(tid->x_ranges);
+        tid->x_ranges = NULL;
+        }
+
+    /* free tid */
+    free_list_entry(&tid->links);
+    }
+/* mem_region nodes for mapping */
+mem_region_t *make_mem_region()
+    {
+    register mem_region_t  *node;
+
+    node = (mem_region_t *)alloc_list_entry(mem_region_id);
+    if (node != NULL)
+        {
+        node->region = NULL;
+        }
+
+    return node;
+    }
+
+void free_mem_region(node)
+    register mem_region_t    *node;
+    {
+    assert(node->links.node.struct_id == mem_region_id);
+
+    /* free node */
+    node->links.entry.list.name = NULL; /* not really on any list */
+    node->links.entry.is_hdr = rvm_false;
+    free_list_entry(&node->links.entry);
+    }
+/* dev_region nodes for recovery */
+dev_region_t *make_dev_region()
+    {
+    register dev_region_t  *node;
+
+    node = (dev_region_t *)alloc_list_entry(dev_region_id);
+    if (node == NULL) return NULL;
+
+    node->nv_buf = NULL;
+    node->nv_ptr = NULL;
+    RVM_ZERO_OFFSET(node->log_offset);
+
+    return node;
+    }
+
+void free_dev_region(node)
+    register dev_region_t    *node;
+    {
+    assert(node->links.node.struct_id == dev_region_id);
+
+    /* free node */
+    node->links.entry.list.name = NULL; /* not really on any list */
+    node->links.entry.is_hdr = rvm_false;
+
+    if (node->nv_buf != NULL)
+        {
+        assert(node->nv_buf->struct_id == nv_buf_id);
+        if ((--(node->nv_buf->ref_cnt)) == 0)
+            {
+            free(node->nv_buf);
+            node->nv_buf = NULL;
+            node->nv_ptr = NULL;
+            }
+        }
+    free_list_entry(&node->links.entry);
+    }
+/* RVM exported structures support */
+
+static void free_export(cell,struct_id)
+    list_entry_t    *cell;
+    struct_id_t     struct_id;
+    {
+    cell->struct_id = struct_id;
+    cell->list.name = NULL;
+    cell->preventry = NULL;
+    cell->nextentry = NULL;
+    cell->is_hdr = rvm_false;
+    free_list_entry(cell);
+    }
+
+
+/* rvm_region_t functions    */
+rvm_region_t *rvm_malloc_region()
+    {
+    rvm_region_t    *new_rvm_region;
+
+    if (!free_lists_inited) (void)init_utils();
+    new_rvm_region = (rvm_region_t *)alloc_list_entry(region_rvm_id);
+    if (new_rvm_region != NULL)
+        {
+        rvm_init_region(new_rvm_region);
+        new_rvm_region->from_heap = rvm_true;
+        }
+    return new_rvm_region;
+    }
+
+void rvm_free_region(rvm_region)
+    rvm_region_t    *rvm_region;
+    {
+    if ((!bad_region(rvm_region))&&(free_lists_inited)&&
+        (rvm_region->from_heap))
+        free_export((list_entry_t *)rvm_region,region_rvm_id);
+    }
+void rvm_init_region(rvm_region)
+    rvm_region_t    *rvm_region;
+    {
+        BZERO((char *) rvm_region,sizeof(rvm_region_t));
+        rvm_region->struct_id = rvm_region_id;
+    }
+
+rvm_region_t *rvm_copy_region(rvm_region)
+    rvm_region_t    *rvm_region;
+    {
+    rvm_region_t    *new_rvm_region;
+
+    if (bad_region(rvm_region)) return NULL;
+    if (!free_lists_inited) (void)init_utils();
+
+    new_rvm_region = (rvm_region_t *)alloc_list_entry(region_rvm_id);
+    if (new_rvm_region != NULL)
+        {
+        (void)BCOPY((char *)rvm_region,(char *)new_rvm_region,
+                    sizeof(rvm_region_t));
+        new_rvm_region->from_heap = rvm_true;
+        }
+    return new_rvm_region;
+    }
+/* rvm_statistics_t functions */
+rvm_statistics_t *rvm_malloc_statistics()
+    {
+    rvm_statistics_t    *new_rvm_statistics;
+
+    new_rvm_statistics = (rvm_statistics_t *)
+                         alloc_list_entry(statistics_rvm_id);
+    if (new_rvm_statistics != NULL)
+        {
+        rvm_init_statistics(new_rvm_statistics);
+        new_rvm_statistics->from_heap = rvm_true;
+        }
+    return new_rvm_statistics;
+    }
+
+void rvm_free_statistics(rvm_statistics)
+    rvm_statistics_t    *rvm_statistics;
+    {
+    if ((!bad_statistics(rvm_statistics)) && (free_lists_inited)
+        && (rvm_statistics->from_heap))
+        free_export((list_entry_t *)rvm_statistics,statistics_rvm_id);
+    }
+void rvm_init_statistics(rvm_statistics)
+    rvm_statistics_t    *rvm_statistics;
+    {
+    if (rvm_statistics != NULL)
+        {
+        BZERO((char *)rvm_statistics,sizeof(rvm_statistics_t));
+        rvm_statistics->struct_id = rvm_statistics_id;
+        }
+    }
+
+rvm_statistics_t *rvm_copy_statistics(rvm_statistics)
+    rvm_statistics_t   *rvm_statistics;
+    {
+    rvm_statistics_t   *new_rvm_statistics;
+
+    if (bad_statistics(rvm_statistics)) return NULL;
+    if (!free_lists_inited) (void)init_utils();
+
+    new_rvm_statistics =
+        (rvm_statistics_t *)alloc_list_entry(statistics_rvm_id);
+    if (new_rvm_statistics != NULL)
+        {
+        (void) BCOPY((char *)rvm_statistics,(char *)new_rvm_statistics,
+                     sizeof(rvm_statistics_t));
+        new_rvm_statistics->from_heap = rvm_true;
+        }
+    return new_rvm_statistics;
+    }
+/* rvm_options_t functions */
+
+rvm_options_t *rvm_malloc_options()
+    {
+    rvm_options_t    *new_rvm_options;
+
+    if (!free_lists_inited) (void)init_utils();
+    new_rvm_options = (rvm_options_t *)
+        alloc_list_entry(options_rvm_id);
+    if (new_rvm_options != NULL)
+        {
+        rvm_init_options(new_rvm_options);
+        new_rvm_options->from_heap = rvm_true;
+        }
+    return new_rvm_options;
+    }
+
+void rvm_free_options(rvm_options)
+    rvm_options_t    *rvm_options;
+    {
+    if (!bad_options(rvm_options, rvm_false) && free_lists_inited &&
+	rvm_options->from_heap)
+        {
+        /* release tid_array */
+        if (rvm_options->tid_array != NULL)
+            {
+            free((char *)rvm_options->tid_array);
+            rvm_options->tid_array = (rvm_tid_t *)NULL;
+            rvm_options->n_uncommit = 0;
+            }
+
+        /* free options record */
+        free_export((list_entry_t *)rvm_options,options_rvm_id);
+        }
+    }
+void rvm_init_options(rvm_options)
+    rvm_options_t    *rvm_options;
+    {
+    if (rvm_options != NULL)
+        {
+        BZERO((char *)rvm_options,sizeof(rvm_options_t));
+        rvm_options->struct_id = rvm_options_id;
+        rvm_options->truncate = TRUNCATE;
+        rvm_options->recovery_buf_len = RECOVERY_BUF_LEN;
+        rvm_options->flush_buf_len = FLUSH_BUF_LEN;
+        rvm_options->max_read_len = MAX_READ_LEN;
+        rvm_options->create_log_file = rvm_false;
+        RVM_ZERO_OFFSET(rvm_options->create_log_size);
+        rvm_options->create_log_mode = 0600;
+        }
+    }
+
+rvm_options_t *rvm_copy_options(rvm_options)
+    rvm_options_t   *rvm_options;
+    {
+    rvm_options_t   *new_rvm_options;
+
+    if (bad_options(rvm_options, rvm_true)) return NULL;
+    if (!free_lists_inited) (void)init_utils();
+
+    new_rvm_options = (rvm_options_t *)
+        alloc_list_entry(options_rvm_id);
+    if (new_rvm_options != NULL)
+        {
+        (void) BCOPY((char *)rvm_options,(char *)new_rvm_options,
+                     sizeof(rvm_options_t));
+        new_rvm_options->from_heap = rvm_true;
+        }
+    return new_rvm_options;
+    }
+/*      rvm_tid_t functions    */
+
+rvm_tid_t *rvm_malloc_tid()
+    {
+    rvm_tid_t    *new_rvm_tid;
+
+    if (!free_lists_inited) (void)init_utils();
+    new_rvm_tid = (rvm_tid_t *)alloc_list_entry(tid_rvm_id);
+    if (new_rvm_tid != NULL)
+        {
+        rvm_init_tid(new_rvm_tid);
+        new_rvm_tid->from_heap = rvm_true;
+        }
+    return new_rvm_tid;
+    }
+
+void rvm_free_tid(rvm_tid)
+    rvm_tid_t    *rvm_tid;
+    {
+    if ((!bad_tid(rvm_tid))&&(free_lists_inited)&&
+        (rvm_tid->from_heap))
+        free_export((list_entry_t *)rvm_tid,tid_rvm_id);
+    }
+
+void rvm_init_tid(rvm_tid_t *rvm_tid)
+{
+	if (rvm_tid != NULL) {
+		BZERO((char *)rvm_tid,sizeof(rvm_tid_t));
+		rvm_tid->struct_id = rvm_tid_id;
+        }
+}
+
+rvm_tid_t *rvm_copy_tid(rvm_tid)
+    rvm_tid_t    *rvm_tid;
+    {
+    rvm_tid_t    *new_rvm_tid;
+
+    if (bad_tid(rvm_tid)) return NULL;
+    if (!free_lists_inited) (void)init_utils();
+
+    new_rvm_tid = (rvm_tid_t *)alloc_list_entry(tid_rvm_id);
+    if (new_rvm_tid != NULL)
+        {
+        (void) BCOPY((char *)rvm_tid,(char *)new_rvm_tid,
+                     sizeof(rvm_tid_t));
+        new_rvm_tid->from_heap = rvm_true;
+        }
+    return new_rvm_tid;
+    }
+/* RVM User enumeration type print name support */
+static char *return_codes[(long)rvm_last_code-(long)rvm_first_code-1] =
+    {
+    "RVM_EINIT","RVM_EINTERNAL","RVM_EIO","RVM_EPLACEHOLDER","RVM_ELOG",
+    "RVM_ELOG_VERSION_SKEW","RVM_EMODE","RVM_ENAME_TOO_LONG",
+    "RVM_ENO_MEMORY","RVM_ENOT_MAPPED","RVM_EOFFSET",
+    "RVM_EOPTIONS","RVM_EOVERLAP","RVM_EPAGER","RVM_ERANGE",
+    "RVM_EREGION","RVM_EREGION_DEF","RVM_ESRC","RVM_ESTATISTICS",
+    "RVM_ESTAT_VERSION_SKEW","RVM_ETERMINATED","RVM_ETHREADS",
+    "RVM_ETID","RVM_ETOO_BIG","RVM_EUNCOMMIT",
+    "RVM_EVERSION_SKEW","RVM_EVM_OVERLAP"
+     };
+
+static char *rvm_modes[(long)rvm_last_mode-(long)rvm_first_mode-1] =
+    {
+    "restore","no_restore","flush","no_flush"
+    };
+
+static char *rvm_types[(long)rvm_last_struct_id-(long)rvm_first_struct_id-1] =
+    {
+    "rvm_region_t","rvm_options_t","rvm_tid_t","rvm_statistics_id"
+    };
+/* RVM enumeration type print name routines */
+char *rvm_return(code)
+    rvm_return_t    code;
+    {
+    if (code == RVM_SUCCESS) return "RVM_SUCCESS";
+
+    if (((long)code > (long)rvm_first_code) &&
+        ((long)code < (long)rvm_last_code))
+        return return_codes[(long)code-(long)rvm_first_code-1];
+    else
+        return "Invalid RVM return code";
+    }
+char *rvm_mode(mode)
+    rvm_mode_t      mode;
+    {
+    if (((long)mode > (long)rvm_first_mode) &&
+        ((long)mode < (long)rvm_last_mode))
+        return rvm_modes[(long)mode-(long)rvm_first_mode-1];
+    else
+        return "Invalid RVM transaction mode";
+    }
+char *rvm_type(id)
+    rvm_struct_id_t   id;
+    {
+    if (((long)id > (long)rvm_first_struct_id) &&
+        ((long)id < (long)rvm_last_struct_id))
+        return rvm_types[(long)id-(long)rvm_first_struct_id-1];
+    else
+        return "Invalid RVM structure type";
+    }
+/* Byte-aligned checksum and move functions */
+
+/* zero-pad unused bytes of word */
+rvm_length_t zero_pad_word(word,addr,leading)
+    rvm_length_t    word;               /* value to be zero-padded */
+    char            *addr;              /* address of 1st/last byte */
+    rvm_bool_t      leading;            /* true if leading bytes are zeroed */
+    {
+    char            *word_array = (char *)&word; /* byte access of
+						    word value */
+    int             skew = BYTE_SKEW(addr);
+    int             i;
+
+    if (leading)                        /* zero pad leading bytes */
+        {
+        for (i=sizeof(rvm_length_t)-1; i>0; i--)
+            if (i <= skew) word_array[i-1] = 0;
+        }
+    else                                /* zero pad trailing bytes */
+        {
+        for (i=0; i<(sizeof(rvm_length_t)-1); i++)
+          if (i >= skew) word_array[i+1] = 0;
+        }
+
+    return word;
+    }
+/* checksum function: forms checksum of arbitrarily aligned range
+   by copying preceeding, trailing bytes to make length 0 mod length size */
+rvm_length_t chk_sum(nvaddr,len)
+    char            *nvaddr;            /* address of 1st byte */
+    rvm_length_t    len;                /* byte count */
+    {
+    rvm_length_t    *base;              /* 0 mod sizeof(rvm_length_t) addr */
+    rvm_length_t    length;             /* number of words to sum */
+    rvm_length_t    chk_sum;            /* check sum temp */
+    rvm_length_t    i;
+
+    if (len == 0) return 0;
+
+    /* get zero-byte aligned base address & length of region */
+    base = (rvm_length_t *)CHOP_TO_LENGTH(nvaddr);
+    length = (ALIGNED_LEN(nvaddr,len)/sizeof(rvm_length_t)) - 1;
+
+    /* process boundary words */
+    chk_sum = zero_pad_word(*base,nvaddr,rvm_true);
+    if (length >= 2)
+        chk_sum += zero_pad_word(base[length],&nvaddr[len-1],rvm_false);
+    if (length <= 1) return chk_sum;
+
+    /* form check sum of remaining full words */
+    for (i=1; i < length; i++)
+        chk_sum += base[i];
+
+    return chk_sum;
+    }
+/* copy arbitrarily aligned range, maintaining 1st src byte alignment */
+void src_aligned_bcopy(src,dest,len)
+    char            *src;               /* source address */
+    char            *dest;              /* destination address */
+    rvm_length_t    len;                /* length of range */
+    {
+
+    if (len != 0)
+        (void)BCOPY(src,
+                    RVM_ADD_LENGTH_TO_ADDR(dest,BYTE_SKEW(src)),
+                    len);
+
+    }
+
+/* copy arbitrarily aligned range, maintaining 1st dest byte alignment */
+void dest_aligned_bcopy(src,dest,len)
+    char            *src;               /* source address */
+    char            *dest;              /* destination address */
+    rvm_length_t    len;                /* length of range */
+    {
+
+    if (len != 0)
+        (void)BCOPY(RVM_ADD_LENGTH_TO_ADDR(src,BYTE_SKEW(dest)),
+                    dest,len);
+
+    }
+/* rw_lock functions */
+
+/* rw_lock initializer */
+void init_rw_lock(rwl)
+    rw_lock_t   *rwl;
+    {
+
+    mutex_init(&rwl->mutex);
+    init_list_header(&rwl->queue,rw_qentry_id);
+    rwl->read_cnt = 0;
+    rwl->write_cnt = 0;
+    rwl->lock_mode = f;
+    }
+
+/* rw_lock finalizer */
+void rw_lock_clear(rwl)
+    rw_lock_t   *rwl;
+    {
+
+    assert(LOCK_FREE(rwl->mutex));
+    assert(LIST_EMPTY(rwl->queue));
+    assert(rwl->read_cnt == 0);
+    assert(rwl->write_cnt == 0);
+    assert(rwl->lock_mode == f);
+
+    mutex_clear(&rwl->mutex);
+    }
+void rw_lock(rwl,mode)
+    rw_lock_t       *rwl;               /* ptr to rw_lock structure */
+    rw_lock_mode_t  mode;               /* r or w */
+    {
+#ifdef RVM_USELWP
+    if (mode == r) ObtainReadLock(&rwl->mutex);
+    else           ObtainWriteLock(&rwl->mutex);
+#else
+    rw_qentry_t      q;                 /* queue entry */
+
+    CRITICAL(rwl->mutex,                /* begin rw_lock mutex crit sec */
+        {
+        assert((mode == r) || (mode == w));
+        assert(rwl->read_cnt >= 0);
+        assert((rwl->write_cnt >= 0) && (rwl->write_cnt <= 1));
+        assert((rwl->write_cnt > 0) ? (rwl->read_cnt == 0) : 1);
+        assert((rwl->read_cnt > 0) ? (rwl->write_cnt == 0) : 1);
+
+        /* see if must block */
+        if (((mode == w) && ((rwl->read_cnt+rwl->write_cnt) != 0))
+            || ((mode == r) && (rwl->write_cnt != 0))
+            || (LIST_NOT_EMPTY(rwl->queue))) /* this term prevents starvation
+                                                of writers by readers */
+            {
+            /* must block: initialize queue entry & put on lock queue */
+            q.links.struct_id = rw_qentry_id;
+            q.links.is_hdr = rvm_false;
+            q.links.list.name = NULL;
+            condition_init(&q.wait);
+            q.mode = mode;
+            (void)move_list_entry(NULL,&rwl->queue,(list_entry_t *)&q);
+
+            /* wait to be signaled when access ready */
+            condition_wait(&q.wait,&rwl->mutex);
+            assert(rwl->lock_mode == mode);
+            assert((mode == w) ?
+                   ((rwl->write_cnt==1) && (rwl->read_cnt==0)) : 1);
+            assert((mode == r) ?
+                   ((rwl->write_cnt==0) && (rwl->read_cnt>=1)) : 1);
+            }
+        else
+            {                           /* immediate access: set the lock */
+            assert((rwl->lock_mode == r) || (rwl->lock_mode == f));
+            if (mode == r) rwl->read_cnt++;
+            else rwl->write_cnt++;
+            rwl->lock_mode = mode;
+            }
+        });                             /* end rw_lock mutex crit sec */
+#endif
+    }
+void rw_unlock(rwl,mode)
+    rw_lock_t       *rwl;               /* ptr to rw_lock structure */
+    rw_lock_mode_t  mode;               /* r or w (for consistency chk only) */
+    {
+#ifdef RVM_USELWP
+    if (mode == r) ReleaseReadLock(&rwl->mutex);
+    else           ReleaseWriteLock(&rwl->mutex);
+#else
+    rw_qentry_t      *q,*old_q;         /* thread queue elements */
+
+    CRITICAL(rwl->mutex,                /* begin rw_lock mutex crit sec */
+        {
+        assert((mode == r) || (mode == w));
+        assert(rwl->lock_mode == mode);
+        assert(rwl->read_cnt >= 0);
+        assert((rwl->write_cnt >= 0) && (rwl->write_cnt <= 1));
+        assert((rwl->write_cnt > 0) ? (rwl->read_cnt == 0) : 1);
+        assert((rwl->read_cnt > 0) ? (rwl->write_cnt == 0) : 1);
+
+        /* update lock counts */
+        if (rwl->lock_mode == r)
+            rwl->read_cnt--;
+        else rwl->write_cnt--;
+
+        /* clear lock mode if lock free */
+        if ((rwl->write_cnt == 0) && (rwl->read_cnt == 0))
+            rwl->lock_mode = f;
+
+        /* see if any threads should be signaled */
+        if (LIST_NOT_EMPTY(rwl->queue))
+            {
+            /* wake up single writer iff current readers done */
+            q = (rw_qentry_t *)rwl->queue.nextentry;
+            if (q->mode == w)
+                {
+                if (rwl->lock_mode == f)
+                    {                   /* wake up writer */
+                    q = (rw_qentry_t *)
+                        move_list_entry(&rwl->queue,NULL,NULL);
+                    rwl->write_cnt++;
+                    rwl->lock_mode = w;
+                    condition_signal(&q->wait);
+                    }
+                else
+                    assert((rwl->lock_mode==r) && (rwl->write_cnt==0));
+                }
+            else
+                do  /* wake up all readers before next writer */
+                    {
+                    old_q = q;          /* save entry ptr */
+                    q = (rw_qentry_t *)q->links.nextentry;
+
+                    old_q = (rw_qentry_t *)
+                            move_list_entry(&rwl->queue,NULL,NULL);
+                    rwl->read_cnt++;
+                    assert(rwl->lock_mode != w);
+                    rwl->lock_mode = r;
+                    condition_signal(&old_q->wait);
+                    }
+                while (!(q->links.is_hdr || (q->mode == w)));
+            }
+        });                             /* end rw_lock mutex crit sec */
+#endif
+    }
+/*  binary tree functions
+    all functions leave locking to caller
+    lookup requires a comparator function with signature:
+            int cmp(target,test)
+                rvm_offset_t *target;    node to locate
+                rvm_offset_t *test;      node to test against
+
+            returns:    1           target > test
+                        0           target = test
+                       -1           target < test
+*/
+/* traversal vector initializer */
+static void chk_traverse(tree)
+    tree_root_t     *tree;
+    {
+    if (tree->traverse_len <= (tree->max_depth+1))
+        {
+        tree->traverse_len += TRAVERSE_LEN_INCR;
+        if (tree->traverse != NULL)
+            free((char *)tree->traverse);
+        tree->traverse=(tree_pos_t *)
+            malloc(tree->traverse_len*sizeof(tree_pos_t));
+        if (tree->traverse == NULL)
+            assert(rvm_false);
+        }
+    }
+
+#define SET_TRAVERSE(tr,pt,st) \
+    (tr)->traverse[++(tr)->level].ptr = (tree_node_t *)(pt); \
+    (tr)->traverse[(tr)->level].state = (st)
+
+/* tree root initialization */
+void init_tree_root(root)
+    tree_root_t     *root;
+    {
+    root->struct_id = tree_root_id;
+    root->root = NULL;
+    root->traverse = NULL;
+    root->traverse_len = 0;
+    root->n_nodes = 0;
+    root->max_depth = 0;
+    root->unlink = rvm_false;
+    }
+
+/* tree root finalizer */
+void clear_tree_root(root)
+    tree_root_t     *root;
+    {
+
+    assert(root->struct_id == tree_root_id);
+    if (root->traverse != NULL)
+        {
+        free(root->traverse);
+        root->traverse = NULL;
+        root->traverse_len = 0;
+        }
+    }
+
+#ifdef UNUSED_FUNCTIONS
+/* balance checker */
+static int get_depth(node,n_nodes)
+    tree_node_t     *node;
+    long            *n_nodes;
+    {
+    int             lss_depth = 1;      /* init to 1 for this node */
+    int             gtr_depth = 1;
+
+    if (node == NULL) return 0;
+    assert((node->bf >= -1) && (node->bf <= 1));
+
+    if (n_nodes != NULL) (*n_nodes)++;
+    lss_depth += get_depth(node->lss,n_nodes);
+    gtr_depth += get_depth(node->gtr,n_nodes);
+
+    assert(node->bf == (gtr_depth - lss_depth)); /* check balance factor */
+    assert((node->bf >= -1) && (node->bf <= 1));
+
+    if (gtr_depth > lss_depth)          /* return depth of deeper side */
+        return gtr_depth;
+    else
+        return lss_depth;
+    }
+
+/* Guaranteed to return 0, for now */
+static int chk_balance(tree)
+    tree_root_t     *tree;              /* ptr to root of tree */
+    {
+    long            n_nodes = 0;
+    get_depth(tree->root,&n_nodes);
+
+    assert(n_nodes == tree->n_nodes);
+    return 0;
+    }
+#endif
+
+/* binary tree lookup -- returns ptr to node found (or NULL) */
+tree_node_t *tree_lookup(tree,node,cmp)
+    tree_root_t     *tree;              /* root of tree */
+    tree_node_t     *node;              /* node w/ range to lookup */
+    cmp_func_t      *cmp;               /* ptr to comparator function */
+    {
+    tree_node_t     *cur;               /* current search node */
+    tree_node_t     *par = NULL;        /* parent of cur */
+
+    assert(tree->struct_id == tree_root_id);
+    cur = tree->root;
+    while (cur != NULL)                 /* search */
+        {
+        assert(cur != par);             /* loop detector */
+        par = cur;
+        switch ((*cmp)(node,cur))
+            {
+          case -1:                      /* lss */
+            cur = cur->lss;
+            break;
+          case 0:                       /* match */
+            return cur;
+          case 1:                       /* gtr */
+            cur = cur->gtr;
+            break;
+          default:  assert(rvm_false);
+            }
+        }
+
+    return NULL;                        /* not found */
+    }
+/* insertion rotation function */
+static void insert_rotate(tree,bal_pnt,bal_pnt_par,sub_root,new_bf)
+    tree_root_t     *tree;              /* ptr to root of tree */
+    tree_node_t     *bal_pnt;           /* balance point */
+    tree_node_t     *bal_pnt_par;       /* parent of bal_pnt */
+    tree_node_t     *sub_root;          /* root of unbalanced sub-tree */
+    long            new_bf;             /* new balance factor */
+    {
+    tree_node_t     *new_bal_pnt = sub_root; /* new balance node */
+
+    assert(tree->struct_id == tree_root_id);
+    if (new_bf == 1)
+        {
+        /* right heavy */
+        if (sub_root->bf == 1)
+            {
+            /* rotate RR */
+            bal_pnt->gtr = sub_root->lss;
+            sub_root->lss = bal_pnt;
+            bal_pnt->bf = sub_root->bf = 0;
+            }
+        else
+            {
+            /* RL rotations */
+            new_bal_pnt = sub_root->lss;
+            sub_root->lss = new_bal_pnt->gtr;
+            bal_pnt->gtr = new_bal_pnt->lss;
+            new_bal_pnt->gtr = sub_root;
+            new_bal_pnt->lss = bal_pnt;
+
+            /* adjust balance factors */
+            switch (new_bal_pnt->bf)
+                {
+              case 0:   bal_pnt->bf = sub_root->bf = 0;
+                        break;
+              case 1:   bal_pnt->bf = -1; sub_root->bf = 0;
+                        break;
+              case -1:  bal_pnt->bf = 0; sub_root->bf = 1;
+                        break;
+              default:  assert(rvm_false);
+                }
+            new_bal_pnt->bf = 0;
+            }
+        }
+    else
+        {
+        /* left heavy */
+        if (sub_root->bf == -1)
+            {
+            /* rotate LL */
+            bal_pnt->lss = sub_root->gtr;
+            sub_root->gtr = bal_pnt;
+            bal_pnt->bf = sub_root->bf = 0;
+            }
+        else
+            {
+            /* LR rotations */
+            new_bal_pnt = sub_root->gtr;
+            sub_root->gtr = new_bal_pnt->lss;
+            bal_pnt->lss = new_bal_pnt->gtr;
+            new_bal_pnt->lss = sub_root;
+            new_bal_pnt->gtr = bal_pnt;
+
+            /* adjust balance factors */
+            switch (new_bal_pnt->bf)
+                {
+              case 0:   bal_pnt->bf = sub_root->bf = 0;
+                        break;
+              case 1:   bal_pnt->bf = 0; sub_root->bf = -1;
+                        break;
+              case -1:  bal_pnt->bf = 1; sub_root->bf = 0;
+                        break;
+              default:  assert(rvm_false);
+                }
+            new_bal_pnt->bf = 0;
+            }
+        }
+
+    /* complete rotation by re-inserting balanced sub-tree */
+    if (bal_pnt_par == NULL)
+        tree->root = new_bal_pnt;
+    else
+        if (bal_pnt == bal_pnt_par->gtr)
+            bal_pnt_par->gtr = new_bal_pnt;
+        else
+            if (bal_pnt == bal_pnt_par->lss)
+                bal_pnt_par->lss = new_bal_pnt;
+    }
+/* binary tree insertion - traverse vector left suitable for
+   successor iterator */
+rvm_bool_t tree_insert(tree,node,cmp)
+    tree_root_t     *tree;              /* ptr to root of tree */
+    tree_node_t     *node;              /* node to insert */
+    cmp_func_t      *cmp;               /* comparator */
+    {
+    tree_node_t     *cur;               /* current search node */
+    tree_node_t     *par = NULL;        /* parent of cur */
+    tree_node_t     *sub_root;          /* root of unbalanced sub-tree */
+    tree_node_t     *last_unbal;        /* last seen unbalanced node */
+    tree_node_t     *last_unbal_par = NULL; /* parent of last unbalanced node */
+    int             val=0;              /* last comparison value */
+    long            new_bf;             /* new balance factor */
+
+    assert(tree->struct_id == tree_root_id);
+    chk_traverse(tree);
+    node->gtr = node->lss = NULL;
+    node->bf = 0;
+    if (tree->root == NULL)
+        {
+        tree->root = node;              /* insert node into empty tree */
+        tree->n_nodes = tree->max_depth = 1;
+        return rvm_true;
+        }
+
+    /* search for insertion point */
+    tree->level = -1;
+    /* tree->root cannot be null */
+    cur = last_unbal = tree->root;
+    assert(cur != NULL);
+    while (cur != NULL)
+        {
+        /* retain most recent unbalanced node and parent */
+        if (cur->bf != 0)
+            {
+            last_unbal = cur;
+            last_unbal_par = par;
+            }
+
+        /* compare for insertion point */
+        assert((cur->bf >= -1) && (cur->bf <= 1));
+        par = cur;
+        switch (val=(*cmp)(node,cur))
+            {
+          case -1:  SET_TRAVERSE(tree,cur,lss); /* lss */
+                    cur = cur->lss; break;
+          case 0:   SET_TRAVERSE(tree,cur,self);/* match; leave ptr to node */
+                    return rvm_false;
+          case 1:   SET_TRAVERSE(tree,NULL,gtr); /* gtr */
+                    cur = cur->gtr; break;
+          default:  assert(rvm_false);
+            }
+        }
+    /* insert node */
+    if (val == 1)
+        par->gtr = node;
+    else
+        par->lss = node;
+    tree->n_nodes++;
+
+    /* determine side of possible imbalance and set new balance factor */
+    if ((new_bf=(*cmp)(node,last_unbal)) == 1)
+        sub_root = last_unbal->gtr;
+    else
+        sub_root = last_unbal->lss;
+
+    /* set balance factors of sub-tree, all of which must have been 0 */
+    cur = sub_root;
+    while (cur != node)
+        {
+        assert(cur->bf == 0);
+        if ((cur->bf=(*cmp)(node,cur)) == 1)
+            cur = cur->gtr;
+        else
+            cur = cur->lss;
+        }
+
+    /* set balance factor of first unbalanced node; check for imbalance */
+    if (last_unbal->bf == 0)
+        {
+        last_unbal->bf = new_bf;
+        tree->level++;
+        }
+    else
+        if ((last_unbal->bf+new_bf) == 0)
+            last_unbal->bf = 0;
+        else                            /* tree unbalanced: rotate */
+            insert_rotate(tree,last_unbal,last_unbal_par,
+                          sub_root,new_bf);
+
+    /* record maximum depth */
+    if ((tree->level+1) > tree->max_depth)
+        tree->max_depth = tree->level+1;
+
+    return rvm_true;
+    }
+/* deletion rotation function */
+static rvm_bool_t delete_rotate(tree,bal_pnt,bal_pnt_par,sub_root,new_bf)
+    tree_root_t     *tree;              /* ptr to root of tree */
+    tree_node_t     *bal_pnt;           /* balance point */
+    tree_node_t     *bal_pnt_par;       /* parent of bal_pnt */
+    tree_node_t     *sub_root;          /* root of unbalanced sub-tree */
+    long            new_bf;             /* new balance factor */
+    {
+    tree_node_t     *new_bal_pnt = sub_root; /* new balance point */
+    long            old_sub_root_bf = sub_root->bf;
+
+    assert(tree->struct_id == tree_root_id);
+    if (new_bf == 1)
+        {                               /* right heavy */
+        if ((sub_root->bf == 1)
+            || ((sub_root->bf == 0) && (sub_root->lss->bf == -1)))
+            {                           /* RR rotations */
+            bal_pnt->gtr = sub_root->lss;
+            sub_root->lss = bal_pnt;
+            if (sub_root->bf == 1)
+                bal_pnt->bf = sub_root->bf = 0;
+            else
+                {
+                bal_pnt->bf = 1;
+                sub_root->bf = -1;
+                }
+            }
+        else
+            {                           /* RL rotations */
+            new_bal_pnt = sub_root->lss;
+            sub_root->lss = new_bal_pnt->gtr;
+            bal_pnt->gtr = new_bal_pnt->lss;
+            new_bal_pnt->gtr = sub_root;
+            new_bal_pnt->lss = bal_pnt;
+
+            /* adjust balance factors */
+            switch (new_bal_pnt->bf)
+                {
+              case 0:   bal_pnt->bf = 0; sub_root->bf++;
+                        break;
+              case 1:   bal_pnt->bf = -1; sub_root->bf++;
+                        break;
+              case -1:  bal_pnt->bf = 0; sub_root->bf = 1;
+                        break;
+              default:  assert(rvm_false);
+                }
+            if (old_sub_root_bf == 0) new_bal_pnt->bf = 1;
+            else new_bal_pnt->bf = 0;
+            }
+        }
+    else
+        {                               /* left heavy */
+        if ((sub_root->bf == -1)
+            || ((sub_root->bf == 0) && (sub_root->gtr->bf == 1)))
+            {                           /* LL rotations */
+            bal_pnt->lss = sub_root->gtr;
+            sub_root->gtr = bal_pnt;
+            if (sub_root->bf == -1)
+                bal_pnt->bf = sub_root->bf = 0;
+            else
+                {
+                bal_pnt->bf = -1;
+                sub_root->bf = 1;
+                }
+            }
+        else
+            {                           /* LR rotations */
+            new_bal_pnt = sub_root->gtr;
+            sub_root->gtr = new_bal_pnt->lss;
+            bal_pnt->lss = new_bal_pnt->gtr;
+            new_bal_pnt->lss = sub_root;
+            new_bal_pnt->gtr = bal_pnt;
+
+            /* adjust balance factors */
+            switch (new_bal_pnt->bf)
+                {
+              case 0:   bal_pnt->bf = 0; sub_root->bf--;
+                        break;
+              case 1:   bal_pnt->bf = 0; sub_root->bf = -1;
+                        break;
+              case -1:  bal_pnt->bf = 1; sub_root->bf--;
+                        break;
+              default:  assert(rvm_false);
+                }
+            if (old_sub_root_bf == 0) new_bal_pnt->bf = -1;
+            else new_bal_pnt->bf = 0;
+            }
+        }
+    /* complete rotation by re-inserting balanced sub-tree */
+    if (bal_pnt_par == NULL)
+        tree->root = new_bal_pnt;
+    else
+        if (bal_pnt == bal_pnt_par->gtr)
+            bal_pnt_par->gtr = new_bal_pnt;
+        else
+            if (bal_pnt == bal_pnt_par->lss)
+                bal_pnt_par->lss = new_bal_pnt;
+
+    /* return true if depth changed */
+    if (new_bal_pnt->bf == 0)
+        return rvm_true;
+    return rvm_false;
+    }
+/* binary tree deletion -- does not free the node
+   traverse vector not left suitable for iterators */
+rvm_bool_t tree_delete(tree,node,cmp)
+    tree_root_t     *tree;              /* ptr to root of tree */
+    tree_node_t     *node;              /* node to delete */
+    cmp_func_t      *cmp;               /* comparator */
+    {
+    tree_node_t     *cur;               /* current search node */
+    tree_node_t     *sub_root=NULL;     /* unbalanced sub tree root */
+    tree_node_t     *bal_pnt_par;       /* parent of balance point */
+    tree_node_t     *old_root = tree->root; /* save state of old root */
+    long            old_root_bf = tree->root->bf;
+    int             node_level;         /* level at which node found */
+    long            new_bf=0;           /* new balance factor */
+
+    /* search for target node */
+    assert(tree->struct_id == tree_root_id);
+    chk_traverse(tree);
+    tree->level = -1;
+    cur = tree->root;
+    while (cur != NULL)
+        {
+        /* determine branch to follow */
+        assert((cur->bf >= -1) && (cur->bf <= 1));
+        switch ((*cmp)(node,cur))
+            {
+          case -1:                      /* lss */
+            SET_TRAVERSE(tree,cur,lss);
+            cur = cur->lss;
+            break;
+          case 0:
+            SET_TRAVERSE(tree,cur,self);
+            if (cur == node) goto delete; /* located */
+            assert(rvm_false);          /* multiple entries ?!?! */
+          case 1:                       /* gtr */
+            SET_TRAVERSE(tree,cur,gtr);
+            cur = cur->gtr;
+            break;
+          default:  assert(rvm_false);
+            }
+        }
+
+        return rvm_false;               /* not found */
+    /* see if simple delete: node has <= 1 child */
+  delete:
+    tree->n_nodes--;
+    node_level = tree->level;
+    if (node->lss == NULL)
+        {
+        cur = node->gtr;
+        tree->traverse[tree->level].state = gtr;
+        }
+    else if (node->gtr == NULL)
+        {
+        cur = node->lss;
+        tree->traverse[tree->level].state = lss;
+        }
+    else
+        {
+        /* must select replacement node - use deeper side if possible,
+           otherwise choose alternately with depth */
+        if ((new_bf = node->bf) == 0)
+            {
+            new_bf = tree->level & 1;
+            if (new_bf == 0) new_bf = -1;
+            }
+        if (new_bf == 1)
+            {
+            cur = node->gtr;            /* locate successor */
+            tree->traverse[tree->level].state = gtr;
+            }
+        else
+            {
+            cur = node->lss;            /* locate predecessor */
+            tree->traverse[tree->level].state = lss;
+            }
+        while (cur != NULL)
+            {
+            assert((cur->bf >= -1) && (cur->bf <= 1));
+            if (new_bf == 1)
+                {
+                SET_TRAVERSE(tree,cur,lss);
+                cur = cur->lss;
+                }
+            else
+                {
+                SET_TRAVERSE(tree,cur,gtr);
+                cur = cur->gtr;
+                }
+            }
+        /* unlink selected node */
+        if (tree->level == 0)
+            {
+            cur = tree->root;
+            if (new_bf == 1)
+                tree->root = cur->gtr;
+            else
+                tree->root = cur->lss;
+            }
+        else
+            {
+            if (new_bf == 1)
+                cur = tree->traverse[tree->level].ptr->gtr;
+            else
+                cur = tree->traverse[tree->level].ptr->lss;
+            if (tree->traverse[tree->level-1].state == lss)
+                tree->traverse[tree->level-1].ptr->lss = cur;
+            else
+                tree->traverse[tree->level-1].ptr->gtr = cur;
+            cur = tree->traverse[tree->level].ptr;
+            }
+
+        /* update selected node's state with target state */
+        cur->bf = node->bf;
+        cur->gtr = node->gtr;
+        cur->lss = node->lss;
+        }
+
+    /* delete target node */
+    if (node_level == 0)
+        tree->root = cur;
+    else
+        if (tree->traverse[node_level-1].state == lss)
+            tree->traverse[node_level-1].ptr->lss = cur;
+        else
+            tree->traverse[node_level-1].ptr->gtr = cur;
+    tree->traverse[node_level].ptr = cur;
+    /* rebalance as necessary up path */
+    while (--tree->level >= 0)
+        {
+        switch (tree->traverse[tree->level].state)
+            {
+          case lss:
+            new_bf = 1;
+            sub_root = tree->traverse[tree->level].ptr->gtr;
+            break;
+          case gtr:
+            new_bf = -1;
+            sub_root = tree->traverse[tree->level].ptr->lss;
+            break;
+          case self:
+          default:      assert(rvm_false);
+            }
+
+        /* if tree balanced at this point, set new factor and quit */
+        if (tree->traverse[tree->level].ptr->bf == 0)
+            {
+            tree->traverse[tree->level].ptr->bf = new_bf;
+            break;
+            }
+        if ((tree->traverse[tree->level].ptr->bf+new_bf) == 0)
+            {
+            tree->traverse[tree->level].ptr->bf = 0;
+            continue;
+            }
+
+        /* must rotate to balance */
+        if (tree->level == 0)
+            bal_pnt_par = NULL;
+        else
+            bal_pnt_par = tree->traverse[tree->level-1].ptr;
+        if (!delete_rotate(tree,tree->traverse[tree->level].ptr,
+                           bal_pnt_par,sub_root,new_bf))
+            break;                      /* done, depth didn't change */
+        }
+
+    /* adjust maximum height */
+    if ((tree->root == NULL)
+        || ((old_root == tree->root) && (old_root_bf != 0)
+            && (tree->root->bf == 0))
+        || ((old_root != tree->root) && (tree->root->bf == 0)))
+        tree->max_depth--;
+
+    return rvm_true;
+    }
+/* forward order iteration generator: balance not maintained if nodes unlinked */
+tree_node_t *tree_successor(tree)
+    tree_root_t     *tree;              /* ptr to tree root descriptor */
+    {
+    tree_node_t     *cur;               /* current search node */
+
+    /* determine how to continue */
+    assert(tree->struct_id == tree_root_id);
+
+    DO_FOREVER
+        {
+        cur = tree->traverse[tree->level].ptr;
+        if (cur != NULL)
+            assert((cur->bf >= -1) && (cur->bf <= 1));
+        switch (tree->traverse[tree->level].state)
+            {
+          case gtr:
+            if (cur == NULL)
+                {
+                if (--tree->level < 0)
+                    return NULL;
+                continue;
+                }
+          case lss:
+            tree->traverse[tree->level].state = self;
+            tree->traverse[tree->level].ptr = cur->gtr;
+            goto unlink;
+          case self:
+            tree->traverse[tree->level].state = gtr;
+            if (cur == NULL) continue;
+            if (cur->lss != NULL) break;
+            tree->traverse[tree->level].ptr = cur->gtr;
+            goto unlink;
+          case init:
+            assert(tree->level == 0);
+            tree->traverse[0].state = lss;
+            break;
+          default:  assert(rvm_false);
+            }
+
+        /* locate successor */
+        while ((cur=cur->lss) != NULL)
+            {
+            assert((cur->bf >= -1) && (cur->bf <= 1));
+            SET_TRAVERSE(tree,cur,lss);
+            }
+        }
+    /* set next traverse node ptr */
+  unlink:
+    assert(cur != NULL);
+    if (tree->unlink)
+        {
+        tree->n_nodes--;
+        if (tree->level == 0)
+            tree->root = cur->gtr;
+        else
+            tree->traverse[tree->level-1].ptr->lss = cur->gtr;
+        assert(cur->lss == NULL);
+        }
+
+    assert((cur->bf >= -1) && (cur->bf <= 1));
+    return cur;
+    }
+/* reverse order iterator generator: balance not maintained if nodes unlinked */
+tree_node_t *tree_predecessor(tree)
+    tree_root_t     *tree;              /* ptr to tree root descriptor */
+    {
+    tree_node_t     *cur;               /* current search node */
+
+    /* determine how to continue */
+    assert(tree->struct_id == tree_root_id);
+
+    DO_FOREVER
+        {
+        cur = tree->traverse[tree->level].ptr;
+        if (cur != NULL)
+            assert((cur->bf >= -1) && (cur->bf <= 1));
+        switch (tree->traverse[tree->level].state)
+            {
+          case lss:
+            if (cur == NULL)
+                {
+                if (--tree->level < 0)
+                    return NULL;
+                continue;
+                }
+          case gtr:
+            tree->traverse[tree->level].state = self;
+            tree->traverse[tree->level].ptr = cur->lss;
+            goto unlink;
+          case self:
+            tree->traverse[tree->level].state = lss;
+            if (cur == NULL) continue;
+            if (cur->gtr != NULL) break;
+            tree->traverse[tree->level].ptr = cur->lss;
+            goto unlink;
+          case init:
+            assert(tree->level == 0);
+            tree->traverse[0].state = gtr;
+            break;
+          default:  assert(rvm_false);
+            }
+
+        /* locate predecessor */
+        while ((cur=cur->gtr) != NULL)
+            {
+            assert((cur->bf >= -1) && (cur->bf <= 1));
+            SET_TRAVERSE(tree,cur,gtr);
+            }
+        }
+    /* set next traverse node ptr */
+  unlink:
+    assert(cur != NULL);
+    if (tree->unlink)
+        {
+        tree->n_nodes--;
+        if (tree->level == 0)
+            tree->root = cur->lss;
+        else
+            tree->traverse[tree->level-1].ptr->gtr = cur->lss;
+        assert(cur->gtr == NULL);
+        }
+
+    assert((cur->bf >= -1) && (cur->bf <= 1));
+    return cur;
+    }
+/* tree iteration initializers */
+tree_node_t *init_tree_generator(tree,direction,unlink)
+    tree_root_t     *tree;              /* ptr to tree root descriptor */
+    rvm_bool_t      direction;          /* FORWARD ==> lss -> gtr */
+    rvm_bool_t      unlink;             /* unlink nodes from tree if true */
+    {
+
+    assert(tree->struct_id == tree_root_id);
+    tree->unlink = unlink;
+    tree->level = -1;
+    if (tree->root == NULL) return NULL;
+    chk_traverse(tree);
+    SET_TRAVERSE(tree,tree->root,init);
+
+    if (direction == FORWARD)
+        return tree_successor(tree);
+    else
+        return tree_predecessor(tree);
+    }
+/* initilizer for iteration after insertion failure */
+tree_node_t *tree_iterate_insert(tree,node,cmp)
+    tree_root_t     *tree;              /* ptr to root of tree */
+    tree_node_t     *node;              /* node to insert */
+    cmp_func_t      *cmp;               /* comparator */
+    {
+    tree_node_t     *cur;               /* current search node */
+    int             first_level;        /* level of smallest node */
+
+    /* try to insert node */
+    assert(tree->struct_id == tree_root_id);
+    tree->unlink = rvm_false;
+    if (tree_insert(tree,node,cmp))
+        return NULL;                    /* done, no iteration required */
+
+    /* collision, locate smallest conflicting node */
+    first_level = tree->level;
+    cur = tree->traverse[tree->level].ptr->lss;
+    tree->traverse[tree->level].state = lss;
+    while (cur != NULL)
+        switch ((*cmp)(cur,node))
+            {
+          case -1:  SET_TRAVERSE(tree,NULL,gtr);
+                    cur = cur->gtr;
+                    break;
+          case 0:   SET_TRAVERSE(tree,cur,lss);
+                    first_level = tree->level;
+                    cur = cur->lss;
+                    break;
+          case 1:                       /* shouldn't happen */
+          default:  assert(rvm_false);
+            }
+
+    /* return smallest conflicting node */
+    tree->level = first_level;
+    cur = tree->traverse[tree->level].ptr;
+    tree->traverse[tree->level].ptr = cur->gtr;
+    tree->traverse[tree->level].state = self;
+
+    return cur;
+    }
+/* histogram data gathering function */
+void enter_histogram(val,histo,histo_def,length)
+    long            val;                /* value to log */
+    long            *histo;             /* histogram data */
+    long            *histo_def;         /* histogram bucket sizes */
+    long            length;             /* length of histogram vectors */
+    {
+    long            i;
+
+    /* increment proper bucket */
+    for (i=0; i<length-1; i++)
+        if (val <= histo_def[i])
+            {
+            histo[i]++;
+            return;
+            }
+
+    histo[length-1]++;                  /* outsized */
+    return;
+    }
+/* The following functions are needed only on machines without 64-bit
+   integer operations and are used only within macros defined in rvm.h
+*/
+/* rvm_offset_t constructor */
+rvm_offset_t rvm_mk_offset(x,y)
+    rvm_length_t        x;
+    rvm_length_t        y;
+    {
+    rvm_offset_t        tmp;
+
+    tmp.high = x;
+    tmp.low = y;
+
+    return tmp;
+    }
+
+/* add rvm_length to rvm_offset; return (offset + length) */
+rvm_offset_t rvm_add_length_to_offset(offset,length)
+    rvm_offset_t    *offset;            /* ptr to offset */
+    rvm_length_t    length;
+    {
+    rvm_offset_t    tmp;
+
+    tmp.high = offset->high;
+    tmp.low = offset->low + length;
+    if (tmp.low < offset->low)          /* test for overflow */
+        tmp.high ++;                    /* do carry */
+
+    return tmp;
+    }
+
+/* subtract rvm_length from rvm_offset; return (offset - length)) */
+rvm_offset_t rvm_sub_length_from_offset(offset,length)
+    rvm_offset_t    *offset;            /* ptr to offset */
+    rvm_length_t    length;             /* length to subtract */
+    {
+    rvm_offset_t    tmp;
+
+    tmp.high = offset->high;
+    tmp.low = offset->low - length;
+    if (tmp.low > offset->low)          /* test for underflow */
+        tmp.high --;                    /* do borrow */
+
+    return tmp;
+    }
+/* add rvm_offset to rvm_offset; return (x+y) */
+rvm_offset_t rvm_add_offsets(x,y)
+    rvm_offset_t    *x,*y;              /* operand ptrs */
+    {
+    rvm_offset_t    tmp;
+
+    tmp.high = x->high + y->high;       /* add high order bits */
+    tmp.low = x->low + y->low;          /* add low order bits */
+    if (tmp.low < x->low)               /* test for overflow */
+        tmp.high ++;                    /* do carry */
+
+    return tmp;
+    }
+
+/* subtract rvm_offset from rvm_offset; return (x-y) */
+rvm_offset_t rvm_sub_offsets(x,y)
+    rvm_offset_t    *x,*y;              /* operand ptrs */
+    {
+    rvm_offset_t    tmp;
+
+    tmp.high = x->high - y->high;       /* subtract high order bits */
+    tmp.low = x->low - y->low;          /* subtract low order bits */
+    if (tmp.low > x->low)               /* test for underflow */
+        tmp.high --;                    /* do borrow */
+
+
+    return tmp;
+    }
+/* page rounding functions for rvm_offset; return offset rounded up/down
+   to page boundrary: used only for rvm.h macro support */
+rvm_offset_t rvm_rnd_offset_up_to_page(x)
+    rvm_offset_t    *x;                 /* operand ptr */
+    {
+    rvm_offset_t    tmp;
+
+    tmp = rvm_add_length_to_offset(x,page_size-1);
+    tmp.low = tmp.low & page_mask;
+
+    return tmp;
+    }
+
+rvm_offset_t rvm_rnd_offset_dn_to_page(x)
+    rvm_offset_t    *x;                 /* operand ptr */
+    {
+    rvm_offset_t    tmp;
+
+    tmp.high = x->high;
+    tmp.low = x->low & page_mask;
+
+    return tmp;
+    }
+
+/* page size, mask export functions: used only for rvm.h macros */
+rvm_length_t rvm_page_size()
+    {
+    return page_size;
+    }
+
+rvm_length_t rvm_page_mask()
+    {
+    return page_mask;
+    }
+
+/* round offset to sector size support */
+rvm_offset_t rvm_rnd_offset_to_sector(x)
+    rvm_offset_t    *x;
+    {
+    rvm_offset_t    tmp;
+
+    tmp = RVM_ADD_LENGTH_TO_OFFSET((*x),SECTOR_SIZE-1);
+    tmp.low = tmp.low & (SECTOR_MASK);
+
+    return tmp;
+    }
-- 
1.8.3.2

