From 413a9446f9a3fc1b289478e8f9dff80a5459e81c Mon Sep 17 00:00:00 2001
From: "anand.vidwansa" <anand_vidwansa@xyratex.com>
Date: Thu, 22 Aug 2013 04:24:22 -0700
Subject: [PATCH 15/15] bigmemalloc: - Use of m0_varr in IO path where memory
 allocations are prone to go over   PAGE_CACHE_SIZE. - Compilation is still
 due along with testing.

---
 lib/varr.c                          |  16 +-
 m0t1fs/linux_kernel/file.c          | 306 +++++++++++++++++++++++++++++-------
 m0t1fs/linux_kernel/file_internal.h |  65 +++++++-
 3 files changed, 318 insertions(+), 69 deletions(-)

diff --git a/lib/varr.c b/lib/varr.c
index 6455a7b..9d1156a 100644
--- a/lib/varr.c
+++ b/lib/varr.c
@@ -163,6 +163,13 @@ M0_INTERNAL int m0_varr_init(struct m0_varr *arr,
 	M0_PRE(bufsize > 0);
 	M0_PRE(bufsize % size == 0);
 
+	arr->va_nr        = nr;
+	arr->va_alloc     = arr->va_dealloc = 0;
+	arr->va_sizeof    = size;
+	arr->va_bufsize   = bufsize;
+	arr->va_bufptr_nr = arr->va_bufsize / VA_TNODEPTR_SIZE;
+	m0_varr_bob_init(arr);
+
 	/*
 	 * Since two successive buffs are not guaranteed to be contiguous,
 	 * structures bigger than page size can't fit in such array since
@@ -172,12 +179,6 @@ M0_INTERNAL int m0_varr_init(struct m0_varr *arr,
 	if (!m0_varr_size_is_valid(arr))
 		return -EINVAL;
 
-	arr->va_nr        = nr;
-	arr->va_alloc     = arr->va_dealloc = 0;
-	arr->va_sizeof    = size;
-	arr->va_bufsize   = bufsize;
-	arr->va_bufptr_nr = arr->va_bufsize / VA_TNODEPTR_SIZE;
-	m0_varr_bob_init(arr);
 	for (i = 0; i < VA_TNODE_NR; ++i)
 		arr->va_tree[i] = NULL;
 
@@ -235,6 +236,7 @@ M0_INTERNAL unsigned long *varr_buffer(const struct m0_varr *arr,
 
 	buf    = index / varr_obj_nr_in_buff(arr);
 	holder = arr->va_tree[buf / max_buff_nr_till_lev_n_pn(arr, 1)];
+	buf    = buf % max_buff_nr_till_lev_n_pn(arr, 1);
 	M0_ASSERT(holder != NULL);
 
 	for (level = 1; level <= arr->va_depth && buf != 0; ++level) {
@@ -243,8 +245,8 @@ M0_INTERNAL unsigned long *varr_buffer(const struct m0_varr *arr,
 		if (level != 1)
 			buf -= max_buff_nr_till_lev_n_pn(arr, level) * id;
 		/* Dereferences the buffer pointer at given offset. */
-		M0_ASSERT((unsigned long *)*holder!= NULL);
 		holder = (unsigned long *)*holder;
+		M0_ASSERT(holder != NULL);
 	}
 
 	M0_POST_EX(varr_invariant(arr));
diff --git a/m0t1fs/linux_kernel/file.c b/m0t1fs/linux_kernel/file.c
index fd60ce6..11c2530 100644
--- a/m0t1fs/linux_kernel/file.c
+++ b/m0t1fs/linux_kernel/file.c
@@ -649,6 +649,129 @@ M0_INTERNAL void io_bob_tlists_init(void)
 	M0_ASSERT(iofop_bobtype.bt_magix == M0_T1FS_IOFOP_MAGIC);
 }
 
+static int io_indexvec_alloc(struct io_indexvec *ivec,
+			     uint32_t            obj_nr)
+{
+	int rc;
+
+	M0_PRE(ivec != NULL);
+
+	ivec->ii_ivec.iv_index       = NULL;
+	ivec->ii_ivec.iv_vec.v_count = NULL;
+	ivec->ii_ivec.iv_vec.v_nr    = obj_nr;
+	rc = m0_varr_init(&ivec->ii_indices, obj_nr, sizeof m0_bindex_t,
+			  PAGE_CACHE_SIZE);
+	if (rc != 0)
+		return rc;
+
+	return m0_varr_init(&ivec->ii_counts, obj_nr, sizeof m0_bcount_t,
+			    PAGE_CACHE_SIZE);
+}
+
+static void io_indexvec_dealloc(struct io_indexvec *ivec)
+{
+	M0_PRE(ivec != NULL);
+
+	ivec->ii_ivec.iv_vec.v_nr = 0;
+	m0_varr_fini(&ivec->ii_indices);
+	m0_varr_fini(&ivec->ii_counts);
+}
+
+static m0_bindex_t io_indexvec_index_read(const struct io_indexvec *ivec,
+					  uint32_t                  index)
+{
+	M0_PRE(ivec != NULL);
+
+	return *m0_varr_ele_get(&ivec->ii_indices, m0_bindex_t, index);
+}
+
+static void io_indexvec_index_write(struct io_indexvec *ivec,
+				    uint32_t            index,
+				    m0_bindex_t         val)
+{
+	M0_PRE(ivec != NULL);
+
+	*m0_varr_ele_get(&ivec->ii_indices, m0_bindex_t, index) = val;
+}
+
+static m0_bcount_t io_indexvec_count_read(const struct io_indexvec *ivec,
+					  uint32_t                  index)
+{
+	M0_PRE(ivec != NULL);
+
+	return *m0_varr_ele_get(&ivec->ii_counts, m0_bcount_t, index);
+}
+
+static void io_indexvec_count_write(struct io_bufvec *bvec,
+				    uint32_t          index,
+				    m0_bcount_t       val)
+{
+	M0_PRE(bvec != NULL);
+
+	*m0_varr_ele_get(&ivec->ii_counts, m0_bcount_t, index) = val;
+}
+
+static int io_bufvec_alloc(struct io_bufvec *bvec,
+			   uint32_t          obj_nr)
+{
+	int rc;
+
+	M0_PRE(bvec != NULL);
+
+	bvec->ib_bvec.ov_buf         = NULL;
+	bvec->ib_bvec.ov_vec.v_count = NULL;
+	bvec->ib_bvec.ov_vec.v_nr    = obj_nr;
+	rc = m0_varr_init(&bvec->ib_bufs, obj_nr, sizeof void *,
+			  PAGE_CACHE_SIZE);
+	if (rc != 0)
+		return rc;
+	return m0_varr_init(&bvec->ib_counts, obj_nr, sizeof m0_bcount_t,
+			    PAGE_CACHE_SIZE);
+}
+
+static void io_bufvec_dealloc(struct io_bufvec *bvec)
+{
+	M0_PRE(bvec != NULL);
+
+	bvec->ov_vec.v_nr = 0;
+	m0_varr_fini(&bvec->ib_bufs);
+	m0_varr_fini(&bvec->ib_counts);
+}
+
+static void *io_bufvec_bufptr_read(const struct io_bufvec *bvec,
+				   uint32_t                index)
+{
+	M0_PRE(bvec != NULL);
+
+	return *m0_varr_ele_get(&bvec->ib_bufs, void *, index);
+}
+
+static void io_bufvec_bufptr_write(struct io_bufvec *bvec,
+				   uint32_t          index,
+				   void             *val)
+{
+	M0_PRE(bvec != NULL);
+
+	*m0_varr_ele_get(&bvec->ib_bufs, void *, index) = val;
+}
+
+static m0_bcount_t io_bufvec_count_read(const struct io_bufvec *bvec,
+					uint32_t                index)
+{
+	M0_PRE(bvec != NULL);
+
+	return *m0_varr_ele_get(&bvec->ib_counts, m0_bcount_t, index);
+}
+
+static void io_bufvec_count_write(struct io_bufvec *bvec,
+				  uint32_t          index,
+				  m0_bcount_t       val)
+{
+	M0_PRE(bvec != NULL);
+
+	*m0_varr_ele_get(&bvec->ib_counts, m0_bcount_t, index) = val;
+}
+
 static void io_rpc_item_cb (struct m0_rpc_item *item);
 static void io_req_fop_release(struct m0_ref *ref);
 
@@ -982,7 +1105,7 @@ static bool target_ioreq_invariant(const struct target_ioreq *ti)
 		target_ioreq_bob_check(ti) &&
 		ti->ti_session	    != NULL &&
 		ti->ti_nwxfer	    != NULL &&
-		ti->ti_bufvec.ov_buf != NULL &&
+		//ti->ti_bufvec.ov_buf != NULL &&
 		m0_fid_is_valid(&ti->ti_fid) &&
 		m0_tl_forall(iofops, iofop, &ti->ti_iofops,
 			     io_req_fop_invariant(iofop));
@@ -2657,33 +2780,44 @@ static int dgmode_rwvec_alloc_init(struct target_ioreq *ti)
 
 	cnt = page_nr(req->ir_iomap_nr * layout_unit_size(play) *
 		      (layout_n(play) + layout_k(play)));
-	rc  = m0_indexvec_alloc(&dg->dr_ivec, cnt, &m0t1fs_addb_ctx,
-			        M0T1FS_ADDB_LOC_READVEC_ALLOC_IVEC_FAIL);
+	/* TODO: Replace with m0_varr. */
+	/*rc  = m0_indexvec_alloc(&dg->dr_ivec, cnt, &m0t1fs_addb_ctx,
+			        M0T1FS_ADDB_LOC_READVEC_ALLOC_IVEC_FAIL);*/
+	rc = io_indexvec_alloc(&dg->dr_ivec, cnt);
 	if (rc != 0)
 		goto failed;
 
-	M0_ALLOC_ARR_ADDB(dg->dr_bufvec.ov_buf, cnt, &m0_addb_gmc,
+	/* TODO: Replace with m0_varr. */
+	/*M0_ALLOC_ARR_ADDB(dg->dr_bufvec.ov_buf, cnt, &m0_addb_gmc,
 			  M0T1FS_ADDB_LOC_READVEC_ALLOC_BVEC, &m0t1fs_addb_ctx);
+	
 	if (dg->dr_bufvec.ov_buf == NULL) {
 		rc = -ENOMEM;
 		goto failed;
-	}
+	}*/
 
-	M0_ALLOC_ARR_ADDB(dg->dr_bufvec.ov_vec.v_count, cnt, &m0_addb_gmc,
+	/* TODO: Replace with m0_varr. */
+	/*M0_ALLOC_ARR_ADDB(dg->dr_bufvec.ov_vec.v_count, cnt, &m0_addb_gmc,
 			  M0T1FS_ADDB_LOC_READVEC_ALLOC_BVEC_CNT,
 			  &m0t1fs_addb_ctx);
 	if (dg->dr_bufvec.ov_vec.v_count == NULL) {
 		rc = -ENOMEM;
 		goto failed;
-	}
+	}*/
+	rc = io_bufvec_alloc(&dg->dr_bufvec, cnt);
+	if (rc != 0)
+		goto failed;
 
-	M0_ALLOC_ARR_ADDB(dg->dr_pageattrs, cnt, &m0_addb_gmc,
+	/* TODO: Replace with m0_varr. */
+	/*M0_ALLOC_ARR_ADDB(dg->dr_pageattrs, cnt, &m0_addb_gmc,
 			  M0T1FS_ADDB_LOC_READVEC_ALLOC_PAGEATTR,
 			  &m0t1fs_addb_ctx);
 	if (dg->dr_pageattrs == NULL) {
 		rc = -ENOMEM;
 		goto failed;
-	}
+	}*/
+	m0_varr_init(&dg->dr_pageattrs, cnt, sizeof enum page_attr,
+		     PAGE_CACHE_SIZE);
 
 	/*
 	 * This value is incremented every time a new segment is added
@@ -2695,10 +2829,11 @@ static int dgmode_rwvec_alloc_init(struct target_ioreq *ti)
 	M0_RETURN(0);
 failed:
 	ti->ti_dgvec = NULL;
-	if (dg->dr_bufvec.ov_buf != NULL)
+	/*if (dg->dr_bufvec.ov_buf != NULL)
 		m0_free(dg->dr_bufvec.ov_buf);
 	if (dg->dr_bufvec.ov_vec.v_count != NULL)
-		m0_free(dg->dr_bufvec.ov_vec.v_count);
+		m0_free(dg->dr_bufvec.ov_vec.v_count);*/
+	io_bufvec_dealloc(&dg->dr_bufvec);
 	m0_free(dg);
 	M0_RETERR(rc, "Dgmode read vector allocation failed");
 }
@@ -2721,10 +2856,13 @@ static void dgmode_rwvec_dealloc_fini(struct dgmode_rwvec *dg)
 	if (dg->dr_ivec.iv_vec.v_nr == 0)
 		++dg->dr_ivec.iv_vec.v_nr;
 
-	m0_indexvec_free(&dg->dr_ivec);
-	m0_free(dg->dr_bufvec.ov_buf);
-	m0_free(dg->dr_bufvec.ov_vec.v_count);
-	m0_free(dg->dr_pageattrs);
+	//m0_indexvec_free(&dg->dr_ivec);
+	io_indexvec_dealloc(&dg->dr_ivec);
+	//m0_free(dg->dr_bufvec.ov_buf);
+	//m0_free(dg->dr_bufvec.ov_vec.v_count);
+	io_bufvec_dealloc(&dg->dr_bufvec);
+	//m0_free(dg->dr_pageattrs);
+	m0_varr_fini(&dg->dr_pageattrs);
 }
 
 /*
@@ -3664,33 +3802,53 @@ static int target_ioreq_init(struct target_ioreq    *ti,
 	tioreqs_tlink_init(ti);
 	target_ioreq_bob_init(ti);
 
+	/*
+	 * TODO: Replace ti->ti_ivec.iv_vec.v_count and ti->ti_ivec.iv_index
+	 * with m0_varr.
 	rc = m0_indexvec_alloc(&ti->ti_ivec, page_nr(size),
 	                       &m0t1fs_addb_ctx,
 	                       M0T1FS_ADDB_LOC_TI_REQ_INIT_IV);
+	 */
+	rc = io_indexvec_alloc(&ti->ti_ivec, page_nr(size));
 	if (rc != 0)
 		goto out;
 
-	ti->ti_bufvec.ov_vec.v_nr = page_nr(size);
+	//ti->ti_bufvec.ov_vec.v_nr = page_nr(size);
+	/* TODO: Replace with m0_varr. */
+	/*
 	M0_ALLOC_ARR_ADDB(ti->ti_bufvec.ov_vec.v_count,
 	                  ti->ti_bufvec.ov_vec.v_nr, &m0_addb_gmc,
 			  M0T1FS_ADDB_LOC_IOREQ_INIT_BVECC,
 	                  &m0t1fs_addb_ctx);
 	if (ti->ti_bufvec.ov_vec.v_count == NULL)
 		goto fail;
+		*/
 
+	/* TODO: Replace with m0_varr. */
+	/*
 	M0_ALLOC_ARR_ADDB(ti->ti_bufvec.ov_buf, ti->ti_bufvec.ov_vec.v_nr,
 	                  &m0_addb_gmc,
 	                  M0T1FS_ADDB_LOC_IOREQ_INIT_BVECB,
 	                  &m0t1fs_addb_ctx);
 	if (ti->ti_bufvec.ov_buf == NULL)
 		goto fail;
+		*/
+	rc = io_bufvec_alloc(&ti->ti_bufvec, page_nr(size));
+	if (rc != 0)
+		goto fail;
 
-	M0_ALLOC_ARR_ADDB(ti->ti_pageattrs, ti->ti_bufvec.ov_vec.v_nr,
+	/* TODO: Replace with m0_varr. */
+	/*M0_ALLOC_ARR_ADDB(ti->ti_pageattrs, ti->ti_bufvec.ov_vec.v_nr,
 	                  &m0_addb_gmc,
 			  M0T1FS_ADDB_LOC_IOREQ_INIT_PGATTRS,
 	                  &m0t1fs_addb_ctx);
 	if (ti->ti_pageattrs == NULL)
 		goto fail;
+		*/
+	rc = m0_varr_init(&ti->ti_pageattrs, page_nr(size),
+			  sizeof enum page_attr, PAGE_CACHE_SIZE);
+	if (rc != 0)
+		goto fail;
 
 	/*
 	 * This value is incremented when new segments are added to the
@@ -3701,9 +3859,11 @@ static int target_ioreq_init(struct target_ioreq    *ti,
 	M0_POST(target_ioreq_invariant(ti));
 	M0_RETURN(0);
 fail:
-	m0_indexvec_free(&ti->ti_ivec);
-	m0_free(ti->ti_bufvec.ov_vec.v_count);
-	m0_free(ti->ti_bufvec.ov_buf);
+	//m0_indexvec_free(&ti->ti_ivec);
+	io_indexvec_dealloc(&ti->ti_ivec);
+	//m0_free(ti->ti_bufvec.ov_vec.v_count);
+	//m0_free(ti->ti_bufvec.ov_buf);
+	io_bufvec_dealloc(&ti->ti_bufvec);
 out:
 	M0_RETERR(-ENOMEM, "Failed to allocate memory in target_ioreq_init");
 }
@@ -3724,16 +3884,19 @@ static void target_ioreq_fini(struct target_ioreq *ti)
 	if (ti->ti_ivec.iv_vec.v_nr == 0)
 		ti->ti_ivec.iv_vec.v_nr = ti->ti_bufvec.ov_vec.v_nr;
 
-	m0_indexvec_free(&ti->ti_ivec);
-	m0_free(ti->ti_bufvec.ov_buf);
-	m0_free(ti->ti_bufvec.ov_vec.v_count);
-	m0_free(ti->ti_pageattrs);
+	//m0_indexvec_free(&ti->ti_ivec);
+	io_indexvec_dealloc(&ti->ti_ivec);
+	//m0_free(ti->ti_bufvec.ov_buf);
+	//m0_free(ti->ti_bufvec.ov_vec.v_count);
+	io_bufvec_dealloc(&ti->ti_bufvec);
+	//m0_free(ti->ti_pageattrs);
+	m0_varr_fini(ti->ti_pagrattrs);
 	if (ti->ti_dgvec != NULL)
 		dgmode_rwvec_dealloc_fini(ti->ti_dgvec);
 
-	ti->ti_bufvec.ov_buf	     = NULL;
-	ti->ti_bufvec.ov_vec.v_count = NULL;
-	ti->ti_pageattrs	     = NULL;
+	//ti->ti_bufvec.ov_buf	     = NULL;
+	//ti->ti_bufvec.ov_vec.v_count = NULL;
+	//ti->ti_pageattrs	     = NULL;
 	M0_LEAVE();
 }
 
@@ -3874,10 +4037,13 @@ static void target_ioreq_seg_add(struct target_ioreq              *ti,
 	struct m0_pdclust_layout  *play;
 	uint64_t	           frame = tgt->ta_frame;
 	uint64_t	           unit  = src->sa_unit;
-	struct m0_indexvec        *ivec;
-	struct m0_bufvec          *bvec;
+	//struct m0_indexvec        *ivec;
+	struct io_indexvec        *ivec;
+	//struct m0_bufvec          *bvec;
+	struct io_bufvec          *bvec;
 	enum m0_pdclust_unit_type  unit_type;
-	enum page_attr            *pattr;
+	//enum page_attr            *pattr;
+	struct m0_varr              *pattr;
 
 	M0_ENTRY("tio req %p, gob_offset %llu, count %llu frame %llu unit %llu",
 		 ti, gob_offset, count, frame, unit);
@@ -3905,21 +4071,24 @@ static void target_ioreq_seg_add(struct target_ioreq              *ti,
 		M0_ASSERT(ti->ti_dgvec != NULL);
 		ivec  = &ti->ti_dgvec->dr_ivec;
 		bvec  = &ti->ti_dgvec->dr_bufvec;
-		pattr = ti->ti_dgvec->dr_pageattrs;
+		pattr = &ti->ti_dgvec->dr_pageattrs;
 	} else {
 		ivec  = &ti->ti_ivec;
 		bvec  = &ti->ti_bufvec;
-		pattr = ti->ti_pageattrs;
+		pattr = &ti->ti_pageattrs;
 	}
 
 	while (pgstart < toff + count) {
 		pgend = min64u(pgstart + PAGE_CACHE_SIZE, toff + count);
 		seg   = SEG_NR(ivec);
 
-		INDEX(ivec, seg) = pgstart;
-		COUNT(ivec, seg) = pgend - pgstart;
+		//INDEX(ivec, seg) = pgstart;
+		io_indexvec_index_write(ivec, seg, pgstart);
+		//COUNT(ivec, seg) = pgend - pgstart;
+		io_indexvec_count_write(ivec, seg, pgend - pgstart);
 
-		bvec->ov_vec.v_count[seg] = pgend - pgstart;
+		//bvec->ov_vec.v_count[seg] = pgend - pgstart;
+		io_bufvec_count_write(bvec, seg, pgend - pgstart);
 
 		if (unit_type == M0_PUT_DATA) {
 			uint32_t row;
@@ -3928,7 +4097,8 @@ static void target_ioreq_seg_add(struct target_ioreq              *ti,
 			page_pos_get(map, goff, &row, &col);
 			buf = map->pi_databufs[row][col];
 
-			pattr[seg] |= PA_DATA;
+			//pattr[seg] |= PA_DATA;
+			*m0_varr_ele_get(pattr, enum page_attr, seg) |= PA_DATA;
 			M0_LOG(M0_INFO, "Data seg added");
 		} else {
 			/*
@@ -3937,22 +4107,31 @@ static void target_ioreq_seg_add(struct target_ioreq              *ti,
 			 */
 			buf = map->pi_paritybufs[page_id(goff)]
 						[unit % data_col_nr(play)];
-			pattr[seg] |= PA_PARITY;
+			//pattr[seg] |= PA_PARITY;
+			*m0_varr_ele_get(pattr, enum page_attr, seg) |= PA_PARITY;
 			M0_LOG(M0_INFO, "Parity seg added");
 		}
 
-		bvec->ov_buf[seg]  = buf->db_buf.b_addr;
-		pattr[seg] |= buf->db_flags;
+		//bvec->ov_buf[seg]  = buf->db_buf.b_addr;
+		io_bufvec_bufptr_write(bvec, seg, buf->db_buf.b_addr);
+		//pattr[seg] |= buf->db_flags;
+		*m0_varr_ele_get(pattr, enum page_attr, seg) |= buf->db_flags;
 
 		M0_LOG(M0_INFO, "pageaddr = %p, index = %llu, size = %llu\n",
-			bvec->ov_buf[seg], INDEX(ivec, seg), COUNT(ivec, seg));
+			/*bvec->ov_buf[seg]*/
+			io_bufvec_bufptr_read(bvec, seg), /*INDEX(ivec, seg)*/
+			io_indexvec_index_read(ivec, seg),/*COUNT(ivec, seg))*/
+			io_indexvec_count_read(ivec, seg));
 		M0_LOG(M0_DEBUG, "Seg id %d [%llu, %llu] added to target_ioreq "
 		       "with fid [%llu:%llu] with flags 0x%x: ", seg,
-		       INDEX(ivec, seg), COUNT(ivec, seg),
+		       /*INDEX(ivec, seg)*/io_indexvec_index_read(ivec, seg),
+		       /*COUNT(ivec, seg)*/io_indexvec_count_read(ivec, seg),
 		       ti->ti_fid.f_container, ti->ti_fid.f_key,
-		       pattr[seg]);
+		       /*pattr[seg]*/*m0_varr_ele_get(pattr, enum page_attr,
+				      seg));
 
-		goff += COUNT(ivec, seg);
+		//goff += COUNT(ivec, seg);
+		goff += io_indexvec_count_read(ivec, seg);
 		++ivec->iv_vec.v_nr;
 		pgstart = pgend;
 	}
@@ -4738,7 +4917,8 @@ static int bulk_buffer_add(struct io_req_fop	   *irfop,
 	int		    rc;
 	int		    seg_nr;
 	struct io_request  *req;
-	struct m0_indexvec *ivec;
+	//struct m0_indexvec *ivec;
+	struct io_indexvec *ivec;
 
 	M0_ENTRY("io_req_fop %p net_domain %p delta_size %d",
 		 irfop, dom, *delta);
@@ -4756,7 +4936,7 @@ static int bulk_buffer_add(struct io_req_fop	   *irfop,
 		  &irfop->irf_tioreq->ti_ivec :
 		  &irfop->irf_tioreq->ti_dgvec->dr_ivec;
 	seg_nr  = min32(m0_net_domain_get_max_buffer_segments(dom),
-		       SEG_NR(ivec));
+		        SEG_NR(ivec));
 	*delta += io_desc_size(dom);
 
 	if (m0_io_fop_size_get(&irfop->irf_iofop.if_fop) + *delta < maxsize) {
@@ -4786,10 +4966,13 @@ static int target_ioreq_iofops_prepare(struct target_ioreq *ti,
 	uint32_t		        maxsize;
 	uint32_t		        delta;
 	enum page_attr		        rw;
-	enum page_attr                 *pattr;
-	struct m0_bufvec               *bvec;
+	//enum page_attr                 *pattr;
+	struct m0_varr                 *pattr;
+	//struct m0_bufvec               *bvec;
+	struct io_bufvec               *bvec;
 	struct io_request              *req;
-	struct m0_indexvec             *ivec;
+	//struct m0_indexvec             *ivec;
+	struct io_indexvec             *ivec;
 	struct io_req_fop              *irfop;
 	struct m0_net_domain           *ndom;
 	struct m0_rpc_bulk_buf         *rbuf;
@@ -4819,12 +5002,12 @@ static int target_ioreq_iofops_prepare(struct target_ioreq *ti,
 	     req->ir_sns_state   == SRS_REPAIR_NOTDONE)) {
 		ivec  = &ti->ti_ivec;
 		bvec  = &ti->ti_bufvec;
-		pattr = ti->ti_pageattrs;
+		pattr = &ti->ti_pageattrs;
 	} else {
 		M0_ASSERT(ti->ti_dgvec != NULL);
 		ivec  = &ti->ti_dgvec->dr_ivec;
 		bvec  = &ti->ti_dgvec->dr_bufvec;
-		pattr = ti->ti_dgvec->dr_pageattrs;
+		pattr = &ti->ti_dgvec->dr_pageattrs;
 	}
 
 	ndom	= ti->ti_session->s_conn->c_rpc_machine->rm_tm.ntm_dom;
@@ -4841,9 +5024,13 @@ static int target_ioreq_iofops_prepare(struct target_ioreq *ti,
 		bbsegs = 0;
 
 		M0_LOG(M0_INFO, "pageattr = %u, filter = %u, rw = %u",
-			pattr[buf], filter, rw);
+			/*pattr[buf]*/*m0_varr_ele_get(pattr, enum page_attr,
+				       buf), filter, rw);
 
-		if (!(pattr[buf] & filter) || !(pattr[buf] & rw)) {
+		if (!(/*pattr[buf]*/
+		      *m0_varr_ele_get(pattr, enum page_attr, buf) & filter) ||
+		      !(/*pattr[buf]*/
+			*m0_varr_ele_get(pattr, enum page_attr, buf) & rw)) {
 			++buf;
 			continue;
 		}
@@ -4887,14 +5074,21 @@ static int target_ioreq_iofops_prepare(struct target_ioreq *ti,
 			 * Adds a page to rpc bulk buffer only if it passes
 			 * through the filter.
 			 */
-			if (pattr[buf] & rw && pattr[buf] & filter) {
+			if (/*pattr[buf]*/
+			    *m0_varr_ele_get(pattr, enum page_attr, buf) & rw &&
+			    /*pattr[buf]*/
+			    *m0_varr_ele_get(pattr, enum page_attr, buf) & filter) {
 
 				delta += io_seg_size();
 
 				rc = m0_rpc_bulk_buf_databuf_add(rbuf,
-						bvec->ov_buf[buf],
-						COUNT(ivec, buf),
-						INDEX(ivec, buf), ndom);
+						//bvec->ov_buf[buf],
+						io_bufvec_bufptr_read(bvec, buf),
+						//COUNT(ivec, buf),
+						io_indexvec_count_read(ivec, buf),
+						//INDEX(ivec, buf),
+						io_indexvec_index_read(ivec, buf),
+						ndom);
 
 				if (rc == -EMSGSIZE) {
 
diff --git a/m0t1fs/linux_kernel/file_internal.h b/m0t1fs/linux_kernel/file_internal.h
index e719fd9..1873679 100644
--- a/m0t1fs/linux_kernel/file_internal.h
+++ b/m0t1fs/linux_kernel/file_internal.h
@@ -1357,6 +1357,7 @@ struct io_request {
          * Array of struct pargrp_iomap pointers.
          * Each pargrp_iomap structure describes the part of parity group
          * spanned by segments from ::ir_ivec.
+	 * TODO: Move to m0_varr.
          */
         struct pargrp_iomap        **ir_iomaps;
 
@@ -1650,6 +1651,52 @@ struct target_ioreq_ops {
 };
 
 /**
+ * Most of the potential memory allocations which can cross PAGE_CACHE_SIZE
+ * are in
+ * - m0_bufvec   (::ov_buf, ::ov_vec::v_count) and
+ * - m0_indexvec (::iv_index, ::iv_vec::v_count)
+ * And m0_bufvec and m0_indexvec are mostly used by target_ioreq and
+ * dgmode_rwvec structures.
+ *
+ * Ideally, replacing the above mentioned fields with m0_varr would be the
+ * right thing. Another approach is to have wrapper structures over
+ * m0_bufvec and m0_indexvec with m0_varr object embedded which will take
+ * care of handling memory accesses.
+ * But given the timeframe, the wrapper structure approach is chosen to
+ * make it as complete as possible.
+ * Later, modifying m0_indexvec and m0_bufvec can be taken up which can
+ * scale up to changing all invocations of these structures in whole
+ * Mero code base.
+ */
+struct io_indexvec {
+	/** Associated index vector. */
+	struct m0_indexvec ii_ivec;
+
+	/** The virtual array which will fill in for m0_indexvec::iv_index. */
+	struct m0_varr     ii_indices;
+
+	/**
+	 * The virtual array which will fill in for
+	 * m0_indexvec::iv_vec::v_count.
+	 */
+	struct m0_varr     ii_counts;
+};
+
+struct io_bufvec {
+	/** Associated bufvec. */
+	struct m0_bufvec   ib_bvec;
+
+	/** The virtual array which will fill in for m0_bufvec::ov_buf. */
+	struct m0_varr     ib_bufs;
+
+	/**
+	 * The virtual array which will fill in for
+	 * m0_bufvec::ov_vec::v_count.
+	 */
+	struct m0_varr     ib_counts;
+};
+
+/**
  * IO vector for degraded mode read or write.
  * This is not used when pool state is healthy.
  */
@@ -1658,16 +1705,19 @@ struct dgmode_rwvec {
 	 * Index vector to hold page indices during degraded mode
 	 * read/write IO.
 	 */
-	struct m0_indexvec   dr_ivec;
+	//struct m0_indexvec   dr_ivec;
+	struct io_indexvec   dr_ivec;
 
 	/**
 	 * Buffer vector to hold page addresses during degraded mode
 	 * read/write IO.
 	 */
-	struct m0_bufvec     dr_bufvec;
+	//struct m0_bufvec     dr_bufvec;
+	struct io_bufvec     dr_bufvec;
 
 	/** Represents attributes for pages from ::ti_dgvec. */
-	enum page_attr      *dr_pageattrs;
+	//enum page_attr      *dr_pageattrs;
+	struct m0_varr       dr_pageattrs;
 
 	/** Backlink to parent target_ioreq structure. */
 	struct target_ioreq *dr_tioreq;
@@ -1710,13 +1760,15 @@ struct target_ioreq {
 	 * Each segment in this vector is worth PAGE_CACHE_SIZE except
 	 * the very last one.
          */
-        struct m0_indexvec             ti_ivec;
+        //struct m0_indexvec             ti_ivec;
+	struct io_indexvec             ti_ivec;
 
         /**
 	 * Buffer vector corresponding to index vector above.
 	 * This buffer is in sync with ::ti_ivec.
 	 */
-        struct m0_bufvec               ti_bufvec;
+        //struct m0_bufvec               ti_bufvec;
+	struct io_bufvec               ti_bufvec;
 
 	/**
 	 * Degraded mode read/write IO vector.
@@ -1729,7 +1781,8 @@ struct target_ioreq {
 	 * Array of page attributes.
 	 * Represents attributes for pages from ::ti_ivec and ::ti_bufvec.
 	 */
-        enum page_attr                *ti_pageattrs;
+        //enum page_attr                *ti_pageattrs;
+	struct m0_varr                 ti_pageattrs;
 
         /** target_ioreq operation vector. */
         const struct target_ioreq_ops *ti_ops;
-- 
1.8.3.2

